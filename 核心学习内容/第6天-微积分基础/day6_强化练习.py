#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬6å¤©å¾®ç§¯åˆ†åŸºç¡€ - å¼ºåŒ–ç»ƒä¹ 
ç›®æ ‡ï¼šå°†ç†è®ºç†è§£è½¬åŒ–ä¸ºå®é™…æ“ä½œèƒ½åŠ›
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡å­—ä½“
rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
rcParams['axes.unicode_minus'] = False

print("ğŸ¯ ç¬¬6å¤©å¾®ç§¯åˆ†åŸºç¡€å¼ºåŒ–ç»ƒä¹ ")
print("=" * 50)

# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼–ç¨‹å®è·µ (20åˆ†é’Ÿ)
# ============================================================================

print("\nğŸ’» ç¬¬ä¸‰éƒ¨åˆ†ï¼šç¼–ç¨‹å®è·µ")
print("-" * 30)

# ----------------------------------------------------------------------------
# ä»»åŠ¡1ï¼šåŸºç¡€æ¢¯åº¦ä¸‹é™å®ç° (8åˆ†é’Ÿ)
# ----------------------------------------------------------------------------
print("\nğŸ“ ä»»åŠ¡1ï¼šåŸºç¡€æ¢¯åº¦ä¸‹é™å®ç°")

def simple_gradient_descent(start_point, learning_rate, num_iterations):
    """
    å®ç°ç®€å•çš„æ¢¯åº¦ä¸‹é™ç®—æ³•
    å‡½æ•°: f(x,y) = (x-3)Â² + (y-1)Â²
    ç›®æ ‡: æ‰¾åˆ°æœ€å°å€¼ç‚¹ (3, 1)
    
    å‚æ•°:
        start_point: èµ·å§‹ç‚¹ [x, y]
        learning_rate: å­¦ä¹ ç‡
        num_iterations: è¿­ä»£æ¬¡æ•°
    
    è¿”å›:
        æœ€ç»ˆç‚¹çš„åæ ‡ [x, y]
    """
    # TODO: ä½ çš„å®ç°
    # æç¤ºï¼š
    # 1. å®šä¹‰æŸå¤±å‡½æ•° f(x,y) = (x-3)Â² + (y-1)Â²
    # 2. è®¡ç®—æ¢¯åº¦ âˆ‡f = [2(x-3), 2(y-1)]
    # 3. æ›´æ–°å‚æ•° new_point = old_point - learning_rate * gradient
    # 4. é‡å¤è¿­ä»£
    
    current_point = np.array(start_point, dtype=float)
    
    for i in range(num_iterations):
        # åœ¨è¿™é‡Œå®ç°æ¢¯åº¦ä¸‹é™çš„ä¸€æ­¥
        pass
    
    return current_point.tolist()

# æµ‹è¯•ä½ çš„å®ç°
print("æµ‹è¯•åŸºç¡€æ¢¯åº¦ä¸‹é™:")
result = simple_gradient_descent([0, 0], 0.1, 20)
print(f"èµ·å§‹ç‚¹: [0, 0]")
print(f"æœ€ç»ˆç»“æœ: {result}")
print(f"æœŸæœ›ç»“æœ: [3, 1]")
print(f"è¯¯å·®: {abs(result[0] - 3) + abs(result[1] - 1):.6f}")

# ----------------------------------------------------------------------------
# ä»»åŠ¡2ï¼šä¸åŒå­¦ä¹ ç‡å¯¹æ¯” (6åˆ†é’Ÿ)
# ----------------------------------------------------------------------------
print("\nğŸ“ ä»»åŠ¡2ï¼šä¸åŒå­¦ä¹ ç‡å¯¹æ¯”")

def compare_learning_rates():
    """
    æµ‹è¯•ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ
    è§‚å¯Ÿæ”¶æ•›é€Ÿåº¦å’Œç¨³å®šæ€§
    """
    learning_rates = [0.01, 0.1, 0.5, 0.9]
    start_point = [0, 0]
    iterations = 50
    
    print("å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ:")
    print("èµ·å§‹ç‚¹:", start_point)
    print("è¿­ä»£æ¬¡æ•°:", iterations)
    print("-" * 40)
    
    for lr in learning_rates:
        # TODO: å®ç°å¯¹æ¯”å®éªŒ
        # 1. ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡è¿è¡Œæ¢¯åº¦ä¸‹é™
        # 2. è®°å½•æœ€ç»ˆç»“æœå’Œæ”¶æ•›æƒ…å†µ
        # 3. åˆ†æå­¦ä¹ ç‡çš„å½±å“
        
        result = simple_gradient_descent(start_point, lr, iterations)
        error = abs(result[0] - 3) + abs(result[1] - 1)
        
        print(f"å­¦ä¹ ç‡ {lr:4.2f}: ç»“æœ {result}, è¯¯å·® {error:.6f}")
    
    print("\nåˆ†æ:")
    print("- å­¦ä¹ ç‡å¤ªå°(0.01): æ”¶æ•›æ…¢")
    print("- å­¦ä¹ ç‡é€‚ä¸­(0.1): æ”¶æ•›å¿«ä¸”ç¨³å®š")
    print("- å­¦ä¹ ç‡è¾ƒå¤§(0.5): å¯èƒ½éœ‡è¡")
    print("- å­¦ä¹ ç‡è¿‡å¤§(0.9): å¯èƒ½ä¸æ”¶æ•›")

# è¿è¡Œå­¦ä¹ ç‡å¯¹æ¯”
compare_learning_rates()

# ----------------------------------------------------------------------------
# ä»»åŠ¡3ï¼šä¸åŒèµ·å§‹ç‚¹å¯¹æ¯” (6åˆ†é’Ÿ)
# ----------------------------------------------------------------------------
print("\nğŸ“ ä»»åŠ¡3ï¼šä¸åŒèµ·å§‹ç‚¹å¯¹æ¯”")

def compare_start_points():
    """
    æµ‹è¯•ä¸åŒèµ·å§‹ç‚¹çš„æ”¶æ•›ç»“æœ
    éªŒè¯æ¢¯åº¦ä¸‹é™çš„é²æ£’æ€§
    """
    start_points = [[0, 0], [5, 5], [-2, 3], [1, -1]]
    learning_rate = 0.1
    iterations = 30
    
    print("èµ·å§‹ç‚¹å¯¹æ¯”å®éªŒ:")
    print("å­¦ä¹ ç‡:", learning_rate)
    print("è¿­ä»£æ¬¡æ•°:", iterations)
    print("-" * 40)
    
    for start in start_points:
        # TODO: å®ç°å¯¹æ¯”å®éªŒ
        # 1. ä½¿ç”¨ä¸åŒèµ·å§‹ç‚¹è¿è¡Œæ¢¯åº¦ä¸‹é™
        # 2. è§‚å¯Ÿæ˜¯å¦éƒ½èƒ½æ”¶æ•›åˆ°åŒä¸€ç‚¹
        # 3. æ¯”è¾ƒæ”¶æ•›é€Ÿåº¦
        
        result = simple_gradient_descent(start, learning_rate, iterations)
        error = abs(result[0] - 3) + abs(result[1] - 1)
        
        print(f"èµ·å§‹ç‚¹ {start}: ç»“æœ {result}, è¯¯å·® {error:.6f}")
    
    print("\nåˆ†æ:")
    print("- å¯¹äºå‡¸å‡½æ•°ï¼Œä¸åŒèµ·å§‹ç‚¹éƒ½èƒ½æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ç‚¹")
    print("- è·ç¦»æœ€ä¼˜ç‚¹è¶Šè¿‘ï¼Œæ”¶æ•›è¶Šå¿«")
    print("- æ¢¯åº¦ä¸‹é™å¯¹èµ·å§‹ç‚¹é€‰æ‹©å…·æœ‰é²æ£’æ€§")

# è¿è¡Œèµ·å§‹ç‚¹å¯¹æ¯”
compare_start_points()

# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šå›¾å½¢åŒ–ç»ƒä¹  (5åˆ†é’Ÿ)
# ============================================================================

print("\nğŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šå›¾å½¢åŒ–ç»ƒä¹ ")
print("-" * 30)

def plot_function_and_derivative():
    """
    ç»˜åˆ¶å‡½æ•° f(x) = xÂ³ - 3xÂ² + 2x å’Œå…¶å¯¼æ•°
    è¦æ±‚ï¼š
    1. åˆ›å»º1è¡Œ2åˆ—çš„å­å›¾
    2. å·¦å›¾æ˜¾ç¤ºåŸå‡½æ•°
    3. å³å›¾æ˜¾ç¤ºå¯¼æ•°å‡½æ•°
    4. æ ‡è®°å¯¼æ•°ä¸º0çš„ç‚¹
    5. æ·»åŠ ç½‘æ ¼å’Œæ ‡ç­¾
    """
    
    def f(x):
        """åŸå‡½æ•° f(x) = xÂ³ - 3xÂ² + 2x"""
        return x**3 - 3*x**2 + 2*x
    
    def df_dx(x):
        """å¯¼æ•°å‡½æ•° f'(x) = 3xÂ² - 6x + 2"""
        return 3*x**2 - 6*x + 2
    
    # åˆ›å»ºæ•°æ®
    x = np.linspace(-1, 4, 1000)
    y = f(x)
    dy = df_dx(x)
    
    # TODO: å®ç°å›¾å½¢ç»˜åˆ¶
    # 1. åˆ›å»ºå­å›¾
    # 2. ç»˜åˆ¶åŸå‡½æ•°å’Œå¯¼æ•°å‡½æ•°
    # 3. æ‰¾åˆ°å¹¶æ ‡è®°å¯¼æ•°ä¸º0çš„ç‚¹
    # 4. æ·»åŠ ç½‘æ ¼ã€æ ‡ç­¾ã€æ ‡é¢˜
    
    # åˆ›å»ºå­å›¾
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å·¦å›¾ï¼šåŸå‡½æ•°
    ax1.plot(x, y, 'b-', linewidth=2, label='f(x) = xÂ³ - 3xÂ² + 2x')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlabel('x')
    ax1.set_ylabel('f(x)')
    ax1.set_title('åŸå‡½æ•°')
    ax1.legend()
    
    # å³å›¾ï¼šå¯¼æ•°å‡½æ•°
    ax2.plot(x, dy, 'r-', linewidth=2, label="f'(x) = 3xÂ² - 6x + 2")
    ax2.axhline(y=0, color='k', linestyle='--', alpha=0.5)
    ax2.grid(True, alpha=0.3)
    ax2.set_xlabel('x')
    ax2.set_ylabel("f'(x)")
    ax2.set_title('å¯¼æ•°å‡½æ•°')
    ax2.legend()
    
    # æ‰¾åˆ°å¯¼æ•°ä¸º0çš„ç‚¹
    # è§£æ–¹ç¨‹ 3xÂ² - 6x + 2 = 0
    coeffs = [3, -6, 2]  # 3xÂ² - 6x + 2 = 0
    roots = np.roots(coeffs)
    
    # æ ‡è®°æå€¼ç‚¹
    for root in roots:
        if -1 <= root <= 4:
            # åœ¨å·¦å›¾æ ‡è®°
            ax1.plot(root, f(root), 'ro', markersize=8)
            ax1.annotate(f'æå€¼ç‚¹\n({root:.2f}, {f(root):.2f})', 
                        xy=(root, f(root)), xytext=(10, 10),
                        textcoords='offset points', fontsize=9)
            
            # åœ¨å³å›¾æ ‡è®°
            ax2.plot(root, 0, 'ro', markersize=8)
            ax2.annotate(f"f'({root:.2f}) = 0", 
                        xy=(root, 0), xytext=(10, 10),
                        textcoords='offset points', fontsize=9)
    
    plt.tight_layout()
    plt.show()
    
    print(f"å¯¼æ•°ä¸º0çš„ç‚¹: {roots}")
    print(f"å¯¹åº”çš„å‡½æ•°å€¼: {[f(root) for root in roots]}")

# è¿è¡Œå›¾å½¢åŒ–ç»ƒä¹ 
print("\nğŸ“ˆ ç»˜åˆ¶å‡½æ•°å’Œå¯¼æ•°å¯¹æ¯”å›¾:")
plot_function_and_derivative()

# ============================================================================
# é¢å¤–ç»ƒä¹ ï¼šæ¢¯åº¦ä¸‹é™å¯è§†åŒ–
# ============================================================================

def visualize_gradient_descent():
    """
    å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
    """
    def loss_function(x, y):
        return (x - 3)**2 + (y - 1)**2
    
    def gradient(x, y):
        return np.array([2*(x - 3), 2*(y - 1)])
    
    # æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
    start_point = np.array([0.0, 0.0])
    learning_rate = 0.1
    num_iterations = 20
    
    path = [start_point.copy()]
    current_point = start_point.copy()
    
    for i in range(num_iterations):
        grad = gradient(current_point[0], current_point[1])
        current_point = current_point - learning_rate * grad
        path.append(current_point.copy())
    
    path = np.array(path)
    
    # ç»˜åˆ¶ç­‰é«˜çº¿å’Œè·¯å¾„
    x = np.linspace(-1, 4, 100)
    y = np.linspace(-1, 3, 100)
    X, Y = np.meshgrid(x, y)
    Z = loss_function(X, Y)
    
    plt.figure(figsize=(10, 8))
    
    # ç»˜åˆ¶ç­‰é«˜çº¿
    contour = plt.contour(X, Y, Z, levels=20, alpha=0.6)
    plt.clabel(contour, inline=True, fontsize=8)
    
    # ç»˜åˆ¶æ¢¯åº¦ä¸‹é™è·¯å¾„
    plt.plot(path[:, 0], path[:, 1], 'ro-', linewidth=2, markersize=4, 
             label='æ¢¯åº¦ä¸‹é™è·¯å¾„')
    plt.plot(path[0, 0], path[0, 1], 'go', markersize=10, label='èµ·å§‹ç‚¹')
    plt.plot(path[-1, 0], path[-1, 1], 'bs', markersize=10, label='ç»ˆç‚¹')
    plt.plot(3, 1, 'r*', markersize=15, label='çœŸå®æœ€ä¼˜ç‚¹(3,1)')
    
    plt.xlabel('x')
    plt.ylabel('y')
    plt.title('æ¢¯åº¦ä¸‹é™å¯è§†åŒ–')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.axis('equal')
    plt.show()
    
    print(f"æ¢¯åº¦ä¸‹é™è·¯å¾„:")
    for i, point in enumerate(path[:6]):  # åªæ˜¾ç¤ºå‰6æ­¥
        print(f"æ­¥éª¤ {i}: ({point[0]:.3f}, {point[1]:.3f})")

print("\nğŸ¨ æ¢¯åº¦ä¸‹é™å¯è§†åŒ–:")
visualize_gradient_descent()

# ============================================================================
# å­¦ä¹ æ€»ç»“
# ============================================================================

print("\n" + "=" * 50)
print("ğŸ“ å­¦ä¹ æ€»ç»“")
print("=" * 50)

print("""
ä»Šæ—¥ç»ƒä¹ é‡ç‚¹ï¼š
1. âœ… å®ç°äº†åŸºç¡€æ¢¯åº¦ä¸‹é™ç®—æ³•
2. âœ… å¯¹æ¯”äº†ä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ
3. âœ… æµ‹è¯•äº†ä¸åŒèµ·å§‹ç‚¹çš„æ”¶æ•›æ€§
4. âœ… ç»˜åˆ¶äº†å‡½æ•°å’Œå¯¼æ•°çš„å¯¹æ¯”å›¾
5. âœ… å¯è§†åŒ–äº†æ¢¯åº¦ä¸‹é™è¿‡ç¨‹

å…³é”®æ”¶è·ï¼š
- ç†è§£äº†æ¢¯åº¦ä¸‹é™çš„å®Œæ•´å®ç°æµç¨‹
- ä½“éªŒäº†å­¦ä¹ ç‡å¯¹æ”¶æ•›çš„å½±å“
- è§‚å¯Ÿäº†æ¢¯åº¦ä¸‹é™çš„å‡ ä½•æ„ä¹‰
- æé«˜äº†matplotlibç»˜å›¾æŠ€èƒ½

ä¸‹ä¸€æ­¥ï¼š
- å¦‚æœæŒæ¡è‰¯å¥½ï¼Œå¯ä»¥å­¦ä¹ ç¬¬7å¤©å†…å®¹
- å¦‚æœè¿˜æœ‰å›°éš¾ï¼Œç»§ç»­ç»ƒä¹ ç›¸å…³å†…å®¹
- é‡ç‚¹å…³æ³¨ç†è®ºä¸å®è·µçš„ç»“åˆ
""")

print("\nğŸ¯ æ­å–œå®Œæˆç¬¬6å¤©å¼ºåŒ–ç»ƒä¹ ï¼") 