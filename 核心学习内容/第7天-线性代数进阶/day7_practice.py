"""
ç¬¬7å¤©ç»ƒä¹  - çº¿æ€§ä»£æ•°è¿›é˜¶
ç›®æ ‡ï¼šé€šè¿‡ç¼–ç¨‹ç»ƒä¹ å·©å›ºç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡å’ŒPCAçš„ç†è§£
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.datasets import make_classification
import warnings
warnings.filterwarnings('ignore')

# æ™ºèƒ½ä¸­æ–‡å­—ä½“è®¾ç½®
import matplotlib.font_manager as fm

def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œè‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿå¯ç”¨å­—ä½“"""
    chinese_fonts = [
        'PingFang SC',      # macOS è‹¹æ–¹
        'Helvetica',        # macOS é€šç”¨
        'STHeiti',          # macOS åæ–‡é»‘ä½“
        'Arial Unicode MS', # macOS/Windows
        'SimHei',          # Windows é»‘ä½“
        'Microsoft YaHei', # Windows å¾®è½¯é›…é»‘
        'DejaVu Sans'      # å¤‡ç”¨
    ]
    
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font]
            plt.rcParams['axes.unicode_minus'] = False
            print(f"âœ… ä½¿ç”¨å­—ä½“: {font}")
            return font
    
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
    plt.rcParams['axes.unicode_minus'] = False
    return 'DejaVu Sans'

current_font = setup_chinese_font()
use_chinese_labels = current_font not in ['DejaVu Sans']

print("ğŸ¯ ç¬¬7å¤©ç»ƒä¹  - çº¿æ€§ä»£æ•°è¿›é˜¶")
print("="*50)

# ç»ƒä¹ 1ï¼šç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„è®¡ç®—
print("\nğŸ“š ç»ƒä¹ 1ï¼šç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡")
print("-" * 30)

def practice_eigenvalues():
    """ç»ƒä¹ è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡"""
    # åˆ›å»ºä¸€ä¸ªå¯¹ç§°çŸ©é˜µï¼ˆæ›´å®¹æ˜“ç†è§£ï¼‰
    A = np.array([[4, 2], 
                  [2, 1]])
    
    print("çŸ©é˜µ A:")
    print(A)
    
    # è®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    eigenvalues, eigenvectors = np.linalg.eig(A)
    
    print(f"\nç‰¹å¾å€¼: {eigenvalues}")
    print(f"ç‰¹å¾å‘é‡:\n{eigenvectors}")
    
    # éªŒè¯ Av = Î»v
    for i in range(len(eigenvalues)):
        v = eigenvectors[:, i]
        Î» = eigenvalues[i]
        
        print(f"\néªŒè¯ç‰¹å¾å‘é‡ {i+1}:")
        print(f"Av = {A @ v}")
        print(f"Î»v = {Î» * v}")
        print(f"è¯¯å·®: {np.linalg.norm(A @ v - Î» * v):.10f}")
    
    return eigenvalues, eigenvectors

eigenvalues, eigenvectors = practice_eigenvalues()


# ç»ƒä¹ 2ï¼šæ‰‹åŠ¨å®ç°PCA
print("\nğŸ“š ç»ƒä¹ 2ï¼šæ‰‹åŠ¨å®ç°PCA")
print("-" * 30)

def manual_pca(X, n_components=2):
    """æ‰‹åŠ¨å®ç°PCAç®—æ³•"""
    # æ­¥éª¤1ï¼šæ•°æ®ä¸­å¿ƒåŒ–
    X_centered = X - np.mean(X, axis=0)
    
    # æ­¥éª¤2ï¼šè®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov_matrix = np.cov(X_centered.T)
    
    # æ­¥éª¤3ï¼šè®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    
    # æ­¥éª¤4ï¼šæŒ‰ç‰¹å¾å€¼å¤§å°æ’åº
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues_sorted = eigenvalues[idx]
    eigenvectors_sorted = eigenvectors[:, idx]
    
    # æ­¥éª¤5ï¼šé€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
    components = eigenvectors_sorted[:, :n_components]
    
    # æ­¥éª¤6ï¼šæŠ•å½±æ•°æ®
    X_pca = X_centered @ components
    
    # è®¡ç®—æ–¹å·®è§£é‡Šæ¯”ä¾‹
    explained_variance_ratio = eigenvalues_sorted / np.sum(eigenvalues_sorted)
    
    return X_pca, components, explained_variance_ratio[:n_components]

# ç”Ÿæˆæµ‹è¯•æ•°æ®
np.random.seed(42)
X_test = np.random.randn(100, 4)  # 100ä¸ªæ ·æœ¬ï¼Œ4ä¸ªç‰¹å¾
# è®©ç‰¹å¾ä¹‹é—´æœ‰ä¸€å®šçš„ç›¸å…³æ€§
X_test[:, 1] = X_test[:, 0] + 0.5 * np.random.randn(100)
X_test[:, 2] = X_test[:, 0] - 0.3 * X_test[:, 1] + 0.2 * np.random.randn(100)

print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {X_test.shape}")

# ä½¿ç”¨æ‰‹åŠ¨å®ç°çš„PCA
X_pca_manual, components_manual, explained_ratio_manual = manual_pca(X_test, n_components=2)

# ä½¿ç”¨sklearnçš„PCAå¯¹æ¯”
pca_sklearn = PCA(n_components=2)
X_pca_sklearn = pca_sklearn.fit_transform(X_test)

print(f"\næ‰‹åŠ¨PCAç»“æœå½¢çŠ¶: {X_pca_manual.shape}")
print(f"sklearn PCAç»“æœå½¢çŠ¶: {X_pca_sklearn.shape}")
print(f"æœ€å¤§å·®å¼‚: {np.max(np.abs(X_pca_manual - X_pca_sklearn)):.10f}")
print(f"æ‰‹åŠ¨PCAæ–¹å·®è§£é‡Šæ¯”ä¾‹: {explained_ratio_manual}")
print(f"sklearn PCAæ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca_sklearn.explained_variance_ratio_}")


# ç»ƒä¹ 3ï¼šPCAé™ç»´æ•ˆæœå¯è§†åŒ–
print("\nğŸ“š ç»ƒä¹ 3ï¼šPCAé™ç»´æ•ˆæœå¯è§†åŒ–")
print("-" * 30)

def visualize_pca_effect():
    """å¯è§†åŒ–PCAé™ç»´æ•ˆæœ"""
    # ç”Ÿæˆå…·æœ‰æ˜æ˜¾ç»“æ„çš„æ•°æ®
    X, y = make_classification(n_samples=300, n_features=4, n_redundant=0, 
                              n_informative=2, n_clusters_per_class=1, 
                              random_state=42)
    
    # åº”ç”¨PCA
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    
    # åˆ›å»ºå›¾å½¢
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # åŸå§‹æ•°æ®çš„å‰ä¸¤ä¸ªç‰¹å¾
    scatter1 = axes[0, 0].scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)
    if use_chinese_labels:
        axes[0, 0].set_title('åŸå§‹æ•°æ® (ç‰¹å¾1 vs ç‰¹å¾2)')
        axes[0, 0].set_xlabel('ç‰¹å¾1')
        axes[0, 0].set_ylabel('ç‰¹å¾2')
    else:
        axes[0, 0].set_title('Original Data (Feature 1 vs 2)')
        axes[0, 0].set_xlabel('Feature 1')
        axes[0, 0].set_ylabel('Feature 2')
    axes[0, 0].grid(True, alpha=0.3)
    
    # åŸå§‹æ•°æ®çš„åä¸¤ä¸ªç‰¹å¾
    scatter2 = axes[0, 1].scatter(X[:, 2], X[:, 3], c=y, cmap='viridis', alpha=0.7)
    if use_chinese_labels:
        axes[0, 1].set_title('åŸå§‹æ•°æ® (ç‰¹å¾3 vs ç‰¹å¾4)')
        axes[0, 1].set_xlabel('ç‰¹å¾3')
        axes[0, 1].set_ylabel('ç‰¹å¾4')
    else:
        axes[0, 1].set_title('Original Data (Feature 3 vs 4)')
        axes[0, 1].set_xlabel('Feature 3')
        axes[0, 1].set_ylabel('Feature 4')
    axes[0, 1].grid(True, alpha=0.3)
    
    # PCAé™ç»´åçš„æ•°æ®
    scatter3 = axes[1, 0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)
    axes[1, 0].set_title(f'PCAé™ç»´å\n(è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%})')
    axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    axes[1, 0].grid(True, alpha=0.3)
    
    # æ–¹å·®è§£é‡Šæ¯”ä¾‹
    axes[1, 1].bar(['PC1', 'PC2'], pca.explained_variance_ratio_)
    axes[1, 1].set_title('å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[1, 1].set_ylabel('æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    print(f"åŸå§‹æ•°æ®ç»´åº¦: {X.shape[1]}")
    print(f"PCAåç»´åº¦: {X_pca.shape[1]}")
    print(f"æ€»æ–¹å·®è§£é‡Šæ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.2%}")
    
    return X, X_pca, pca

X_original, X_pca_viz, pca_viz = visualize_pca_effect()


# ç»ƒä¹ 4ï¼šä¸åŒä¸»æˆåˆ†æ•°é‡çš„æ•ˆæœ
print("\nğŸ“š ç»ƒä¹ 4ï¼šä¸åŒä¸»æˆåˆ†æ•°é‡çš„æ•ˆæœ")
print("-" * 30)

def compare_pca_components():
    """æ¯”è¾ƒä¸åŒä¸»æˆåˆ†æ•°é‡çš„é™ç»´æ•ˆæœ"""
    # ç”Ÿæˆé«˜ç»´æ•°æ®
    X, y = make_classification(n_samples=200, n_features=10, n_redundant=5, 
                              n_informative=5, random_state=42)
    
    # æµ‹è¯•ä¸åŒçš„ä¸»æˆåˆ†æ•°é‡
    n_components_list = [1, 2, 3, 5, 7, 10]
    
    print("ä¸»æˆåˆ†æ•°é‡ | æ–¹å·®è§£é‡Šæ¯”ä¾‹ | ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹")
    print("-" * 50)
    
    explained_ratios = []
    cumulative_ratios = []
    
    for n_comp in n_components_list:
        pca = PCA(n_components=n_comp)
        pca.fit(X)
        
        total_explained = pca.explained_variance_ratio_.sum()
        explained_ratios.append(total_explained)
        
        print(f"    {n_comp:2d}       |     {total_explained:.2%}      |        {total_explained:.2%}")
    
    # å¯è§†åŒ–
    plt.figure(figsize=(10, 6))
    plt.plot(n_components_list, explained_ratios, 'o-', linewidth=2, markersize=8)
    plt.axhline(y=0.9, color='r', linestyle='--', label='90%é˜ˆå€¼')
    plt.axhline(y=0.95, color='g', linestyle='--', label='95%é˜ˆå€¼')
    plt.xlabel('ä¸»æˆåˆ†æ•°é‡')
    plt.ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    plt.title('ä¸åŒä¸»æˆåˆ†æ•°é‡çš„æ–¹å·®è§£é‡Šæ•ˆæœ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    
    # æ‰¾åˆ°è¾¾åˆ°90%æ–¹å·®è§£é‡Šæ‰€éœ€çš„ä¸»æˆåˆ†æ•°é‡
    for i, ratio in enumerate(explained_ratios):
        if ratio >= 0.9:
            print(f"\nğŸ¯ è¾¾åˆ°90%æ–¹å·®è§£é‡Šéœ€è¦ {n_components_list[i]} ä¸ªä¸»æˆåˆ†")
            break
    
    return explained_ratios

explained_ratios = compare_pca_components()


# ç»ƒä¹ 5ï¼šPCAçš„å®é™…åº”ç”¨æ¡ˆä¾‹
print("\nğŸ“š ç»ƒä¹ 5ï¼šPCAå®é™…åº”ç”¨æ¡ˆä¾‹")
print("-" * 30)

def pca_application_case():
    """PCAåœ¨å®é™…é—®é¢˜ä¸­çš„åº”ç”¨æ¡ˆä¾‹"""
    # æ¨¡æ‹Ÿä¸€ä¸ªå®¢æˆ·æ•°æ®é›†
    np.random.seed(42)
    n_customers = 1000
    
    # ç”Ÿæˆå®¢æˆ·ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿç”µå•†æ•°æ®ï¼‰
    # å¹´é¾„ã€æ”¶å…¥ã€è´­ä¹°é¢‘ç‡ã€å¹³å‡è®¢å•é‡‘é¢ã€ç½‘ç«™åœç•™æ—¶é—´ã€è¯„åˆ†ç­‰
    age = np.random.normal(35, 10, n_customers)
    income = np.random.normal(50000, 15000, n_customers)
    purchase_freq = np.random.poisson(12, n_customers)
    avg_order = np.random.normal(150, 50, n_customers)
    time_on_site = np.random.exponential(20, n_customers)
    rating = np.random.normal(4.2, 0.8, n_customers)
    
    # æ·»åŠ ä¸€äº›ç›¸å…³æ€§
    purchase_freq = purchase_freq + 0.3 * (income / 10000) + np.random.normal(0, 2, n_customers)
    avg_order = avg_order + 0.5 * (income / 1000) + np.random.normal(0, 20, n_customers)
    
    # ç»„åˆç‰¹å¾
    customer_features = np.column_stack([age, income, purchase_freq, avg_order, time_on_site, rating])
    feature_names = ['å¹´é¾„', 'æ”¶å…¥', 'è´­ä¹°é¢‘ç‡', 'å¹³å‡è®¢å•é‡‘é¢', 'ç½‘ç«™åœç•™æ—¶é—´', 'è¯„åˆ†']
    
    print("å®¢æˆ·æ•°æ®é›†:")
    print(f"æ ·æœ¬æ•°: {customer_features.shape[0]}")
    print(f"ç‰¹å¾æ•°: {customer_features.shape[1]}")
    print(f"ç‰¹å¾åç§°: {feature_names}")
    
    # æ ‡å‡†åŒ–æ•°æ®
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    customer_features_scaled = scaler.fit_transform(customer_features)
    
    # åº”ç”¨PCA
    pca = PCA()
    customer_pca = pca.fit_transform(customer_features_scaled)
    
    # åˆ†æç»“æœ
    print(f"\nå„ä¸»æˆåˆ†çš„æ–¹å·®è§£é‡Šæ¯”ä¾‹:")
    for i, ratio in enumerate(pca.explained_variance_ratio_):
        print(f"PC{i+1}: {ratio:.2%}")
    
    # ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹
    cumulative_ratio = np.cumsum(pca.explained_variance_ratio_)
    print(f"\nç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹:")
    for i, ratio in enumerate(cumulative_ratio):
        print(f"å‰{i+1}ä¸ªä¸»æˆåˆ†: {ratio:.2%}")
    
    # å¯è§†åŒ–
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šæ¯”ä¾‹
    axes[0, 0].bar(range(1, len(pca.explained_variance_ratio_) + 1), 
                   pca.explained_variance_ratio_)
    axes[0, 0].set_title('å„ä¸»æˆåˆ†æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[0, 0].set_xlabel('ä¸»æˆåˆ†')
    axes[0, 0].set_ylabel('æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[0, 0].grid(True, alpha=0.3)
    
    # ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹
    axes[0, 1].plot(range(1, len(cumulative_ratio) + 1), cumulative_ratio, 'o-')
    axes[0, 1].axhline(y=0.8, color='r', linestyle='--', label='80%é˜ˆå€¼')
    axes[0, 1].axhline(y=0.9, color='g', linestyle='--', label='90%é˜ˆå€¼')
    axes[0, 1].set_title('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[0, 1].set_xlabel('ä¸»æˆåˆ†æ•°é‡')
    axes[0, 1].set_ylabel('ç´¯ç§¯æ–¹å·®è§£é‡Šæ¯”ä¾‹')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # å‰ä¸¤ä¸ªä¸»æˆåˆ†çš„æ•°æ®åˆ†å¸ƒ
    axes[1, 0].scatter(customer_pca[:, 0], customer_pca[:, 1], alpha=0.6)
    axes[1, 0].set_title('å‰ä¸¤ä¸ªä¸»æˆåˆ†çš„æ•°æ®åˆ†å¸ƒ')
    axes[1, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')
    axes[1, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')
    axes[1, 0].grid(True, alpha=0.3)
    
    # ä¸»æˆåˆ†è½½è·å›¾ï¼ˆç‰¹å¾è´¡çŒ®ï¼‰
    components_df = pca.components_[:2, :]  # å‰ä¸¤ä¸ªä¸»æˆåˆ†
    axes[1, 1].imshow(components_df, cmap='RdBu', aspect='auto')
    axes[1, 1].set_title('ä¸»æˆåˆ†è½½è·å›¾')
    axes[1, 1].set_xlabel('åŸå§‹ç‰¹å¾')
    axes[1, 1].set_ylabel('ä¸»æˆåˆ†')
    axes[1, 1].set_xticks(range(len(feature_names)))
    axes[1, 1].set_xticklabels(feature_names, rotation=45)
    axes[1, 1].set_yticks([0, 1])
    axes[1, 1].set_yticklabels(['PC1', 'PC2'])
    
    plt.tight_layout()
    plt.show()
    
    # é™ç»´å»ºè®®
    n_components_80 = np.argmax(cumulative_ratio >= 0.8) + 1
    n_components_90 = np.argmax(cumulative_ratio >= 0.9) + 1
    
    print(f"\nğŸ“Š é™ç»´å»ºè®®:")
    print(f"ä¿ç•™80%æ–¹å·®: ä½¿ç”¨ {n_components_80} ä¸ªä¸»æˆåˆ†")
    print(f"ä¿ç•™90%æ–¹å·®: ä½¿ç”¨ {n_components_90} ä¸ªä¸»æˆåˆ†")
    print(f"é™ç»´æ•ˆæœ: ä» {len(feature_names)} ç»´é™åˆ° {n_components_80} ç»´")
    
    return customer_pca, pca

customer_pca, pca_customer = pca_application_case()

print("\nğŸ¯ ç»ƒä¹ å®Œæˆï¼")
print("="*50)
print("âœ… ä½ å·²ç»å®Œæˆäº†æ‰€æœ‰çº¿æ€§ä»£æ•°è¿›é˜¶ç»ƒä¹ ")
print("âœ… ç†è§£äº†ç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡çš„è®¡ç®—å’Œåº”ç”¨")
print("âœ… æŒæ¡äº†PCAçš„åŸç†å’Œå®ç°")
print("âœ… å­¦ä¼šäº†åˆ†æå’Œé€‰æ‹©åˆé€‚çš„ä¸»æˆåˆ†æ•°é‡")
print("âœ… äº†è§£äº†PCAåœ¨å®é™…é—®é¢˜ä¸­çš„åº”ç”¨")
print("\nğŸš€ ç°åœ¨ä½ å¯ä»¥å¼€å§‹æ›´é«˜çº§çš„æœºå™¨å­¦ä¹ ç®—æ³•å­¦ä¹ äº†ï¼") 