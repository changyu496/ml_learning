#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬3å¤©å’Œç¬¬4å¤©ç»¼åˆç¼–ç¨‹ç»ƒä¹ 
åŒ…å«å‘é‡åŸºç¡€ + æ¦‚ç‡ç»Ÿè®¡åº”ç”¨

ä½œè€…ï¼šå¤§æ¨¡å‹è½¬å‹å­¦ä¹ 
æ—¥æœŸï¼šç¬¬3-4å¤©
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

print("ğŸ¯ ç¬¬3-4å¤©ç»¼åˆç¼–ç¨‹ç»ƒä¹ ")
print("="*50)
print("åŒ…å«ï¼šå‘é‡åŸºç¡€ + æ¦‚ç‡ç»Ÿè®¡åº”ç”¨")
print("="*50)

# ============================================================================
# ç¬¬3å¤©ï¼šå‘é‡åŸºç¡€ç»ƒä¹ 
# ============================================================================

print("\nğŸ“š ç¬¬3å¤©ï¼šå‘é‡åŸºç¡€ç»ƒä¹ ")
print("-" * 30)

# ç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ
print("\nğŸ”¢ ç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ")
print("ä»»åŠ¡ï¼šå®Œæˆä»¥ä¸‹å‘é‡è¿ç®—")

def exercise_1_vector_operations():
    """
    ç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ")
    
    # åˆ›å»ºä¸¤ä¸ªå‘é‡
    vector_a = np.array([1, 2, 3, 4, 5])
    vector_b = np.array([2, 4, 6, 8, 10])
    
    print(f"å‘é‡A: {vector_a}")
    print(f"å‘é‡B: {vector_b}")
    
    # TODO 1.1: è®¡ç®—å‘é‡Açš„L2èŒƒæ•°
    # æç¤ºï¼šä½¿ç”¨ np.linalg.norm() æˆ– np.sqrt(np.sum(vector_a**2))
    l2_norm_a = np.linalg.norm(vector_a)  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"å‘é‡Açš„L2èŒƒæ•°: {l2_norm_a}")
    
    # TODO 1.2: è®¡ç®—å‘é‡Aå’Œå‘é‡Bçš„ç‚¹ç§¯
    # æç¤ºï¼šä½¿ç”¨ np.dot() æˆ– np.sum(vector_a * vector_b)
    dot_product = np.dot(vector_a, vector_b)  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"å‘é‡Aå’ŒBçš„ç‚¹ç§¯: {dot_product}")
    
    # TODO 1.3: è®¡ç®—å‘é‡Aå’Œå‘é‡Bçš„ä½™å¼¦ç›¸ä¼¼åº¦
    # æç¤ºï¼šcos_sim = dot_product / (norm_a * norm_b)
    cosine_similarity = dot_product / (l2_norm_a * np.linalg.norm(vector_b))  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"ä½™å¼¦ç›¸ä¼¼åº¦: {cosine_similarity}")
    
    # TODO 1.4: è®¡ç®—å‘é‡Aå’Œå‘é‡Bçš„æ¬§å‡ é‡Œå¾—è·ç¦»
    # æç¤ºï¼šä½¿ç”¨ np.linalg.norm(vector_a - vector_b)
    euclidean_distance = np.linalg.norm(vector_a - vector_b)  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"æ¬§å‡ é‡Œå¾—è·ç¦»: {euclidean_distance}")
    
    return {
        'l2_norm_a': l2_norm_a,
        'dot_product': dot_product,
        'cosine_similarity': cosine_similarity,
        'euclidean_distance': euclidean_distance
    }

# ç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°
print("\nğŸ¯ ç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°")
print("ä»»åŠ¡ï¼šå®ç°åŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„æ¨èç³»ç»Ÿ")

def exercise_2_recommendation_system():
    """
    ç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°")
    
    # ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ (ç”¨æˆ·æ•°=5, ç‰©å“æ•°=4)
    # 0è¡¨ç¤ºæœªè¯„åˆ†
    ratings_matrix = np.array([
        [5, 3, 0, 1],  # ç”¨æˆ·1
        [4, 0, 0, 1],  # ç”¨æˆ·2
        [1, 1, 0, 5],  # ç”¨æˆ·3
        [1, 0, 0, 4],  # ç”¨æˆ·4
        [0, 1, 5, 4]   # ç”¨æˆ·5
    ])
    
    print("ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ:")
    print(ratings_matrix)
    
    # TODO 2.1: è®¡ç®—ç”¨æˆ·1å’Œç”¨æˆ·2çš„ä½™å¼¦ç›¸ä¼¼åº¦
    # æç¤ºï¼šåªè€ƒè™‘ä¸¤ä¸ªç”¨æˆ·éƒ½è¯„è¿‡åˆ†çš„ç‰©å“
    user1_ratings = ratings_matrix[0]  # ç”¨æˆ·1çš„è¯„åˆ†
    user2_ratings = ratings_matrix[1]  # ç”¨æˆ·2çš„è¯„åˆ†
    
    # æ‰¾åˆ°ä¸¤ä¸ªç”¨æˆ·éƒ½è¯„è¿‡åˆ†çš„ç‰©å“ç´¢å¼•
    common_items = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"å…±åŒè¯„åˆ†çš„ç‰©å“ç´¢å¼•: {common_items}")
    
    # æå–å…±åŒè¯„åˆ†çš„å‘é‡
    user1_common = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    user2_common = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    similarity_1_2 = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"ç”¨æˆ·1å’Œç”¨æˆ·2çš„ä½™å¼¦ç›¸ä¼¼åº¦: {similarity_1_2}")
    
    # TODO 2.2: ä¸ºç”¨æˆ·1æ¨èç‰©å“
    # æ‰¾åˆ°ç”¨æˆ·1æœªè¯„åˆ†çš„ç‰©å“
    user1_unrated = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
    print(f"ç”¨æˆ·1æœªè¯„åˆ†çš„ç‰©å“: {user1_unrated}")
    
    # è®¡ç®—ç”¨æˆ·1ä¸å…¶ä»–ç”¨æˆ·çš„ç›¸ä¼¼åº¦
    similarities = []
    for i in range(1, len(ratings_matrix)):  # è·³è¿‡ç”¨æˆ·1è‡ªå·±
        # è®¡ç®—ç”¨æˆ·1ä¸ç”¨æˆ·içš„ç›¸ä¼¼åº¦
        similarity = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
        similarities.append(similarity)
    
    print(f"ç”¨æˆ·1ä¸å…¶ä»–ç”¨æˆ·çš„ç›¸ä¼¼åº¦: {similarities}")
    
    # åŸºäºç›¸ä¼¼åº¦é¢„æµ‹è¯„åˆ†
    # å¯¹äºç”¨æˆ·1æœªè¯„åˆ†çš„æ¯ä¸ªç‰©å“ï¼Œè®¡ç®—é¢„æµ‹è¯„åˆ†
    predictions = {}
    for item_idx in user1_unrated:
        # è®¡ç®—é¢„æµ‹è¯„åˆ†
        # å…¬å¼ï¼špred = sum(similarity * rating) / sum(similarity)
        predicted_rating = None  # è¯·å®Œæˆè¿™ä¸ªè®¡ç®—
        predictions[item_idx] = predicted_rating
    
    print(f"ç”¨æˆ·1çš„é¢„æµ‹è¯„åˆ†: {predictions}")
    
    return {
        'similarity_1_2': similarity_1_2,
        'user1_unrated': user1_unrated,
        'similarities': similarities,
        'predictions': predictions
    }

# ç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–
print("\nğŸ“Š ç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–")
print("ä»»åŠ¡ï¼šåˆ›å»ºå‘é‡å¯è§†åŒ–å›¾è¡¨")

def exercise_3_vector_visualization():
    """
    ç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–")
    
    # åˆ›å»ºå¤šä¸ªå‘é‡
    vectors = np.array([
        [1, 2],   # å‘é‡1
        [3, 1],   # å‘é‡2
        [2, 3],   # å‘é‡3
        [-1, 2],  # å‘é‡4
        [0, 3]    # å‘é‡5
    ])
    
    print("å‘é‡æ•°æ®:")
    print(vectors)
    
    # TODO 3.1: åˆ›å»ºå‘é‡æ•£ç‚¹å›¾
    # åœ¨2Då¹³é¢ä¸Šç»˜åˆ¶è¿™äº›å‘é‡
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    # å·¦å›¾ï¼šå‘é‡æ•£ç‚¹å›¾
    # è¯·å®Œæˆæ•£ç‚¹å›¾çš„ç»˜åˆ¶
    # æç¤ºï¼šä½¿ç”¨ ax1.scatter() ç»˜åˆ¶ç‚¹ï¼Œä½¿ç”¨ ax1.arrow() ç»˜åˆ¶ç®­å¤´
    
    ax1.set_xlim(-2, 4)
    ax1.set_ylim(-1, 4)
    ax1.set_xlabel('Xåæ ‡')
    ax1.set_ylabel('Yåæ ‡')
    ax1.set_title('å‘é‡æ•£ç‚¹å›¾')
    ax1.grid(True, alpha=0.3)
    ax1.axhline(y=0, color='black', linestyle='-', alpha=0.3)
    ax1.axvline(x=0, color='black', linestyle='-', alpha=0.3)
    
    # TODO 3.2: åˆ›å»ºå‘é‡ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
    # è®¡ç®—æ‰€æœ‰å‘é‡ä¸¤ä¸¤ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    n_vectors = len(vectors)
    similarity_matrix = np.zeros((n_vectors, n_vectors))
    
    # è¯·å®Œæˆç›¸ä¼¼åº¦çŸ©é˜µçš„è®¡ç®—
    # æç¤ºï¼šä½¿ç”¨åŒé‡å¾ªç¯è®¡ç®—æ¯å¯¹å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    im = ax2.imshow(similarity_matrix, cmap='coolwarm', vmin=-1, vmax=1)
    ax2.set_xticks(range(n_vectors))
    ax2.set_yticks(range(n_vectors))
    ax2.set_xticklabels([f'å‘é‡{i+1}' for i in range(n_vectors)])
    ax2.set_yticklabels([f'å‘é‡{i+1}' for i in range(n_vectors)])
    ax2.set_title('å‘é‡ç›¸ä¼¼åº¦çƒ­åŠ›å›¾')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i in range(n_vectors):
        for j in range(n_vectors):
            text = ax2.text(j, i, f'{similarity_matrix[i, j]:.2f}',
                           ha="center", va="center", color="black")
    
    plt.colorbar(im, ax=ax2)
    plt.tight_layout()
    plt.show()
    
    return {
        'vectors': vectors,
        'similarity_matrix': similarity_matrix
    }

# ============================================================================
# ç¬¬4å¤©ï¼šæ¦‚ç‡ç»Ÿè®¡ç»ƒä¹ 
# ============================================================================

print("\nğŸ“š ç¬¬4å¤©ï¼šæ¦‚ç‡ç»Ÿè®¡ç»ƒä¹ ")
print("-" * 30)

# ç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ
print("\nğŸ“Š ç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ")
print("ä»»åŠ¡ï¼šåˆ†ææ¨¡æ‹Ÿç”µå•†æ•°æ®")

def exercise_4_exploratory_analysis():
    """
    ç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿç”µå•†æ•°æ®
    np.random.seed(42)
    n_users = 1000
    
    # ç”¨æˆ·å¹´é¾„ï¼ˆæ­£æ€åˆ†å¸ƒï¼‰
    ages = np.random.normal(35, 10, n_users)
    ages = np.clip(ages, 18, 70)
    
    # æ¶ˆè´¹é‡‘é¢ï¼ˆå¯¹æ•°æ­£æ€åˆ†å¸ƒï¼‰
    spending = np.random.lognormal(4, 0.8, n_users)
    
    # è´­ä¹°é¢‘æ¬¡ï¼ˆæ³Šæ¾åˆ†å¸ƒï¼‰
    purchase_frequency = np.random.poisson(5, n_users)
    
    # ç”¨æˆ·æ»¡æ„åº¦ï¼ˆ1-5åˆ†ï¼‰
    satisfaction = np.random.choice([1,2,3,4,5], n_users, p=[0.05, 0.1, 0.2, 0.4, 0.25])
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame({
        'age': ages,
        'spending': spending,
        'purchase_frequency': purchase_frequency,
        'satisfaction': satisfaction
    })
    
    print("æ•°æ®æ¦‚è§ˆ:")
    print(df.head())
    print(f"\næ•°æ®å½¢çŠ¶: {df.shape}")
    
    # TODO 4.1: è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡
    # è®¡ç®—æ¯ä¸ªå˜é‡çš„å‡å€¼ã€ä¸­ä½æ•°ã€æ ‡å‡†å·®
    stats_summary = {}
    
    # è¯·å®Œæˆç»Ÿè®¡é‡çš„è®¡ç®—
    # æç¤ºï¼šä½¿ç”¨ df.describe() æˆ–åˆ†åˆ«è®¡ç®—æ¯ä¸ªå˜é‡
    
    print("\nåŸºæœ¬ç»Ÿè®¡é‡:")
    print(stats_summary)
    
    # TODO 4.2: æ£€æµ‹å¼‚å¸¸å€¼
    # ä½¿ç”¨3å€æ ‡å‡†å·®æ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼
    outliers = {}
    
    # è¯·å®Œæˆå¼‚å¸¸å€¼æ£€æµ‹
    # æç¤ºï¼šå¯¹äºæ¯ä¸ªæ•°å€¼å˜é‡ï¼Œæ‰¾å‡ºè¶…å‡ºå‡å€¼Â±3å€æ ‡å‡†å·®èŒƒå›´çš„å€¼
    
    print("\nå¼‚å¸¸å€¼æ£€æµ‹:")
    for var, outlier_count in outliers.items():
        print(f"{var}: {outlier_count} ä¸ªå¼‚å¸¸å€¼")
    
    # TODO 4.3: è®¡ç®—å˜é‡é—´ç›¸å…³æ€§
    # è®¡ç®—æ•°å€¼å˜é‡é—´çš„ç›¸å…³ç³»æ•°
    correlation_matrix = None  # è¯·å®Œæˆç›¸å…³æ€§è®¡ç®—
    
    print("\nç›¸å…³æ€§çŸ©é˜µ:")
    print(correlation_matrix)
    
    # å¯è§†åŒ–åˆ†æ
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    
    # å¹´é¾„åˆ†å¸ƒ
    ax1.hist(df['age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')
    ax1.set_xlabel('å¹´é¾„')
    ax1.set_ylabel('é¢‘æ¬¡')
    ax1.set_title('ç”¨æˆ·å¹´é¾„åˆ†å¸ƒ')
    ax1.grid(True, alpha=0.3)
    
    # æ¶ˆè´¹é‡‘é¢åˆ†å¸ƒ
    ax2.hist(df['spending'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
    ax2.set_xlabel('æ¶ˆè´¹é‡‘é¢')
    ax2.set_ylabel('é¢‘æ¬¡')
    ax2.set_title('ç”¨æˆ·æ¶ˆè´¹é‡‘é¢åˆ†å¸ƒ')
    ax2.grid(True, alpha=0.3)
    
    # è´­ä¹°é¢‘æ¬¡åˆ†å¸ƒ
    ax3.hist(df['purchase_frequency'], bins=range(0, 15), alpha=0.7, color='orange', edgecolor='black')
    ax3.set_xlabel('è´­ä¹°é¢‘æ¬¡')
    ax3.set_ylabel('é¢‘æ¬¡')
    ax3.set_title('ç”¨æˆ·è´­ä¹°é¢‘æ¬¡åˆ†å¸ƒ')
    ax3.grid(True, alpha=0.3)
    
    # æ»¡æ„åº¦åˆ†å¸ƒ
    satisfaction_counts = df['satisfaction'].value_counts().sort_index()
    ax4.bar(satisfaction_counts.index, satisfaction_counts.values, alpha=0.7, color='purple')
    ax4.set_xlabel('æ»¡æ„åº¦è¯„åˆ†')
    ax4.set_ylabel('ç”¨æˆ·æ•°')
    ax4.set_title('ç”¨æˆ·æ»¡æ„åº¦åˆ†å¸ƒ')
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'df': df,
        'stats_summary': stats_summary,
        'outliers': outliers,
        'correlation_matrix': correlation_matrix
    }

# ç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ
print("\nğŸ”¬ ç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ")
print("ä»»åŠ¡ï¼šè¿›è¡Œç»Ÿè®¡å‡è®¾æ£€éªŒ")

def exercise_5_hypothesis_testing():
    """
    ç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ")
    
    # ä½¿ç”¨ä¸Šä¸€ç»ƒä¹ çš„æ•°æ®
    df = exercise_4_exploratory_analysis()['df']
    
    # åˆ›å»ºç”¨æˆ·ç¾¤ä½“
    df['age_group'] = pd.cut(df['age'], bins=[0, 30, 45, 100], labels=['é’å¹´', 'ä¸­å¹´', 'è€å¹´'])
    df['spending_group'] = pd.cut(df['spending'], bins=[0, 50, 100, 1000], labels=['ä½æ¶ˆè´¹', 'ä¸­æ¶ˆè´¹', 'é«˜æ¶ˆè´¹'])
    
    print("ç”¨æˆ·ç¾¤ä½“åˆ†å¸ƒ:")
    print(df['age_group'].value_counts())
    print(f"\næ¶ˆè´¹ç¾¤ä½“åˆ†å¸ƒ:")
    print(df['spending_group'].value_counts())
    
    # TODO 5.1: ä¸åŒå¹´é¾„ç¾¤ä½“çš„æ¶ˆè´¹é‡‘é¢å·®å¼‚æ£€éªŒ
    # ä½¿ç”¨å•å› ç´ æ–¹å·®åˆ†æ(ANOVA)
    age_groups = df['age_group'].unique()
    group_data = [df[df['age_group'] == group]['spending'].values for group in age_groups]
    
    # è¯·å®ŒæˆANOVAæ£€éªŒ
    # æç¤ºï¼šä½¿ç”¨ stats.f_oneway(*group_data)
    f_stat = None  # è¯·å®Œæˆè®¡ç®—
    p_value = None  # è¯·å®Œæˆè®¡ç®—
    
    print(f"\nå¹´é¾„ç¾¤ä½“æ¶ˆè´¹å·®å¼‚æ£€éªŒ:")
    print(f"Fç»Ÿè®¡é‡: {f_stat:.4f}")
    print(f"på€¼: {p_value:.4f}")
    print(f"ç»“è®º: {'å­˜åœ¨æ˜¾è‘—å·®å¼‚' if p_value < 0.05 else 'æ— æ˜¾è‘—å·®å¼‚'}")
    
    # TODO 5.2: æ¶ˆè´¹é‡‘é¢ä¸æ»¡æ„åº¦çš„å…³ç³»æ£€éªŒ
    # ä½¿ç”¨tæ£€éªŒæ¯”è¾ƒä¸åŒæ»¡æ„åº¦ç¾¤ä½“çš„æ¶ˆè´¹é‡‘é¢
    low_satisfaction = df[df['satisfaction'] <= 3]['spending']
    high_satisfaction = df[df['satisfaction'] >= 4]['spending']
    
    # è¯·å®Œæˆtæ£€éªŒ
    # æç¤ºï¼šä½¿ç”¨ stats.ttest_ind(low_satisfaction, high_satisfaction)
    t_stat = None  # è¯·å®Œæˆè®¡ç®—
    t_pvalue = None  # è¯·å®Œæˆè®¡ç®—
    
    print(f"\næ»¡æ„åº¦æ¶ˆè´¹å·®å¼‚æ£€éªŒ:")
    print(f"tç»Ÿè®¡é‡: {t_stat:.4f}")
    print(f"på€¼: {t_pvalue:.4f}")
    print(f"ç»“è®º: {'å­˜åœ¨æ˜¾è‘—å·®å¼‚' if t_pvalue < 0.05 else 'æ— æ˜¾è‘—å·®å¼‚'}")
    
    # TODO 5.3: å¹´é¾„ç¾¤ä½“ä¸æ¶ˆè´¹ç¾¤ä½“çš„å…³è”æ€§æ£€éªŒ
    # ä½¿ç”¨å¡æ–¹æ£€éªŒ
    contingency_table = pd.crosstab(df['age_group'], df['spending_group'])
    
    # è¯·å®Œæˆå¡æ–¹æ£€éªŒ
    # æç¤ºï¼šä½¿ç”¨ stats.chi2_contingency(contingency_table)
    chi2_stat = None  # è¯·å®Œæˆè®¡ç®—
    chi2_pvalue = None  # è¯·å®Œæˆè®¡ç®—
    dof = None  # è¯·å®Œæˆè®¡ç®—
    
    print(f"\nå¹´é¾„æ¶ˆè´¹å…³è”æ£€éªŒ:")
    print(f"å¡æ–¹ç»Ÿè®¡é‡: {chi2_stat:.4f}")
    print(f"på€¼: {chi2_pvalue:.4f}")
    print(f"è‡ªç”±åº¦: {dof}")
    print(f"ç»“è®º: {'å­˜åœ¨æ˜¾è‘—å…³è”' if chi2_pvalue < 0.05 else 'æ— æ˜¾è‘—å…³è”'}")
    
    return {
        'f_stat': f_stat,
        'p_value': p_value,
        't_stat': t_stat,
        't_pvalue': t_pvalue,
        'chi2_stat': chi2_stat,
        'chi2_pvalue': chi2_pvalue,
        'dof': dof
    }

# ç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ
print("\nğŸ¢ ç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ")
print("ä»»åŠ¡ï¼šæ¨¡æ‹Ÿæ¨èç³»ç»ŸA/Bæµ‹è¯•")

def exercise_6_ab_testing():
    """
    ç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ
    å®ŒæˆTODOæ ‡è®°çš„ä»»åŠ¡
    """
    print("å¼€å§‹ç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ")
    
    # æ¨¡æ‹ŸA/Bæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_users_per_group = 5000
    
    # å¯¹ç…§ç»„ï¼šä¼ ç»Ÿæ¨èç®—æ³•
    control_conversion = np.random.binomial(1, 0.12, n_users_per_group)
    control_revenue = np.random.exponential(50, n_users_per_group) * control_conversion
    
    # å®éªŒç»„ï¼šæ–°æ¨èç®—æ³•
    treatment_conversion = np.random.binomial(1, 0.15, n_users_per_group)
    treatment_revenue = np.random.exponential(55, n_users_per_group) * treatment_conversion
    
    print("A/Bæµ‹è¯•æ•°æ®æ¦‚è§ˆ:")
    print(f"å¯¹ç…§ç»„ç”¨æˆ·æ•°: {n_users_per_group}")
    print(f"å®éªŒç»„ç”¨æˆ·æ•°: {n_users_per_group}")
    
    # TODO 6.1: è®¡ç®—å…³é”®æŒ‡æ ‡
    # è®¡ç®—è½¬åŒ–ç‡å’Œå¹³å‡æ”¶å…¥çš„æå‡
    control_conv_rate = None  # è¯·å®Œæˆè®¡ç®—
    treatment_conv_rate = None  # è¯·å®Œæˆè®¡ç®—
    conv_lift = None  # è¯·å®Œæˆè®¡ç®—
    
    control_avg_revenue = None  # è¯·å®Œæˆè®¡ç®—
    treatment_avg_revenue = None  # è¯·å®Œæˆè®¡ç®—
    revenue_lift = None  # è¯·å®Œæˆè®¡ç®—
    
    print(f"\nå…³é”®æŒ‡æ ‡å¯¹æ¯”:")
    print(f"è½¬åŒ–ç‡: å¯¹ç…§ç»„ {control_conv_rate:.3f} vs å®éªŒç»„ {treatment_conv_rate:.3f} (æå‡ {conv_lift:.1f}%)")
    print(f"å¹³å‡æ”¶å…¥: å¯¹ç…§ç»„ {control_avg_revenue:.2f} vs å®éªŒç»„ {treatment_avg_revenue:.2f} (æå‡ {revenue_lift:.1f}%)")
    
    # TODO 6.2: ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    # è½¬åŒ–ç‡å·®å¼‚æ£€éªŒï¼ˆæ¯”ä¾‹æ£€éªŒï¼‰
    from scipy.stats import proportions_ztest
    conv_counts = [np.sum(treatment_conversion), np.sum(control_conversion)]
    conv_nobs = [n_users_per_group, n_users_per_group]
    
    # è¯·å®Œæˆæ¯”ä¾‹æ£€éªŒ
    # æç¤ºï¼šä½¿ç”¨ proportions_ztest(conv_counts, conv_nobs)
    conv_z_stat = None  # è¯·å®Œæˆè®¡ç®—
    conv_p_value = None  # è¯·å®Œæˆè®¡ç®—
    
    # æ”¶å…¥å·®å¼‚æ£€éªŒï¼ˆtæ£€éªŒï¼‰
    # è¯·å®Œæˆtæ£€éªŒ
    # æç¤ºï¼šä½¿ç”¨ stats.ttest_ind(treatment_revenue, control_revenue)
    revenue_t_stat = None  # è¯·å®Œæˆè®¡ç®—
    revenue_p_value = None  # è¯·å®Œæˆè®¡ç®—
    
    print(f"\nç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ:")
    print(f"è½¬åŒ–ç‡å·®å¼‚æ£€éªŒ: zç»Ÿè®¡é‡={conv_z_stat:.4f}, på€¼={conv_p_value:.4f}")
    print(f"è½¬åŒ–ç‡æ˜¾è‘—æ€§: {'æ˜¾è‘—' if conv_p_value < 0.05 else 'ä¸æ˜¾è‘—'}")
    print(f"æ”¶å…¥å·®å¼‚æ£€éªŒ: tç»Ÿè®¡é‡={revenue_t_stat:.4f}, på€¼={revenue_p_value:.4f}")
    print(f"æ”¶å…¥æ˜¾è‘—æ€§: {'æ˜¾è‘—' if revenue_p_value < 0.05 else 'ä¸æ˜¾è‘—'}")
    
    # ä¸šåŠ¡å†³ç­–å»ºè®®
    print(f"\nğŸ’¼ ä¸šåŠ¡å†³ç­–å»ºè®®:")
    if conv_p_value < 0.05 and revenue_p_value < 0.05:
        print(f"âœ… å»ºè®®: é‡‡ç”¨æ–°æ¨èç®—æ³•")
        print(f"ç†ç”±: è½¬åŒ–ç‡å’Œæ”¶å…¥éƒ½æœ‰æ˜¾è‘—æå‡")
    elif conv_p_value < 0.05:
        print(f"âš ï¸ å»ºè®®: è°¨æ…é‡‡ç”¨æ–°ç®—æ³•")
        print(f"ç†ç”±: è½¬åŒ–ç‡æœ‰æå‡ï¼Œä½†æ”¶å…¥æå‡ä¸æ˜¾è‘—")
    else:
        print(f"âŒ å»ºè®®: ä¸é‡‡ç”¨æ–°ç®—æ³•")
        print(f"ç†ç”±: å…³é”®æŒ‡æ ‡æå‡ä¸æ˜¾è‘—")
    
    return {
        'control_conv_rate': control_conv_rate,
        'treatment_conv_rate': treatment_conv_rate,
        'conv_lift': conv_lift,
        'control_avg_revenue': control_avg_revenue,
        'treatment_avg_revenue': treatment_avg_revenue,
        'revenue_lift': revenue_lift,
        'conv_z_stat': conv_z_stat,
        'conv_p_value': conv_p_value,
        'revenue_t_stat': revenue_t_stat,
        'revenue_p_value': revenue_p_value
    }

# ============================================================================
# ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç»ƒä¹ 
# ============================================================================

def main():
    """
    ä¸»å‡½æ•°ï¼šè¿è¡Œæ‰€æœ‰ç»ƒä¹ 
    """
    print("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰ç»ƒä¹ ...")
    
    # ç¬¬3å¤©ç»ƒä¹ 
    print("\n" + "="*50)
    print("ç¬¬3å¤©ï¼šå‘é‡åŸºç¡€ç»ƒä¹ ")
    print("="*50)
    
    # ç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ
    print("\nğŸ”¢ è¿è¡Œç»ƒä¹ 1ï¼šåŸºç¡€å‘é‡æ“ä½œ")
    result_1 = exercise_1_vector_operations()
    
    # ç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°
    print("\nğŸ¯ è¿è¡Œç»ƒä¹ 2ï¼šæ¨èç³»ç»Ÿå®ç°")
    result_2 = exercise_2_recommendation_system()
    
    # ç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–
    print("\nğŸ“Š è¿è¡Œç»ƒä¹ 3ï¼šå‘é‡å¯è§†åŒ–")
    result_3 = exercise_3_vector_visualization()
    
    # ç¬¬4å¤©ç»ƒä¹ 
    print("\n" + "="*50)
    print("ç¬¬4å¤©ï¼šæ¦‚ç‡ç»Ÿè®¡ç»ƒä¹ ")
    print("="*50)
    
    # ç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ
    print("\nğŸ“Š è¿è¡Œç»ƒä¹ 4ï¼šæ•°æ®æ¢ç´¢æ€§åˆ†æ")
    result_4 = exercise_4_exploratory_analysis()
    
    # ç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ
    print("\nğŸ”¬ è¿è¡Œç»ƒä¹ 5ï¼šå‡è®¾æ£€éªŒ")
    result_5 = exercise_5_hypothesis_testing()
    
    # ç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ
    print("\nğŸ¢ è¿è¡Œç»ƒä¹ 6ï¼šA/Bæµ‹è¯•æ¨¡æ‹Ÿ")
    result_6 = exercise_6_ab_testing()
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰ç»ƒä¹ è¿è¡Œå®Œæˆï¼")
    print("="*50)
    
    # æ€»ç»“
    print("\nğŸ“‹ ç»ƒä¹ æ€»ç»“:")
    print("âœ… ç¬¬3å¤©ï¼šå‘é‡åŸºç¡€ - å‘é‡è¿ç®—ã€æ¨èç³»ç»Ÿã€å¯è§†åŒ–")
    print("âœ… ç¬¬4å¤©ï¼šæ¦‚ç‡ç»Ÿè®¡ - æ•°æ®åˆ†æã€å‡è®¾æ£€éªŒã€A/Bæµ‹è¯•")
    print("\nğŸ’¡ å…³é”®å­¦ä¹ ç‚¹:")
    print("1. å‘é‡è¿ç®—æ˜¯æœºå™¨å­¦ä¹ çš„åŸºç¡€")
    print("2. æ¦‚ç‡ç»Ÿè®¡æ˜¯æ•°æ®åˆ†æçš„æ ¸å¿ƒ")
    print("3. å®é™…åº”ç”¨éœ€è¦ç»“åˆä¸šåŠ¡ç†è§£")
    print("4. å¯è§†åŒ–æ˜¯ç†è§£æ•°æ®çš„é‡è¦å·¥å…·")
    
    return {
        'day3_results': [result_1, result_2, result_3],
        'day4_results': [result_4, result_5, result_6]
    }

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰ç»ƒä¹ 
    results = main()
    
    print("\nğŸ¯ ç»ƒä¹ å®Œæˆï¼")
    print("è¯·æ£€æŸ¥TODOæ ‡è®°çš„ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ")
    print("å¦‚æœ‰ç–‘é—®ï¼Œè¯·å‚è€ƒæç¤ºå’Œæ–‡æ¡£") 