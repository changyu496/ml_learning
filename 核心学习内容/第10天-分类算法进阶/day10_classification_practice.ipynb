{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9天：分类算法基础（逻辑回归、决策树）🚀\n",
    "\n",
    "## 学习目标\n",
    "1. 理解分类问题的基本概念\n",
    "2. 掌握逻辑回归和决策树的原理与实现\n",
    "3. 学会分类模型的评估方法\n",
    "4. 完成鸢尾花分类项目实战\n",
    "\n",
    "## 今日重点\n",
    "- 分类vs回归的区别\n",
    "- 逻辑回归的数学原理\n",
    "- 决策树的构建过程\n",
    "- 分类模型评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 分类问题简介 📚\n",
    "\n",
    "## 什么是分类？\n",
    "\n",
    "分类是机器学习中最常见的问题之一，目标是将数据分到不同的类别中。\n",
    "\n",
    "### 分类的类型\n",
    "\n",
    "| 类型 | 描述 | 例子 |\n",
    "|------|------|------|\n",
    "| **二分类** | 只有两个类别 | 垃圾邮件/正常邮件、肿瘤良/恶性 |\n",
    "| **多分类** | 有三个及以上类别 | 鸢尾花分类、手写数字识别 |\n",
    "\n",
    "## 分类与回归的区别\n",
    "\n",
    "| 特征 | 回归 | 分类 |\n",
    "|------|------|------|\n",
    "| **输出类型** | 连续值 | 离散类别 |\n",
    "| **例子** | 房价预测 | 邮件分类 |\n",
    "| **评估指标** | MSE、R² | 准确率、精确率、召回率 |\n",
    "| **算法** | 线性回归 | 逻辑回归、决策树、SVM |\n",
    "\n",
    "## 分类问题的应用场景\n",
    "\n",
    "- **医疗诊断**：疾病预测、药物分类\n",
    "- **金融风控**：信用评估、欺诈检测\n",
    "- **图像识别**：人脸识别、物体检测\n",
    "- **自然语言处理**：情感分析、文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "\n",
    "# 设置中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"✅ 库导入成功！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 逻辑回归原理与实践 🔍\n",
    "\n",
    "## 2.1 逻辑回归原理\n",
    "\n",
    "逻辑回归虽然名字叫\"回归\"，但实际上是用于分类的算法。它的核心思想是：\n",
    "\n",
    "### 数学原理\n",
    "\n",
    "**线性回归的输出**：\n",
    "```\n",
    "z = w^T x + b\n",
    "```\n",
    "\n",
    "**逻辑回归的输出**：\n",
    "```\n",
    "P(y=1|x) = σ(z) = 1 / (1 + e^(-z))\n",
    "```\n",
    "\n",
    "其中 σ(z) 是 **sigmoid函数**，将任意实数映射到 (0,1) 区间。\n",
    "\n",
    "### Sigmoid函数的特点\n",
    "\n",
    "- **输出范围**：(0,1)\n",
    "- **单调递增**：输入越大，输出越接近1\n",
    "- **对称性**：σ(-z) = 1 - σ(z)\n",
    "- **概率解释**：输出可以解释为属于正类的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Sigmoid函数可视化\n",
    "print(\"📊 Sigmoid函数可视化\")\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid函数\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# 生成数据\n",
    "z = np.linspace(-6, 6, 100)\n",
    "sigmoid_z = sigmoid(z)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(z, sigmoid_z, 'b-', linewidth=2, label='Sigmoid函数')\n",
    "plt.plot([0, 0], [0, 1], 'r--', alpha=0.5, label='z=0')\n",
    "plt.plot([-6, 6], [0.5, 0.5], 'g--', alpha=0.5, label='y=0.5')\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('σ(z)')\n",
    "plt.title('Sigmoid函数')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 观察：\")\n",
    "print(\"- 当 z > 0 时，σ(z) > 0.5，倾向于预测为正类\")\n",
    "print(\"- 当 z < 0 时，σ(z) < 0.5，倾向于预测为负类\")\n",
    "print(\"- 当 z = 0 时，σ(z) = 0.5，决策边界\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 损失函数（对数损失）\n",
    "\n",
    "逻辑回归使用**对数损失**（也叫交叉熵损失）：\n",
    "\n",
    "```\n",
    "L = -[y log(ŷ) + (1-y) log(1-ŷ)]\n",
    "```\n",
    "\n",
    "其中：\n",
    "- y：真实标签（0或1）\n",
    "- ŷ：预测概率\n",
    "\n",
    "### 为什么用对数损失？\n",
    "\n",
    "1. **概率解释**：输出是概率，对数损失适合概率模型\n",
    "2. **凸函数**：便于优化\n",
    "3. **惩罚机制**：对错误预测给予更大的惩罚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4 逻辑回归代码实践（鸢尾花二分类）\n",
    "print(\"🎯 逻辑回归实战：鸢尾花二分类\")\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(f\"数据集形状: X={X.shape}, y={y.shape}\")\n",
    "print(f\"特征名称: {iris.feature_names}\")\n",
    "print(f\"类别名称: {iris.target_names}\")\n",
    "\n",
    "# 只取前两类（0和1）做二分类\n",
    "X_bin = X[y < 2]\n",
    "y_bin = y[y < 2]\n",
    "\n",
    "print(f\"\\n二分类数据形状: X={X_bin.shape}, y={y_bin.shape}\")\n",
    "print(f\"类别分布: {np.bincount(y_bin)}\")\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_bin, y_bin, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n训练集大小: {X_train.shape[0]}\")\n",
    "print(f\"测试集大小: {X_test.shape[0]}\")\n",
    "\n",
    "# 创建逻辑回归模型\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)\n",
    "\n",
    "# 评估\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n📊 模型评估结果:\")\n",
    "print(f\"准确率: {acc:.4f}\")\n",
    "print(f\"\\n混淆矩阵:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\n分类报告:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 查看模型参数\n",
    "print(f\"\\n🔧 模型参数:\")\n",
    "print(f\"权重: {logreg.coef_[0]}\")\n",
    "print(f\"偏置: {logreg.intercept_[0]:.4f}\")\n",
    "print(f\"\\n特征重要性:\")\n",
    "for i, (name, coef) in enumerate(zip(iris.feature_names, logreg.coef_[0])):\n",
    "    print(f\"{name}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 逻辑回归决策边界可视化\n",
    "\n",
    "我们用前两个特征做二维可视化，展示决策边界。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只用前两个特征做可视化\n",
    "X2 = X_bin[:, :2]\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "    X2, y_bin, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "logreg2 = LogisticRegression(random_state=42)\n",
    "logreg2.fit(X2_train, y2_train)\n",
    "\n",
    "# 创建网格点\n",
    "x_min, x_max = X2[:, 0].min() - 0.5, X2[:, 0].max() + 0.5\n",
    "y_min, y_max = X2[:, 1].min() - 0.5, X2[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                     np.linspace(y_min, y_max, 200))\n",
    "\n",
    "# 预测网格点的类别\n",
    "Z = logreg2.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# 绘制决策边界\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# 绘制数据点\n",
    "scatter = plt.scatter(X2[:, 0], X2[:, 1], c=y_bin, \n",
    "                      edgecolors='k', cmap=plt.cm.coolwarm, s=60)\n",
    "\n",
    "plt.xlabel('花萼长度 (cm)')\n",
    "plt.ylabel('花萼宽度 (cm)')\n",
    "plt.title('逻辑回归决策边界（鸢尾花二分类）')\n",
    "plt.colorbar(scatter)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 观察决策边界:\")\n",
    "print(\"- 红色区域：预测为类别1\")\n",
    "print(\"- 蓝色区域：预测为类别0\")\n",
    "print(\"- 边界线：决策边界，σ(z) = 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 决策树原理与实践 🌳\n",
    "\n",
    "## 3.1 决策树原理\n",
    "\n",
    "决策树是一种树形结构的分类模型，通过一系列if-else规则将数据分到不同类别。\n",
    "\n",
    "### 决策树的特点\n",
    "\n",
    "**优点：**\n",
    "- 可解释性强（类似流程图）\n",
    "- 可处理非线性关系\n",
    "- 可处理分类和数值特征\n",
    "- 不需要特征缩放\n",
    "\n",
    "**缺点：**\n",
    "- 容易过拟合\n",
    "- 对数据变化敏感\n",
    "- 可能产生过于复杂的树\n",
    "\n",
    "### 决策树的构建过程\n",
    "\n",
    "1. **选择最佳分割特征**：计算信息增益或基尼指数\n",
    "2. **确定分割点**：找到最优的分割阈值\n",
    "3. **递归分割**：对子节点重复上述过程\n",
    "4. **停止条件**：达到最大深度、节点样本数过少等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 决策树代码实践（鸢尾花三分类）\n",
    "print(\"🌳 决策树实战：鸢尾花三分类\")\n",
    "\n",
    "# 使用完整的鸢尾花数据集（三分类）\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 创建决策树模型\n",
    "dtree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dtree.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred = dtree.predict(X_test)\n",
    "y_pred_proba = dtree.predict_proba(X_test)\n",
    "\n",
    "# 评估\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\n📊 决策树评估结果:\")\n",
    "print(f\"准确率: {acc:.4f}\")\n",
    "print(f\"\\n混淆矩阵:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(f\"\\n分类报告:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 查看特征重要性\n",
    "print(f\"\\n🔧 特征重要性:\")\n",
    "for i, (name, importance) in enumerate(zip(iris.feature_names, dtree.feature_importances_)):\n",
    "    print(f\"{name}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 决策树可视化\n",
    "print(\"📊 决策树结构可视化\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(dtree, \n",
    "          feature_names=iris.feature_names,\n",
    "          class_names=iris.target_names,\n",
    "          filled=True,\n",
    "          rounded=True,\n",
    "          fontsize=10)\n",
    "plt.title('决策树结构（鸢尾花三分类）', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 决策树解读:\")\n",
    "print(\"- 每个节点显示分割条件和样本分布\")\n",
    "print(\"- 颜色表示主要类别\")\n",
    "print(\"- 叶子节点显示最终预测结果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 决策树深度的影响\n",
    "\n",
    "让我们观察不同深度对模型性能的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试不同深度的决策树\n",
    "depths = [1, 2, 3, 4, 5, 10, None]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in depths:\n",
    "    dtree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    dtree.fit(X_train, y_train)\n",
    "    \n",
    "    train_score = accuracy_score(y_train, dtree.predict(X_train))\n",
    "    test_score = accuracy_score(y_test, dtree.predict(X_test))\n",
    "    \n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depths, train_scores, 'o-', label='训练集准确率', linewidth=2)\n",
    "plt.plot(depths, test_scores, 's-', label='测试集准确率', linewidth=2)\n",
    "plt.xlabel('最大深度')\n",
    "plt.ylabel('准确率')\n",
    "plt.title('决策树深度对性能的影响')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 观察结果:\")\n",
    "print(\"- 深度太浅：模型欠拟合，训练和测试准确率都低\")\n",
    "print(\"- 深度适中：模型泛化能力好\")\n",
    "print(\"- 深度太深：模型过拟合，训练准确率高但测试准确率下降\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 分类模型评估方法 📈\n",
    "\n",
    "## 4.1 常用评估指标\n",
    "\n",
    "### 准确率（Accuracy）\n",
    "```\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "```\n",
    "\n",
    "### 精确率（Precision）\n",
    "```\n",
    "Precision = TP / (TP + FP)\n",
    "```\n",
    "\n",
    "### 召回率（Recall）\n",
    "```\n",
    "Recall = TP / (TP + FN)\n",
    "```\n",
    "\n",
    "### F1分数\n",
    "```\n",
    "F1 = 2 × (Precision × Recall) / (Precision + Recall)\n",
    "```\n",
    "\n",
    "其中：\n",
    "- TP：真正例（预测为正类，实际为正类）\n",
    "- TN：真负例（预测为负类，实际为负类）\n",
    "- FP：假正例（预测为正类，实际为负类）\n",
    "- FN：假负例（预测为负类，实际为正类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 混淆矩阵可视化\n",
    "print(\"📊 混淆矩阵可视化\")\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "plt.xlabel('预测类别')\n",
    "plt.ylabel('真实类别')\n",
    "plt.title('混淆矩阵')\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 混淆矩阵解读:\")\n",
    "print(\"- 对角线元素：正确分类的样本数\")\n",
    "print(\"- 非对角线元素：错误分类的样本数\")\n",
    "print(\"- 颜色越深表示数值越大\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 ROC曲线（二分类）\n",
    "print(\"📈 ROC曲线分析（二分类）\")\n",
    "\n",
    "# 使用逻辑回归的二分类结果\n",
    "y_test_bin = y_test[y_test < 2]  # 只取前两类\n",
    "y_pred_proba_bin = logreg.predict_proba(X_test[y_test < 2])[:, 1]\n",
    "\n",
    "# 计算ROC曲线\n",
    "fpr, tpr, thresholds = roc_curve(y_test_bin, y_pred_proba_bin)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 可视化\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "         label=f'ROC曲线 (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('假阳性率 (FPR)')\n",
    "plt.ylabel('真阳性率 (TPR)')\n",
    "plt.title('ROC曲线')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"💡 ROC曲线解读:\")\n",
    "print(\"- AUC = 1.0：完美分类器\")\n",
    "print(\"- AUC = 0.5：随机分类器\")\n",
    "print(\"- AUC < 0.5：比随机还差\")\n",
    "print(f\"- 当前AUC = {roc_auc:.2f}：模型表现良好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 模型比较与总结 🔍\n",
    "\n",
    "## 5.1 逻辑回归vs决策树比较\n",
    "\n",
    "让我们比较两种算法的性能："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 模型比较\n",
    "print(\"🔍 逻辑回归 vs 决策树 比较\")\n",
    "\n",
    "# 逻辑回归（三分类）\n",
    "logreg_multi = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg_multi.fit(X_train, y_train)\n",
    "logreg_pred = logreg_multi.predict(X_test)\n",
    "logreg_acc = accuracy_score(y_test, logreg_pred)\n",
    "\n",
    "# 决策树（三分类）\n",
    "dtree_multi = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "dtree_multi.fit(X_train, y_train)\n",
    "dtree_pred = dtree_multi.predict(X_test)\n",
    "dtree_acc = accuracy_score(y_test, dtree_pred)\n",
    "\n",
    "# 比较结果\n",
    "print(f\"\\n📊 模型性能比较:\")\n",
    "print(f\"逻辑回归准确率: {logreg_acc:.4f}\")\n",
    "print(f\"决策树准确率: {dtree_acc:.4f}\")\n",
    "\n",
    "# 可视化比较\n",
    "models = ['逻辑回归', '决策树']\n",
    "accuracies = [logreg_acc, dtree_acc]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(models, accuracies, color=['skyblue', 'lightcoral'])\n",
    "plt.ylabel('准确率')\n",
    "plt.title('逻辑回归 vs 决策树 性能比较')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# 在柱状图上添加数值\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 模型特点总结:\")\n",
    "print(\"逻辑回归:\")\n",
    "print(\"- 优点：训练快、可解释性强、不易过拟合\")\n",
    "print(\"- 缺点：只能处理线性关系\")\n",
    "print(\"\\n决策树:\")\n",
    "print(\"- 优点：可处理非线性关系、可解释性强\")\n",
    "print(\"- 缺点：容易过拟合、对数据敏感\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 练习与思考题 💭\n",
    "\n",
    "## 6.1 概念理解题\n",
    "\n",
    "1. **分类和回归的本质区别是什么？**\n",
    "   - 输出类型：分类输出离散类别，回归输出连续值\n",
    "   - 评估指标：分类用准确率、精确率等，回归用MSE、R²等\n",
    "   - 应用场景：分类用于预测类别，回归用于预测数值\n",
    "\n",
    "2. **逻辑回归和线性回归的联系与区别？**\n",
    "   - 联系：都使用线性函数 w^T x + b\n",
    "   - 区别：线性回归直接输出，逻辑回归通过sigmoid函数输出概率\n",
    "\n",
    "3. **决策树如何避免过拟合？**\n",
    "   - 限制最大深度\n",
    "   - 设置最小样本数\n",
    "   - 使用剪枝技术\n",
    "   - 使用集成方法（随机森林等）\n",
    "\n",
    "4. **什么是混淆矩阵？如何解读？**\n",
    "   - 混淆矩阵显示预测结果与真实标签的对比\n",
    "   - 对角线元素表示正确分类的样本数\n",
    "   - 非对角线元素表示错误分类的样本数\n",
    "\n",
    "5. **精确率、召回率、F1分数分别适合什么场景？**\n",
    "   - 精确率：关注预测为正类的准确性（如垃圾邮件检测）\n",
    "   - 召回率：关注找出所有正类的能力（如疾病检测）\n",
    "   - F1分数：平衡精确率和召回率\n",
    "\n",
    "## 6.2 编程练习题\n",
    "\n",
    "1. **尝试不同的逻辑回归参数**\n",
    "   ```python\n",
    "   # 尝试不同的正则化参数\n",
    "   logreg = LogisticRegression(C=0.1, random_state=42)\n",
    "   ```\n",
    "\n",
    "2. **比较不同深度的决策树**\n",
    "   ```python\n",
    "   # 测试不同max_depth参数\n",
    "   depths = [1, 2, 3, 4, 5]\n",
    "   ```\n",
    "\n",
    "3. **使用其他数据集练习**\n",
    "   ```python\n",
    "   from sklearn.datasets import load_breast_cancer\n",
    "   cancer = load_breast_cancer()\n",
    "   ```\n",
    "\n",
    "## 6.3 思考题\n",
    "\n",
    "1. **现实中哪些问题适合用分类算法？**\n",
    "   - 医疗诊断：疾病预测\n",
    "   - 金融风控：信用评估\n",
    "   - 图像识别：物体分类\n",
    "   - 自然语言处理：文本分类\n",
    "\n",
    "2. **决策树为什么容易过拟合？如何解决？**\n",
    "   - 原因：树可以无限生长，记住所有训练数据\n",
    "   - 解决：限制深度、剪枝、使用集成方法\n",
    "\n",
    "3. **逻辑回归能否用于多分类？如何实现？**\n",
    "   - 可以，通过One-vs-Rest或One-vs-One策略\n",
    "   - sklearn默认使用One-vs-Rest\n",
    "\n",
    "---\n",
    "**今日总结**：你已经掌握了分类算法的基本原理和实现方法！明天我们将学习更高级的分类算法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
