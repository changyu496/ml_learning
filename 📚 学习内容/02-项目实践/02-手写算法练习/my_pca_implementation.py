#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹å†™PCAç®—æ³•å®ç°
ä½œè€…: ChangYu
æ—¥æœŸ: 2025-07-28
ç›®æ ‡: é€šè¿‡æ‰‹å†™å®ç°åŠ æ·±å¯¹PCAçš„ç†è§£
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def my_pca(X, n_components):
    """
    æ‰‹å†™PCAç®—æ³•å®ç°
    
    å‚æ•°:
    X: è¾“å…¥æ•°æ®ï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
    n_components: è¦ä¿ç•™çš„ä¸»æˆåˆ†æ•°é‡
    
    è¿”å›:
    X_pca: é™ç»´åçš„æ•°æ®
    components: ä¸»æˆåˆ†å‘é‡
    explained_variance_ratio: è§£é‡Šæ–¹å·®æ¯”ä¾‹
    """
    # 1. æ•°æ®æ ‡å‡†åŒ– (å»ä¸­å¿ƒåŒ–)
    X_centered = X - X.mean(axis=0)
    print(f"æ•°æ®æ ‡å‡†åŒ–å®Œæˆï¼Œå½¢çŠ¶: {X_centered.shape}")
    
    # 2. è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov_matrix = np.cov(X_centered.T)
    print(f"åæ–¹å·®çŸ©é˜µå½¢çŠ¶: {cov_matrix.shape}")
    print(f"åæ–¹å·®çŸ©é˜µæ˜¯å¯¹ç§°çš„: {np.allclose(cov_matrix, cov_matrix.T)}")
    
    # 3. ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    print(f"ç‰¹å¾å€¼: {eigenvalues}")
    print(f"ç‰¹å¾å‘é‡å½¢çŠ¶: {eigenvectors.shape}")
    
    # 4. æŒ‰ç‰¹å¾å€¼å¤§å°æ’åº
    sorted_indices = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_indices]
    sorted_eigenvectors = eigenvectors[:, sorted_indices]
    
    # 5. é€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
    selected_components = sorted_eigenvectors[:, :n_components]
    print(f"é€‰æ‹©çš„ä¸»æˆåˆ†å½¢çŠ¶: {selected_components.shape}")
    
    # 6. æŠ•å½±åˆ°ä¸»æˆåˆ†ç©ºé—´
    X_pca = X_centered @ selected_components
    
    # 7. è®¡ç®—è§£é‡Šæ–¹å·®æ¯”ä¾‹
    explained_variance_ratio = sorted_eigenvalues[:n_components] / np.sum(eigenvalues)
    
    return X_pca, selected_components, explained_variance_ratio

def compare_with_sklearn(X, n_components=2):
    """
    ä¸sklearnçš„PCAç»“æœå¯¹æ¯”
    """
    print("=" * 50)
    print("æ‰‹å†™PCA vs sklearn PCA å¯¹æ¯”")
    print("=" * 50)
    
    # æ‰‹å†™PCA
    print("\n1. æ‰‹å†™PCAå®ç°:")
    X_pca_my, components_my, ratio_my = my_pca(X, n_components)
    print(f"é™ç»´åæ•°æ®å½¢çŠ¶: {X_pca_my.shape}")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {ratio_my}")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(ratio_my):.4f}")
    
    # sklearn PCA
    print("\n2. sklearn PCAå®ç°:")
    pca_sklearn = PCA(n_components=n_components)
    X_pca_sklearn = pca_sklearn.fit_transform(X)
    print(f"é™ç»´åæ•°æ®å½¢çŠ¶: {X_pca_sklearn.shape}")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca_sklearn.explained_variance_ratio_}")
    print(f"ç´¯è®¡è§£é‡Šæ–¹å·®: {np.sum(pca_sklearn.explained_variance_ratio_):.4f}")
    
    # æ¯”è¾ƒç»“æœ
    print("\n3. ç»“æœå¯¹æ¯”:")
    print(f"æ•°æ®å·®å¼‚ (æ‰‹å†™ vs sklearn): {np.mean(np.abs(X_pca_my - X_pca_sklearn)):.6f}")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹å·®å¼‚: {np.mean(np.abs(ratio_my - pca_sklearn.explained_variance_ratio_)):.6f}")
    
    return X_pca_my, X_pca_sklearn, ratio_my, pca_sklearn.explained_variance_ratio_

def visualize_pca_results(X_pca_my, X_pca_sklearn, iris):
    """
    å¯è§†åŒ–PCAç»“æœ
    """
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # æ‰‹å†™PCAç»“æœ
    scatter1 = axes[0].scatter(X_pca_my[:, 0], X_pca_my[:, 1], 
                               c=iris.target, cmap='viridis', alpha=0.7)
    axes[0].set_title('æ‰‹å†™PCAç»“æœ')
    axes[0].set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
    axes[0].set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
    axes[0].grid(True, alpha=0.3)
    
    # sklearn PCAç»“æœ
    scatter2 = axes[1].scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], 
                               c=iris.target, cmap='viridis', alpha=0.7)
    axes[1].set_title('sklearn PCAç»“æœ')
    axes[1].set_xlabel('ç¬¬ä¸€ä¸»æˆåˆ†')
    axes[1].set_ylabel('ç¬¬äºŒä¸»æˆåˆ†')
    axes[1].grid(True, alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    legend1 = axes[0].legend(*scatter1.legend_elements(), title="é¸¢å°¾èŠ±å“ç§")
    legend2 = axes[1].legend(*scatter2.legend_elements(), title="é¸¢å°¾èŠ±å“ç§")
    
    plt.tight_layout()
    plt.show()

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæ‰‹å†™PCAç®—æ³•
    """
    print("ğŸš€ å¼€å§‹æ‰‹å†™PCAç®—æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # 1. åŠ è½½æ•°æ®
    print("1. åŠ è½½Irisæ•°æ®é›†")
    iris = load_iris()
    X = iris.data
    y = iris.target
    
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç‰¹å¾åç§°: {iris.feature_names}")
    print(f"ç›®æ ‡ç±»åˆ«: {iris.target_names}")
    
    # 2. æ•°æ®æ ‡å‡†åŒ–
    print("\n2. æ•°æ®æ ‡å‡†åŒ–")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print(f"æ ‡å‡†åŒ–åæ•°æ®å½¢çŠ¶: {X_scaled.shape}")
    
    # 3. æ‰‹å†™PCA vs sklearn PCA
    print("\n3. æ‰§è¡ŒPCAé™ç»´")
    X_pca_my, X_pca_sklearn, ratio_my, ratio_sklearn = compare_with_sklearn(X_scaled, n_components=2)
    
    # 4. å¯è§†åŒ–ç»“æœ
    print("\n4. å¯è§†åŒ–PCAç»“æœ")
    visualize_pca_results(X_pca_my, X_pca_sklearn, iris)
    
    # 5. è¯¦ç»†åˆ†æ
    print("\n5. è¯¦ç»†åˆ†æ")
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"é™ç»´åç‰¹å¾æ•°é‡: {X_pca_my.shape[1]}")
    print(f"é™ç»´æ¯”ä¾‹: {X_pca_my.shape[1] / X.shape[1]:.2%}")
    print(f"ä¿¡æ¯ä¿ç•™æ¯”ä¾‹: {np.sum(ratio_my):.2%}")
    
    print("\nğŸ‰ æ‰‹å†™PCAç®—æ³•æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main() 