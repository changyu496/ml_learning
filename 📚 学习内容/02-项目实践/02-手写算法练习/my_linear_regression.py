#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰‹å†™çº¿æ€§å›å½’ç®—æ³•å®ç°
ä½œè€…: ChangYu
æ—¥æœŸ: 2025-07-28
ç›®æ ‡: é€šè¿‡æ‰‹å†™å®ç°åŠ æ·±å¯¹çº¿æ€§å›å½’çš„ç†è§£
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

class MyLinearRegression:
    """
    æ‰‹å†™çº¿æ€§å›å½’å®ç°
    """
    
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        """
        åˆå§‹åŒ–å‚æ•°
        
        å‚æ•°:
        learning_rate: å­¦ä¹ ç‡
        n_iterations: è¿­ä»£æ¬¡æ•°
        """
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.cost_history = []
        
    def fit(self, X, y):
        """
        è®­ç»ƒçº¿æ€§å›å½’æ¨¡å‹
        
        å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (n_samples, n_features)
        y: ç›®æ ‡å‘é‡ï¼Œå½¢çŠ¶ä¸º (n_samples,)
        """
        # åˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        print(f"å¼€å§‹è®­ç»ƒï¼Œæ ·æœ¬æ•°: {n_samples}, ç‰¹å¾æ•°: {n_features}")
        print(f"å­¦ä¹ ç‡: {self.learning_rate}, è¿­ä»£æ¬¡æ•°: {self.n_iterations}")
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.n_iterations):
            # å‰å‘ä¼ æ’­
            y_pred = X @ self.weights + self.bias
            
            # è®¡ç®—æŸå¤±
            cost = np.mean((y_pred - y) ** 2)
            self.cost_history.append(cost)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (2/n_samples) * X.T @ (y_pred - y)
            db = (2/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # æ¯100æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡è¿›åº¦
            if (i + 1) % 100 == 0:
                print(f"è¿­ä»£ {i+1}/{self.n_iterations}, æŸå¤±: {cost:.6f}")
        
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæŸå¤±: {self.cost_history[-1]:.6f}")
        
    def predict(self, X):
        """
        é¢„æµ‹
        
        å‚æ•°:
        X: ç‰¹å¾çŸ©é˜µ
        
        è¿”å›:
        y_pred: é¢„æµ‹ç»“æœ
        """
        return X @ self.weights + self.bias
    
    def get_params(self):
        """
        è·å–æ¨¡å‹å‚æ•°
        """
        return {
            'weights': self.weights,
            'bias': self.bias,
            'cost_history': self.cost_history
        }

def compare_with_sklearn(X_train, X_test, y_train, y_test):
    """
    ä¸sklearnçš„çº¿æ€§å›å½’ç»“æœå¯¹æ¯”
    """
    print("=" * 50)
    print("æ‰‹å†™çº¿æ€§å›å½’ vs sklearnçº¿æ€§å›å½’ å¯¹æ¯”")
    print("=" * 50)
    
    # æ‰‹å†™çº¿æ€§å›å½’
    print("\n1. æ‰‹å†™çº¿æ€§å›å½’:")
    my_lr = MyLinearRegression(learning_rate=0.01, n_iterations=1000)
    my_lr.fit(X_train, y_train)
    y_pred_my = my_lr.predict(X_test)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse_my = mean_squared_error(y_test, y_pred_my)
    r2_my = r2_score(y_test, y_pred_my)
    
    print(f"æ‰‹å†™æ¨¡å‹ - MSE: {mse_my:.6f}, RÂ²: {r2_my:.6f}")
    print(f"æ‰‹å†™æ¨¡å‹å‚æ•° - æƒé‡: {my_lr.weights}, åç½®: {my_lr.bias:.6f}")
    
    # sklearnçº¿æ€§å›å½’
    print("\n2. sklearnçº¿æ€§å›å½’:")
    sklearn_lr = LinearRegression()
    sklearn_lr.fit(X_train, y_train)
    y_pred_sklearn = sklearn_lr.predict(X_test)
    
    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)
    r2_sklearn = r2_score(y_test, y_pred_sklearn)
    
    print(f"sklearnæ¨¡å‹ - MSE: {mse_sklearn:.6f}, RÂ²: {r2_sklearn:.6f}")
    print(f"sklearnæ¨¡å‹å‚æ•° - æƒé‡: {sklearn_lr.coef_}, åç½®: {sklearn_lr.intercept_:.6f}")
    
    # æ¯”è¾ƒç»“æœ
    print("\n3. ç»“æœå¯¹æ¯”:")
    print(f"MSEå·®å¼‚: {abs(mse_my - mse_sklearn):.6f}")
    print(f"RÂ²å·®å¼‚: {abs(r2_my - r2_sklearn):.6f}")
    print(f"æƒé‡å·®å¼‚: {np.mean(np.abs(my_lr.weights - sklearn_lr.coef_)):.6f}")
    print(f"åç½®å·®å¼‚: {abs(my_lr.bias - sklearn_lr.intercept_):.6f}")
    
    return my_lr, sklearn_lr, y_pred_my, y_pred_sklearn

def visualize_training_process(my_lr):
    """
    å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    """
    plt.figure(figsize=(12, 4))
    
    # æŸå¤±å‡½æ•°å˜åŒ–
    plt.subplot(1, 2, 1)
    plt.plot(my_lr.cost_history)
    plt.title('æŸå¤±å‡½æ•°å˜åŒ–')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æŸå¤±å€¼')
    plt.grid(True, alpha=0.3)
    
    # æŸå¤±å‡½æ•°å˜åŒ–ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰
    plt.subplot(1, 2, 2)
    plt.semilogy(my_lr.cost_history)
    plt.title('æŸå¤±å‡½æ•°å˜åŒ–ï¼ˆå¯¹æ•°å°ºåº¦ï¼‰')
    plt.xlabel('è¿­ä»£æ¬¡æ•°')
    plt.ylabel('æŸå¤±å€¼ï¼ˆå¯¹æ•°ï¼‰')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def visualize_predictions(X_test, y_test, y_pred_my, y_pred_sklearn):
    """
    å¯è§†åŒ–é¢„æµ‹ç»“æœ
    """
    plt.figure(figsize=(12, 4))
    
    # æ‰‹å†™æ¨¡å‹é¢„æµ‹ç»“æœ
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred_my, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.title('æ‰‹å†™çº¿æ€§å›å½’é¢„æµ‹ç»“æœ')
    plt.xlabel('çœŸå®å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.grid(True, alpha=0.3)
    
    # sklearnæ¨¡å‹é¢„æµ‹ç»“æœ
    plt.subplot(1, 2, 2)
    plt.scatter(y_test, y_pred_sklearn, alpha=0.6)
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    plt.title('sklearnçº¿æ€§å›å½’é¢„æµ‹ç»“æœ')
    plt.xlabel('çœŸå®å€¼')
    plt.ylabel('é¢„æµ‹å€¼')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæ‰‹å†™çº¿æ€§å›å½’ç®—æ³•
    """
    print("ğŸš€ å¼€å§‹æ‰‹å†™çº¿æ€§å›å½’ç®—æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # 1. ç”Ÿæˆæ•°æ®
    print("1. ç”Ÿæˆå›å½’æ•°æ®")
    X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)
    print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
    
    # 2. æ•°æ®åˆ†å‰²
    print("\n2. æ•°æ®åˆ†å‰²")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    print(f"è®­ç»ƒé›†: {X_train.shape}, æµ‹è¯•é›†: {X_test.shape}")
    
    # 3. æ•°æ®æ ‡å‡†åŒ–
    print("\n3. æ•°æ®æ ‡å‡†åŒ–")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    print("æ•°æ®æ ‡å‡†åŒ–å®Œæˆ")
    
    # 4. æ‰‹å†™çº¿æ€§å›å½’ vs sklearnçº¿æ€§å›å½’
    print("\n4. æ‰§è¡Œçº¿æ€§å›å½’")
    my_lr, sklearn_lr, y_pred_my, y_pred_sklearn = compare_with_sklearn(
        X_train_scaled, X_test_scaled, y_train, y_test
    )
    
    # 5. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    print("\n5. å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹")
    visualize_training_process(my_lr)
    
    # 6. å¯è§†åŒ–é¢„æµ‹ç»“æœ
    print("\n6. å¯è§†åŒ–é¢„æµ‹ç»“æœ")
    visualize_predictions(X_test_scaled, y_test, y_pred_my, y_pred_sklearn)
    
    # 7. è¯¦ç»†åˆ†æ
    print("\n7. è¯¦ç»†åˆ†æ")
    print(f"æ‰‹å†™æ¨¡å‹å‚æ•°æ•°é‡: {len(my_lr.weights) + 1}")
    print(f"sklearnæ¨¡å‹å‚æ•°æ•°é‡: {len(sklearn_lr.coef_) + 1}")
    print(f"è®­ç»ƒè¿­ä»£æ¬¡æ•°: {len(my_lr.cost_history)}")
    print(f"æœ€ç»ˆæŸå¤±å€¼: {my_lr.cost_history[-1]:.6f}")
    
    print("\nğŸ‰ æ‰‹å†™çº¿æ€§å›å½’ç®—æ³•æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    main() 