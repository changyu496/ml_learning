# 线性回归数学原理详解

## 1. 什么是线性回归？

**简单理解**：
- 我们有一堆数据点 (x, y)
- 想找一条直线，让这条直线尽可能"贴近"所有数据点
- 这条直线的方程就是：`y = wx + b`

**举个例子**：
- 假设你想预测房价
- x是房子面积，y是房价
- 我们找一条直线：房价 = w × 面积 + b
- w是斜率，b是截距

## 2. 为什么叫"线性"？

**线性** = 一次函数
- `y = 2x + 3` ✅ 线性
- `y = x² + 2x` ❌ 不是线性（有x²）

**多变量线性回归**：
- `y = w₁x₁ + w₂x₂ + w₃x₃ + b`
- 每个x前面只有一个系数w，没有x²、x³等

## 3. 损失函数详解

**问题**：怎么判断我们的直线好不好？

**答案**：计算每个点到直线的距离

**具体步骤**：
1. 对于每个数据点 (x, y)
2. 用我们的直线预测：y_pred = wx + b
3. 计算误差：error = y_true - y_pred
4. 计算误差平方：error² = (y_true - y_pred)²
5. 求所有误差平方的平均值：MSE = (1/n) × Σ(error²)

**为什么用平方？**
- 正误差和负误差都要考虑
- 平方让所有误差都变成正数
- 平方对大误差更敏感

## 4. 梯度下降详解

**问题**：怎么找到最好的w和b？

**答案**：梯度下降

**什么是梯度？**
- 梯度就是"最陡峭的方向"
- 想象你在山上，梯度告诉你下山最快的方向

**梯度下降步骤**：
1. 随机初始化w和b（比如都设为0）
2. 计算当前损失函数的值
3. 计算梯度（损失函数对w和b的偏导数）
4. 更新参数：新参数 = 旧参数 - 学习率 × 梯度
5. 重复步骤2-4，直到损失函数不再下降

## 5. 数学公式详解

**损失函数**：
```
J(w,b) = (1/n) × Σ(y_pred - y_true)²
        = (1/n) × Σ(wx + b - y_true)²
```

**梯度计算**：
```
∂J/∂w = (2/n) × Σ(x × (wx + b - y_true))
∂J/∂b = (2/n) × Σ(wx + b - y_true)
```

**参数更新**：
```
w_new = w_old - α × ∂J/∂w
b_new = b_old - α × ∂J/∂b
```
其中α是学习率（控制每次更新的步长）

## 6. 学习率的作用

**学习率太大**：
- 步子太大，可能跳过最优解
- 损失函数震荡，不收敛

**学习率太小**：
- 步子太小，收敛很慢
- 需要很多次迭代

**好的学习率**：
- 损失函数平稳下降
- 既不太慢也不太震荡

## 7. 实际例子

假设我们有数据：
- (1, 3), (2, 5), (3, 7)

我们想找直线：y = wx + b

**第一次迭代**（假设w=0, b=0, 学习率=0.01）：
1. 预测：y_pred = 0×1 + 0 = 0, 0×2 + 0 = 0, 0×3 + 0 = 0
2. 误差：3-0=3, 5-0=5, 7-0=7
3. 损失：MSE = (3² + 5² + 7²)/3 = (9 + 25 + 49)/3 = 27.67
4. 梯度：∂J/∂w = (2/3) × (1×3 + 2×5 + 3×7) = (2/3) × (3 + 10 + 21) = 22.67
5. 更新：w_new = 0 - 0.01 × 22.67 = -0.2267

## 8. 实现步骤总结

1. **初始化参数**：权重w和偏置b设为0
2. **前向传播**：计算预测值 `y_pred = X @ w + b`
3. **计算损失**：MSE = mean((y_pred - y)²)
4. **计算梯度**：
   - `∂J/∂w = (2/n) * X.T @ (y_pred - y)`
   - `∂J/∂b = (2/n) * sum(y_pred - y)`
5. **更新参数**：`w = w - learning_rate * ∂J/∂w`

---
**创建时间**: 2025-07-28
**用途**: 手写线性回归算法学习参考 