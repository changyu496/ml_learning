# ğŸ“Š PCAä¸»æˆåˆ†åˆ†æè¯¦è§£

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> **PCAæ˜¯æ•°æ®é™ç»´çš„ç»å…¸æ–¹æ³•ï¼Œå®ƒé€šè¿‡æ‰¾åˆ°æ•°æ®ä¸­æ–¹å·®æœ€å¤§çš„æ–¹å‘æ¥å®ç°é«˜æ•ˆçš„ç‰¹å¾æå–**

### ä»€ä¹ˆæ˜¯PCAï¼Ÿ
**å®šä¹‰**ï¼šä¸»æˆåˆ†åˆ†æï¼ˆPrincipal Component Analysisï¼‰æ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œé€šè¿‡æ­£äº¤å˜æ¢å°†å¯èƒ½ç›¸å…³çš„å˜é‡è½¬æ¢ä¸ºä¸ç›¸å…³çš„å˜é‡ï¼ˆä¸»æˆåˆ†ï¼‰ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šæ‰¾åˆ°æ•°æ®ä¸­æ–¹å·®æœ€å¤§çš„æ–¹å‘ï¼Œå°†é«˜ç»´æ•°æ®æŠ•å½±åˆ°ä½ç»´ç©ºé—´ï¼ŒåŒæ—¶ä¿ç•™æœ€å¤šçš„ä¿¡æ¯ã€‚

**ç›´è§‰ç†è§£**ï¼šå°±åƒä»ä¸åŒè§’åº¦è§‚å¯Ÿä¸€ä¸ªä¸‰ç»´ç‰©ä½“ï¼Œé€‰æ‹©æœ€èƒ½å±•ç°ç‰©ä½“ç‰¹å¾çš„è§†è§’ã€‚

---

## ğŸ§  æ•°å­¦åŸç†

### åæ–¹å·®çŸ©é˜µçš„ä½œç”¨
```python
import numpy as np
import matplotlib.pyplot as plt

def understand_covariance_matrix():
    """ç†è§£åæ–¹å·®çŸ©é˜µåœ¨PCAä¸­çš„ä½œç”¨"""
    
    # ç”ŸæˆäºŒç»´ç›¸å…³æ•°æ®
    np.random.seed(42)
    
    # åŸå§‹æ•°æ®ï¼šä¸¤ä¸ªå˜é‡æœ‰ç›¸å…³æ€§
    n_samples = 200
    x1 = np.random.randn(n_samples)
    x2 = 0.5 * x1 + 0.5 * np.random.randn(n_samples)
    
    # ç»„åˆæˆæ•°æ®çŸ©é˜µ
    X = np.column_stack([x1, x2])
    
    print("ç†è§£åæ–¹å·®çŸ©é˜µ")
    print(f"æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"åŸå§‹æ•°æ®:")
    print(f"  X1 å‡å€¼: {np.mean(X[:, 0]):.3f}, æ–¹å·®: {np.var(X[:, 0]):.3f}")
    print(f"  X2 å‡å€¼: {np.mean(X[:, 1]):.3f}, æ–¹å·®: {np.var(X[:, 1]):.3f}")
    
    # ä¸­å¿ƒåŒ–æ•°æ®
    X_centered = X - np.mean(X, axis=0)
    
    # è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov_matrix = np.cov(X_centered.T)
    
    print(f"\nåæ–¹å·®çŸ©é˜µ:")
    print(cov_matrix)
    print(f"å¯¹è§’çº¿å…ƒç´ ï¼ˆæ–¹å·®ï¼‰: {np.diag(cov_matrix)}")
    print(f"éå¯¹è§’çº¿å…ƒç´ ï¼ˆåæ–¹å·®ï¼‰: {cov_matrix[0, 1]:.3f}")
    
    # åæ–¹å·®çŸ©é˜µçš„æ„ä¹‰
    print(f"\nåæ–¹å·®çŸ©é˜µçš„è§£é‡Š:")
    print(f"â€¢ å¯¹è§’çº¿å…ƒç´  = å„å˜é‡çš„æ–¹å·®")
    print(f"â€¢ éå¯¹è§’çº¿å…ƒç´  = å˜é‡é—´çš„åæ–¹å·®")
    print(f"â€¢ æ­£åæ–¹å·® = æ­£ç›¸å…³ï¼Œè´Ÿåæ–¹å·® = è´Ÿç›¸å…³")
    
    return X, X_centered, cov_matrix

X, X_centered, cov_matrix = understand_covariance_matrix()
```

### PCAçš„æ•°å­¦æ¨å¯¼
```python
def pca_mathematical_derivation():
    """PCAçš„æ•°å­¦æ¨å¯¼è¿‡ç¨‹"""
    
    print("PCAæ•°å­¦æ¨å¯¼")
    print("=" * 50)
    
    print("1. é—®é¢˜è®¾å®š:")
    print("   â€¢ åŸå§‹æ•°æ®: X âˆˆ â„â¿Ë£áµˆ (nä¸ªæ ·æœ¬ï¼Œdä¸ªç‰¹å¾)")
    print("   â€¢ ç›®æ ‡: æ‰¾åˆ°kä¸ªæ–¹å‘ï¼Œä½¿å¾—æ•°æ®åœ¨è¿™äº›æ–¹å‘ä¸Šçš„æ–¹å·®æœ€å¤§")
    
    print("\n2. æ•°å­¦è¡¨è¿°:")
    print("   â€¢ ä¸­å¿ƒåŒ–æ•°æ®: XÌƒ = X - Î¼")
    print("   â€¢ åæ–¹å·®çŸ©é˜µ: C = (1/n)XÌƒáµ€XÌƒ")
    print("   â€¢ ç¬¬ä¸€ä¸»æˆåˆ†: max wâ‚áµ€Cwâ‚ s.t. ||wâ‚|| = 1")
    
    print("\n3. æ±‚è§£è¿‡ç¨‹:")
    print("   â€¢ æ‹‰æ ¼æœ—æ—¥å‡½æ•°: L = wâ‚áµ€Cwâ‚ - Î»(wâ‚áµ€wâ‚ - 1)")
    print("   â€¢ æ±‚å¯¼: âˆ‚L/âˆ‚wâ‚ = 2Cwâ‚ - 2Î»wâ‚ = 0")
    print("   â€¢ å¾—åˆ°: Cwâ‚ = Î»wâ‚")
    print("   â€¢ ç»“è®º: wâ‚æ˜¯Cçš„ç‰¹å¾å‘é‡ï¼ŒÎ»æ˜¯å¯¹åº”çš„ç‰¹å¾å€¼")
    
    print("\n4. ä¸»æˆåˆ†é€‰æ‹©:")
    print("   â€¢ ç‰¹å¾å€¼ = è¯¥æ–¹å‘ä¸Šçš„æ–¹å·®")
    print("   â€¢ æŒ‰ç‰¹å¾å€¼å¤§å°æ’åºé€‰æ‹©å‰kä¸ªç‰¹å¾å‘é‡")
    print("   â€¢ è¿™kä¸ªæ–¹å‘å°±æ˜¯ä¸»æˆåˆ†")
    
    print("\n5. é™ç»´å˜æ¢:")
    print("   â€¢ æŠ•å½±çŸ©é˜µ: P = [wâ‚, wâ‚‚, ..., wâ‚–]")
    print("   â€¢ é™ç»´åæ•°æ®: Y = XÌƒP")

pca_mathematical_derivation()
```

---

## ğŸ”¢ æ‰‹å·¥å®ç°PCA

### å®Œæ•´çš„PCAå®ç°
```python
def manual_pca_implementation():
    """æ‰‹å·¥å®ç°PCAç®—æ³•"""
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 100
    
    # åˆ›å»ºç›¸å…³æ•°æ®
    X = np.random.randn(n_samples, 3)
    # æ·»åŠ ç›¸å…³æ€§
    X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)
    X[:, 2] = X[:, 0] + X[:, 1] + 0.3 * np.random.randn(n_samples)
    
    print("æ‰‹å·¥å®ç°PCA")
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    
    # æ­¥éª¤1: ä¸­å¿ƒåŒ–æ•°æ®
    X_mean = np.mean(X, axis=0)
    X_centered = X - X_mean
    
    print(f"åŸå§‹æ•°æ®å‡å€¼: {X_mean}")
    print(f"ä¸­å¿ƒåŒ–åå‡å€¼: {np.mean(X_centered, axis=0)}")
    
    # æ­¥éª¤2: è®¡ç®—åæ–¹å·®çŸ©é˜µ
    cov_matrix = np.cov(X_centered.T)
    
    print(f"\nåæ–¹å·®çŸ©é˜µ:")
    print(cov_matrix)
    
    # æ­¥éª¤3: ç‰¹å¾å€¼åˆ†è§£
    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
    
    # æ­¥éª¤4: æ’åº
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    print(f"\nç‰¹å¾å€¼ (æŒ‰å¤§å°æ’åº): {eigenvalues}")
    print(f"ç‰¹å¾å‘é‡:")
    print(eigenvectors)
    
    # æ­¥éª¤5: è®¡ç®—è§£é‡Šæ–¹å·®æ¯”ä¾‹
    explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
    
    print(f"\nè§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance_ratio}")
    print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹: {np.cumsum(explained_variance_ratio)}")
    
    # æ­¥éª¤6: é€‰æ‹©ä¸»æˆåˆ†æ•°é‡
    k = 2  # é€‰æ‹©å‰2ä¸ªä¸»æˆåˆ†
    selected_eigenvectors = eigenvectors[:, :k]
    
    print(f"\né€‰æ‹©å‰ {k} ä¸ªä¸»æˆåˆ†")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {explained_variance_ratio[:k].sum():.2%}")
    
    # æ­¥éª¤7: æ•°æ®å˜æ¢
    X_pca = X_centered @ selected_eigenvectors
    
    print(f"\né™ç»´åæ•°æ®å½¢çŠ¶: {X_pca.shape}")
    print(f"é™ç»´åæ•°æ®çš„æ–¹å·®:")
    print(f"  PC1: {np.var(X_pca[:, 0]):.3f}")
    print(f"  PC2: {np.var(X_pca[:, 1]):.3f}")
    
    # éªŒè¯ï¼šé™ç»´åçš„æ–¹å·®åº”è¯¥ç­‰äºå¯¹åº”çš„ç‰¹å¾å€¼
    print(f"\néªŒè¯ (é™ç»´åæ–¹å·® vs ç‰¹å¾å€¼):")
    for i in range(k):
        variance_pc = np.var(X_pca[:, i])
        eigenvalue = eigenvalues[i]
        print(f"  PC{i+1}: æ–¹å·®={variance_pc:.3f}, ç‰¹å¾å€¼={eigenvalue:.3f}, å·®å¼‚={abs(variance_pc - eigenvalue):.6f}")
    
    return X, X_centered, X_pca, eigenvalues, eigenvectors, explained_variance_ratio

X, X_centered, X_pca, eigenvalues, eigenvectors, explained_variance_ratio = manual_pca_implementation()
```

### æ•°æ®é‡æ„
```python
def pca_reconstruction():
    """æ¼”ç¤ºPCAçš„æ•°æ®é‡æ„è¿‡ç¨‹"""
    
    print("PCAæ•°æ®é‡æ„")
    print("=" * 30)
    
    # ä½¿ç”¨ä¹‹å‰çš„æ•°æ®
    k = 2  # ä½¿ç”¨2ä¸ªä¸»æˆåˆ†
    
    # é€‰æ‹©å‰kä¸ªä¸»æˆåˆ†
    selected_eigenvectors = eigenvectors[:, :k]
    
    # é‡æ„æ•°æ®
    X_reconstructed = X_pca @ selected_eigenvectors.T
    
    # åŠ å›å‡å€¼
    X_reconstructed += np.mean(X, axis=0)
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"é‡æ„æ•°æ®å½¢çŠ¶: {X_reconstructed.shape}")
    
    # è®¡ç®—é‡æ„è¯¯å·®
    reconstruction_error = np.mean((X - X_reconstructed) ** 2)
    
    print(f"\né‡æ„è¯¯å·® (MSE): {reconstruction_error:.6f}")
    
    # åˆ†æé‡æ„è´¨é‡
    correlation_matrix = np.corrcoef(X.T, X_reconstructed.T)
    
    print(f"\nåŸå§‹ vs é‡æ„æ•°æ®çš„ç›¸å…³æ€§:")
    for i in range(X.shape[1]):
        corr = correlation_matrix[i, i + X.shape[1]]
        print(f"  ç‰¹å¾ {i+1}: {corr:.3f}")
    
    # ä¿¡æ¯ä¿ç•™ç‡
    info_retained = 1 - reconstruction_error / np.var(X)
    print(f"\nä¿¡æ¯ä¿ç•™ç‡: {info_retained:.2%}")
    
    return X_reconstructed, reconstruction_error

X_reconstructed, reconstruction_error = pca_reconstruction()
```

---

## ğŸ¯ ä½¿ç”¨scikit-learnçš„PCA

### æ ‡å‡†PCAä½¿ç”¨
```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

def sklearn_pca_example():
    """ä½¿ç”¨scikit-learnçš„PCAç¤ºä¾‹"""
    
    print("ä½¿ç”¨scikit-learnçš„PCA")
    print("=" * 40)
    
    # ç”Ÿæˆæ›´å¤æ‚çš„æ•°æ®
    np.random.seed(42)
    n_samples = 500
    n_features = 10
    
    # åˆ›å»ºæœ‰ç»“æ„çš„æ•°æ®
    X = np.random.randn(n_samples, n_features)
    
    # æ·»åŠ çº¿æ€§ç»„åˆåˆ›é€ ç›¸å…³æ€§
    X[:, 1] = X[:, 0] + 0.5 * np.random.randn(n_samples)
    X[:, 2] = X[:, 0] + X[:, 1] + 0.3 * np.random.randn(n_samples)
    X[:, 3] = X[:, 1] + 0.4 * np.random.randn(n_samples)
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    
    # æ ‡å‡†åŒ–æ•°æ®ï¼ˆé‡è¦ï¼ï¼‰
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    print(f"æ ‡å‡†åŒ–åæ•°æ®å‡å€¼: {np.mean(X_scaled, axis=0)[:5]}")
    print(f"æ ‡å‡†åŒ–åæ•°æ®æ ‡å‡†å·®: {np.std(X_scaled, axis=0)[:5]}")
    
    # åº”ç”¨PCA
    pca = PCA()
    X_pca = pca.fit_transform(X_scaled)
    
    print(f"\næ‰€æœ‰ä¸»æˆåˆ†çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(pca.explained_variance_ratio_)
    
    print(f"\nç´¯ç§¯è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    print(np.cumsum(pca.explained_variance_ratio_))
    
    # é€‰æ‹©åˆé€‚çš„ä¸»æˆåˆ†æ•°é‡
    cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
    n_components_95 = np.argmax(cumsum_ratio >= 0.95) + 1
    n_components_99 = np.argmax(cumsum_ratio >= 0.99) + 1
    
    print(f"\nä¸»æˆåˆ†æ•°é‡é€‰æ‹©:")
    print(f"ä¿ç•™95%æ–¹å·®éœ€è¦: {n_components_95} ä¸ªä¸»æˆåˆ†")
    print(f"ä¿ç•™99%æ–¹å·®éœ€è¦: {n_components_99} ä¸ªä¸»æˆåˆ†")
    
    # ä½¿ç”¨é€‰å®šçš„ä¸»æˆåˆ†æ•°é‡
    pca_selected = PCA(n_components=n_components_95)
    X_pca_selected = pca_selected.fit_transform(X_scaled)
    
    print(f"\nä½¿ç”¨ {n_components_95} ä¸ªä¸»æˆåˆ†:")
    print(f"é™ç»´åæ•°æ®å½¢çŠ¶: {X_pca_selected.shape}")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca_selected.explained_variance_ratio_.sum():.2%}")
    
    # æ•°æ®é‡æ„
    X_reconstructed = pca_selected.inverse_transform(X_pca_selected)
    
    # è®¡ç®—é‡æ„è¯¯å·®
    reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)
    print(f"é‡æ„è¯¯å·®: {reconstruction_error:.6f}")
    
    return X_scaled, X_pca_selected, pca_selected

X_scaled, X_pca_selected, pca_selected = sklearn_pca_example()
```

### ä¸»æˆåˆ†åˆ†æçš„å¯è§†åŒ–
```python
def visualize_pca_components():
    """å¯è§†åŒ–PCAä¸»æˆåˆ†"""
    
    print("PCAä¸»æˆåˆ†å¯è§†åŒ–")
    print("=" * 30)
    
    # ä½¿ç”¨äºŒç»´æ•°æ®ä¾¿äºå¯è§†åŒ–
    np.random.seed(42)
    n_samples = 300
    
    # åˆ›å»ºæ¤­åœ†åˆ†å¸ƒçš„æ•°æ®
    angle = np.pi / 4  # 45åº¦
    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],
                               [np.sin(angle), np.cos(angle)]])
    
    # ç”Ÿæˆæ¤­åœ†æ•°æ®
    data = np.random.multivariate_normal([0, 0], [[3, 0], [0, 1]], n_samples)
    X_ellipse = data @ rotation_matrix.T
    
    print(f"æ¤­åœ†æ•°æ®å½¢çŠ¶: {X_ellipse.shape}")
    
    # åº”ç”¨PCA
    pca = PCA()
    X_pca = pca.fit_transform(X_ellipse)
    
    print(f"\nä¸»æˆåˆ†åˆ†æç»“æœ:")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
    print(f"ä¸»æˆåˆ†æ–¹å‘:")
    print(pca.components_)
    
    # è®¡ç®—ä¸»æˆåˆ†çš„æ–¹å‘å’Œé•¿åº¦
    pc1_direction = pca.components_[0]
    pc2_direction = pca.components_[1]
    pc1_length = np.sqrt(pca.explained_variance_[0])
    pc2_length = np.sqrt(pca.explained_variance_[1])
    
    print(f"\nä¸»æˆåˆ†æ–¹å‘å’Œé•¿åº¦:")
    print(f"PC1: æ–¹å‘={pc1_direction}, é•¿åº¦={pc1_length:.3f}")
    print(f"PC2: æ–¹å‘={pc2_direction}, é•¿åº¦={pc2_length:.3f}")
    
    # éªŒè¯ä¸»æˆåˆ†çš„æ­£äº¤æ€§
    orthogonality = np.dot(pc1_direction, pc2_direction)
    print(f"\nä¸»æˆåˆ†æ­£äº¤æ€§æ£€æŸ¥: {orthogonality:.10f} (åº”è¯¥æ¥è¿‘0)")
    
    # åˆ†ææ•°æ®åœ¨ä¸»æˆåˆ†æ–¹å‘ä¸Šçš„æŠ•å½±
    print(f"\næŠ•å½±åæ•°æ®çš„ç»Ÿè®¡:")
    print(f"PC1: å‡å€¼={np.mean(X_pca[:, 0]):.6f}, æ–¹å·®={np.var(X_pca[:, 0]):.3f}")
    print(f"PC2: å‡å€¼={np.mean(X_pca[:, 1]):.6f}, æ–¹å·®={np.var(X_pca[:, 1]):.3f}")
    
    return X_ellipse, X_pca, pca

X_ellipse, X_pca, pca = visualize_pca_components()
```

---

## ğŸ¨ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šå›¾åƒæ•°æ®é™ç»´
```python
def image_pca_example():
    """å›¾åƒæ•°æ®çš„PCAé™ç»´ç¤ºä¾‹"""
    
    print("å›¾åƒPCAé™ç»´ç¤ºä¾‹")
    print("=" * 30)
    
    # æ¨¡æ‹Ÿå›¾åƒæ•°æ® (64x64åƒç´ çš„ç°åº¦å›¾åƒ)
    np.random.seed(42)
    n_images = 100
    image_size = 32  # ç®€åŒ–ä¸º32x32
    
    # ç”Ÿæˆæ¨¡æ‹Ÿå›¾åƒæ•°æ®
    images = []
    for i in range(n_images):
        # åˆ›å»ºæœ‰ç»“æ„çš„å›¾åƒï¼ˆåœ†å½¢ã€æ–¹å½¢ç­‰ï¼‰
        img = np.zeros((image_size, image_size))
        
        # éšæœºæ·»åŠ å›¾å½¢
        center_x, center_y = np.random.randint(8, 24, 2)
        radius = np.random.randint(3, 8)
        
        # åˆ›å»ºåœ†å½¢
        y, x = np.ogrid[:image_size, :image_size]
        mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2
        img[mask] = 1
        
        # æ·»åŠ å™ªå£°
        img += 0.1 * np.random.randn(image_size, image_size)
        
        images.append(img.flatten())
    
    X_images = np.array(images)
    
    print(f"å›¾åƒæ•°æ®å½¢çŠ¶: {X_images.shape}")
    print(f"æ¯ä¸ªå›¾åƒçš„åƒç´ æ•°: {image_size * image_size}")
    
    # åº”ç”¨PCA
    pca = PCA(n_components=50)  # ä¿ç•™50ä¸ªä¸»æˆåˆ†
    X_pca = pca.fit_transform(X_images)
    
    print(f"\né™ç»´åå½¢çŠ¶: {X_pca.shape}")
    print(f"ä¿ç•™çš„æ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_.sum():.2%}")
    
    # åˆ†æä¸»æˆåˆ†
    print(f"\nå‰10ä¸ªä¸»æˆåˆ†çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹:")
    for i in range(10):
        print(f"  PC{i+1}: {pca.explained_variance_ratio_[i]:.3%}")
    
    # é‡æ„å›¾åƒ
    X_reconstructed = pca.inverse_transform(X_pca)
    
    # è®¡ç®—é‡æ„è´¨é‡
    reconstruction_error = np.mean((X_images - X_reconstructed) ** 2)
    print(f"\nå¹³å‡é‡æ„è¯¯å·®: {reconstruction_error:.6f}")
    
    # åˆ†æå‹ç¼©æ¯”
    original_size = X_images.size
    compressed_size = X_pca.size + pca.components_.size + pca.mean_.size
    compression_ratio = original_size / compressed_size
    
    print(f"\nå‹ç¼©åˆ†æ:")
    print(f"åŸå§‹æ•°æ®å¤§å°: {original_size} ä¸ªæµ®ç‚¹æ•°")
    print(f"å‹ç¼©æ•°æ®å¤§å°: {compressed_size} ä¸ªæµ®ç‚¹æ•°")
    print(f"å‹ç¼©æ¯”: {compression_ratio:.2f}:1")
    
    return X_images, X_pca, X_reconstructed, pca

X_images, X_pca_img, X_reconstructed, pca_img = image_pca_example()
```

### æ¡ˆä¾‹2ï¼šç‰¹å¾æå–ä¸åˆ†ç±»
```python
def pca_feature_extraction():
    """ä½¿ç”¨PCAè¿›è¡Œç‰¹å¾æå–"""
    
    from sklearn.datasets import make_classification
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score
    
    print("PCAç‰¹å¾æå–ç”¨äºåˆ†ç±»")
    print("=" * 40)
    
    # ç”Ÿæˆé«˜ç»´åˆ†ç±»æ•°æ®
    X, y = make_classification(
        n_samples=1000,
        n_features=100,
        n_informative=20,
        n_redundant=30,
        n_clusters_per_class=1,
        random_state=42
    )
    
    print(f"åŸå§‹æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y)}")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # ä¸ä½¿ç”¨PCAçš„åŸºçº¿æ€§èƒ½
    clf_baseline = LogisticRegression(random_state=42)
    clf_baseline.fit(X_train_scaled, y_train)
    baseline_accuracy = accuracy_score(y_test, clf_baseline.predict(X_test_scaled))
    
    print(f"\nåŸºçº¿æ€§èƒ½ (ä¸ä½¿ç”¨PCA): {baseline_accuracy:.3f}")
    
    # ä½¿ç”¨ä¸åŒæ•°é‡çš„ä¸»æˆåˆ†
    n_components_list = [10, 20, 30, 50, 80]
    
    print(f"\nä¸åŒä¸»æˆåˆ†æ•°é‡çš„æ€§èƒ½:")
    for n_comp in n_components_list:
        # åº”ç”¨PCA
        pca = PCA(n_components=n_comp)
        X_train_pca = pca.fit_transform(X_train_scaled)
        X_test_pca = pca.transform(X_test_scaled)
        
        # è®­ç»ƒåˆ†ç±»å™¨
        clf = LogisticRegression(random_state=42)
        clf.fit(X_train_pca, y_train)
        
        # è¯„ä¼°æ€§èƒ½
        accuracy = accuracy_score(y_test, clf.predict(X_test_pca))
        variance_explained = pca.explained_variance_ratio_.sum()
        
        print(f"  {n_comp:2d}ä¸ªä¸»æˆåˆ†: å‡†ç¡®ç‡={accuracy:.3f}, è§£é‡Šæ–¹å·®={variance_explained:.2%}")
    
    # è‡ªåŠ¨é€‰æ‹©ä¸»æˆåˆ†æ•°é‡
    pca_auto = PCA(n_components=0.95)  # ä¿ç•™95%æ–¹å·®
    X_train_pca_auto = pca_auto.fit_transform(X_train_scaled)
    X_test_pca_auto = pca_auto.transform(X_test_scaled)
    
    clf_auto = LogisticRegression(random_state=42)
    clf_auto.fit(X_train_pca_auto, y_train)
    auto_accuracy = accuracy_score(y_test, clf_auto.predict(X_test_pca_auto))
    
    print(f"\nè‡ªåŠ¨é€‰æ‹© (95%æ–¹å·®): {pca_auto.n_components_}ä¸ªä¸»æˆåˆ†, å‡†ç¡®ç‡={auto_accuracy:.3f}")
    
    # åˆ†æç»“æœ
    print(f"\nç»“æœåˆ†æ:")
    print(f"åŸå§‹ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"è‡ªåŠ¨é€‰æ‹©çš„ä¸»æˆåˆ†æ•°: {pca_auto.n_components_}")
    print(f"ç»´åº¦å‡å°‘: {(1 - pca_auto.n_components_/X.shape[1]):.1%}")
    print(f"æ€§èƒ½å˜åŒ–: {auto_accuracy - baseline_accuracy:+.3f}")

pca_feature_extraction()
```

### æ¡ˆä¾‹3ï¼šæ•°æ®å¯è§†åŒ–
```python
def pca_visualization():
    """ä½¿ç”¨PCAè¿›è¡Œæ•°æ®å¯è§†åŒ–"""
    
    from sklearn.datasets import load_iris
    
    print("PCAæ•°æ®å¯è§†åŒ–")
    print("=" * 30)
    
    # åŠ è½½é¸¢å°¾èŠ±æ•°æ®é›†
    iris = load_iris()
    X, y = iris.data, iris.target
    feature_names = iris.feature_names
    target_names = iris.target_names
    
    print(f"é¸¢å°¾èŠ±æ•°æ®å½¢çŠ¶: {X.shape}")
    print(f"ç‰¹å¾åç§°: {feature_names}")
    print(f"ç±»åˆ«åç§°: {target_names}")
    
    # æ ‡å‡†åŒ–æ•°æ®
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åº”ç”¨PCAé™ç»´åˆ°2D
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X_scaled)
    
    print(f"\né™ç»´åå½¢çŠ¶: {X_pca.shape}")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca.explained_variance_ratio_}")
    print(f"ç´¯ç§¯è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum():.2%}")
    
    # åˆ†æä¸»æˆåˆ†çš„æ„æˆ
    print(f"\nä¸»æˆåˆ†çš„æ„æˆ:")
    components = pca.components_
    
    for i, pc in enumerate(components):
        print(f"ä¸»æˆåˆ† {i+1} (è§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_[i]:.2%}):")
        for j, coef in enumerate(pc):
            print(f"  {feature_names[j]}: {coef:+.3f}")
    
    # åˆ†ææ¯ä¸ªç±»åˆ«åœ¨ä¸»æˆåˆ†ç©ºé—´çš„åˆ†å¸ƒ
    print(f"\nå„ç±»åˆ«åœ¨ä¸»æˆåˆ†ç©ºé—´çš„åˆ†å¸ƒ:")
    for i, class_name in enumerate(target_names):
        class_data = X_pca[y == i]
        print(f"{class_name}:")
        print(f"  PC1: å‡å€¼={np.mean(class_data[:, 0]):+.3f}, æ ‡å‡†å·®={np.std(class_data[:, 0]):.3f}")
        print(f"  PC2: å‡å€¼={np.mean(class_data[:, 1]):+.3f}, æ ‡å‡†å·®={np.std(class_data[:, 1]):.3f}")
    
    # è®¡ç®—ç±»é—´åˆ†ç¦»åº¦
    def compute_class_separation(X_pca, y):
        """è®¡ç®—ç±»é—´åˆ†ç¦»åº¦"""
        n_classes = len(np.unique(y))
        centroids = []
        
        for i in range(n_classes):
            centroid = np.mean(X_pca[y == i], axis=0)
            centroids.append(centroid)
        
        # è®¡ç®—ç±»é—´è·ç¦»
        distances = []
        for i in range(n_classes):
            for j in range(i+1, n_classes):
                dist = np.linalg.norm(centroids[i] - centroids[j])
                distances.append(dist)
        
        return np.mean(distances)
    
    # æ¯”è¾ƒåŸå§‹æ•°æ®å’ŒPCAæ•°æ®çš„åˆ†ç¦»åº¦
    original_separation = compute_class_separation(X_scaled, y)
    pca_separation = compute_class_separation(X_pca, y)
    
    print(f"\nç±»é—´åˆ†ç¦»åº¦:")
    print(f"åŸå§‹æ•°æ® (4D): {original_separation:.3f}")
    print(f"PCAæ•°æ® (2D): {pca_separation:.3f}")
    print(f"åˆ†ç¦»åº¦ä¿æŒ: {pca_separation/original_separation:.1%}")
    
    return X_scaled, X_pca, pca

X_iris, X_iris_pca, pca_iris = pca_visualization()
```

---

## ğŸ”§ PCAçš„å˜ä½“å’Œæ‰©å±•

### å¢é‡PCA
```python
def incremental_pca_example():
    """å¢é‡PCAç¤ºä¾‹ - å¤„ç†å¤§æ•°æ®é›†"""
    
    from sklearn.decomposition import IncrementalPCA
    
    print("å¢é‡PCAç¤ºä¾‹")
    print("=" * 30)
    
    # æ¨¡æ‹Ÿå¤§æ•°æ®é›†
    np.random.seed(42)
    n_samples = 10000
    n_features = 100
    
    # ç”Ÿæˆæ•°æ®
    X_large = np.random.randn(n_samples, n_features)
    
    # æ·»åŠ ä¸€äº›ç»“æ„
    X_large[:, 1] = X_large[:, 0] + 0.5 * np.random.randn(n_samples)
    X_large[:, 2] = X_large[:, 0] + X_large[:, 1] + 0.3 * np.random.randn(n_samples)
    
    print(f"å¤§æ•°æ®é›†å½¢çŠ¶: {X_large.shape}")
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_large_scaled = scaler.fit_transform(X_large)
    
    # ä¼ ç»ŸPCA
    pca_traditional = PCA(n_components=10)
    X_pca_traditional = pca_traditional.fit_transform(X_large_scaled)
    
    # å¢é‡PCA
    batch_size = 1000
    ipca = IncrementalPCA(n_components=10, batch_size=batch_size)
    
    # æ‰¹æ¬¡å¤„ç†
    for i in range(0, n_samples, batch_size):
        batch = X_large_scaled[i:i+batch_size]
        ipca.partial_fit(batch)
    
    # å˜æ¢æ•°æ®
    X_ipca = ipca.transform(X_large_scaled)
    
    print(f"\nä¼ ç»ŸPCAç»“æœ:")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca_traditional.explained_variance_ratio_[:5]}")
    
    print(f"\nå¢é‡PCAç»“æœ:")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {ipca.explained_variance_ratio_[:5]}")
    
    # æ¯”è¾ƒç»“æœ
    mse_components = np.mean((pca_traditional.components_ - ipca.components_) ** 2)
    correlation = np.corrcoef(X_pca_traditional.flatten(), X_ipca.flatten())[0, 1]
    
    print(f"\næ¯”è¾ƒç»“æœ:")
    print(f"ä¸»æˆåˆ†å·®å¼‚ (MSE): {mse_components:.6f}")
    print(f"å˜æ¢ç»“æœç›¸å…³æ€§: {correlation:.6f}")
    
    print(f"\nå¢é‡PCAçš„ä¼˜åŠ¿:")
    print(f"â€¢ å†…å­˜æ•ˆç‡é«˜ï¼šæ‰¹æ¬¡å¤„ç†ï¼Œä¸éœ€è¦åŠ è½½å…¨éƒ¨æ•°æ®")
    print(f"â€¢ é€‚åˆåœ¨çº¿å­¦ä¹ ï¼šå¯ä»¥å¤„ç†æµå¼æ•°æ®")
    print(f"â€¢ é€‚åˆå¤§æ•°æ®ï¼šçªç ´å†…å­˜é™åˆ¶")

incremental_pca_example()
```

### æ ¸PCA
```python
def kernel_pca_example():
    """æ ¸PCAç¤ºä¾‹ - éçº¿æ€§é™ç»´"""
    
    from sklearn.decomposition import KernelPCA
    from sklearn.datasets import make_circles
    
    print("æ ¸PCAç¤ºä¾‹")
    print("=" * 30)
    
    # ç”Ÿæˆéçº¿æ€§æ•°æ®ï¼ˆåŒå¿ƒåœ†ï¼‰
    X_nonlinear, y_nonlinear = make_circles(n_samples=400, noise=0.1, factor=0.3, random_state=42)
    
    print(f"éçº¿æ€§æ•°æ®å½¢çŠ¶: {X_nonlinear.shape}")
    print(f"æ•°æ®æ ‡ç­¾: {np.unique(y_nonlinear)}")
    
    # ä¼ ç»ŸPCA
    pca_linear = PCA(n_components=2)
    X_pca_linear = pca_linear.fit_transform(X_nonlinear)
    
    print(f"\nä¼ ç»ŸPCAç»“æœ:")
    print(f"è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca_linear.explained_variance_ratio_}")
    
    # æ ¸PCA with RBF kernel
    kpca_rbf = KernelPCA(n_components=2, kernel='rbf', gamma=10)
    X_kpca_rbf = kpca_rbf.fit_transform(X_nonlinear)
    
    # æ ¸PCA with polynomial kernel
    kpca_poly = KernelPCA(n_components=2, kernel='poly', degree=3)
    X_kpca_poly = kpca_poly.fit_transform(X_nonlinear)
    
    print(f"\næ ¸PCAç»“æœ:")
    print(f"RBFæ ¸é™ç»´åå½¢çŠ¶: {X_kpca_rbf.shape}")
    print(f"å¤šé¡¹å¼æ ¸é™ç»´åå½¢çŠ¶: {X_kpca_poly.shape}")
    
    # åˆ†æåˆ†ç¦»æ•ˆæœ
    def compute_separation_score(X_transformed, y):
        """è®¡ç®—ç±»åˆ«åˆ†ç¦»åˆ†æ•°"""
        class_0 = X_transformed[y == 0]
        class_1 = X_transformed[y == 1]
        
        # è®¡ç®—ç±»å†…è·ç¦»
        intra_class_0 = np.mean(np.linalg.norm(class_0 - np.mean(class_0, axis=0), axis=1))
        intra_class_1 = np.mean(np.linalg.norm(class_1 - np.mean(class_1, axis=0), axis=1))
        
        # è®¡ç®—ç±»é—´è·ç¦»
        inter_class = np.linalg.norm(np.mean(class_0, axis=0) - np.mean(class_1, axis=0))
        
        # åˆ†ç¦»åˆ†æ•° = ç±»é—´è·ç¦» / å¹³å‡ç±»å†…è·ç¦»
        separation = inter_class / ((intra_class_0 + intra_class_1) / 2)
        
        return separation
    
    # è®¡ç®—åˆ†ç¦»åˆ†æ•°
    original_separation = compute_separation_score(X_nonlinear, y_nonlinear)
    pca_separation = compute_separation_score(X_pca_linear, y_nonlinear)
    kpca_rbf_separation = compute_separation_score(X_kpca_rbf, y_nonlinear)
    kpca_poly_separation = compute_separation_score(X_kpca_poly, y_nonlinear)
    
    print(f"\nåˆ†ç¦»æ•ˆæœæ¯”è¾ƒ:")
    print(f"åŸå§‹æ•°æ®: {original_separation:.3f}")
    print(f"ä¼ ç»ŸPCA: {pca_separation:.3f}")
    print(f"æ ¸PCA (RBF): {kpca_rbf_separation:.3f}")
    print(f"æ ¸PCA (å¤šé¡¹å¼): {kpca_poly_separation:.3f}")
    
    print(f"\næ ¸PCAçš„ä¼˜åŠ¿:")
    print(f"â€¢ å¯ä»¥å¤„ç†éçº¿æ€§æ•°æ®")
    print(f"â€¢ é€šè¿‡æ ¸æŠ€å·§æ˜ å°„åˆ°é«˜ç»´ç©ºé—´")
    print(f"â€¢ é€‚åˆå¤æ‚çš„æ•°æ®ç»“æ„")
    
    return X_nonlinear, X_kpca_rbf, X_kpca_poly

X_nonlinear, X_kpca_rbf, X_kpca_poly = kernel_pca_example()
```

---

## ğŸ“Š PCAçš„æœ€ä½³å®è·µ

### ä¸»æˆåˆ†æ•°é‡é€‰æ‹©ç­–ç•¥
```python
def pca_component_selection_strategies():
    """PCAä¸»æˆåˆ†æ•°é‡é€‰æ‹©ç­–ç•¥"""
    
    print("PCAä¸»æˆåˆ†æ•°é‡é€‰æ‹©ç­–ç•¥")
    print("=" * 50)
    
    # ç”Ÿæˆç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    X = np.random.randn(500, 50)
    
    # æ·»åŠ ä¸€äº›ç»“æ„
    for i in range(1, 10):
        X[:, i] = X[:, 0] + (0.9 ** i) * np.random.randn(500)
    
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # åº”ç”¨PCA
    pca = PCA()
    pca.fit(X_scaled)
    
    # ç­–ç•¥1: ç´¯ç§¯æ–¹å·®è´¡çŒ®ç‡
    cumsum_ratio = np.cumsum(pca.explained_variance_ratio_)
    
    n_80 = np.argmax(cumsum_ratio >= 0.80) + 1
    n_90 = np.argmax(cumsum_ratio >= 0.90) + 1
    n_95 = np.argmax(cumsum_ratio >= 0.95) + 1
    n_99 = np.argmax(cumsum_ratio >= 0.99) + 1
    
    print("ç­–ç•¥1: ç´¯ç§¯æ–¹å·®è´¡çŒ®ç‡")
    print(f"  80%æ–¹å·®: {n_80} ä¸ªä¸»æˆåˆ†")
    print(f"  90%æ–¹å·®: {n_90} ä¸ªä¸»æˆåˆ†")
    print(f"  95%æ–¹å·®: {n_95} ä¸ªä¸»æˆåˆ†")
    print(f"  99%æ–¹å·®: {n_99} ä¸ªä¸»æˆåˆ†")
    
    # ç­–ç•¥2: Kaiserå‡†åˆ™ (ç‰¹å¾å€¼ > 1)
    eigenvalues = pca.explained_variance_
    n_kaiser = np.sum(eigenvalues > 1)
    
    print(f"\nç­–ç•¥2: Kaiserå‡†åˆ™ (ç‰¹å¾å€¼ > 1)")
    print(f"  æ¨è: {n_kaiser} ä¸ªä¸»æˆåˆ†")
    
    # ç­–ç•¥3: è‚˜éƒ¨æ³•åˆ™
    def find_elbow_point(values):
        """æ‰¾åˆ°è‚˜éƒ¨ç‚¹"""
        n_points = len(values)
        
        # è®¡ç®—æ¯ä¸ªç‚¹åˆ°ç›´çº¿çš„è·ç¦»
        line_start = np.array([0, values[0]])
        line_end = np.array([n_points-1, values[-1]])
        
        distances = []
        for i in range(n_points):
            point = np.array([i, values[i]])
            # ç‚¹åˆ°ç›´çº¿çš„è·ç¦»
            dist = np.abs(np.cross(line_end - line_start, line_start - point)) / np.linalg.norm(line_end - line_start)
            distances.append(dist)
        
        return np.argmax(distances)
    
    elbow_point = find_elbow_point(pca.explained_variance_ratio_)
    
    print(f"\nç­–ç•¥3: è‚˜éƒ¨æ³•åˆ™")
    print(f"  æ¨è: {elbow_point + 1} ä¸ªä¸»æˆåˆ†")
    
    # ç­–ç•¥4: äº¤å‰éªŒè¯
    def cross_validation_pca(X, n_components_range, n_folds=5):
        """ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©ä¸»æˆåˆ†æ•°é‡"""
        from sklearn.model_selection import cross_val_score
        from sklearn.linear_model import Ridge
        from sklearn.pipeline import Pipeline
        
        scores = []
        
        for n_comp in n_components_range:
            # åˆ›å»ºç®¡é“
            pipeline = Pipeline([
                ('pca', PCA(n_components=n_comp)),
                ('ridge', Ridge(random_state=42))
            ])
            
            # ç”Ÿæˆå›å½’ç›®æ ‡ï¼ˆåŸºäºå‰å‡ ä¸ªä¸»æˆåˆ†ï¼‰
            y_target = X[:, 0] + X[:, 1] + 0.1 * np.random.randn(len(X))
            
            # äº¤å‰éªŒè¯
            cv_scores = cross_val_score(pipeline, X, y_target, cv=n_folds, scoring='neg_mean_squared_error')
            scores.append(-cv_scores.mean())
        
        return scores
    
    # æµ‹è¯•ä¸åŒä¸»æˆåˆ†æ•°é‡
    n_components_range = range(1, min(21, X.shape[1]))
    cv_scores = cross_validation_pca(X_scaled, n_components_range)
    
    best_n_components = n_components_range[np.argmin(cv_scores)]
    
    print(f"\nç­–ç•¥4: äº¤å‰éªŒè¯")
    print(f"  æ¨è: {best_n_components} ä¸ªä¸»æˆåˆ†")
    
    # æ€»ç»“å»ºè®®
    print(f"\né€‰æ‹©å»ºè®®:")
    print(f"  æ¢ç´¢æ€§åˆ†æ: {n_95} ä¸ªä¸»æˆåˆ† (95%æ–¹å·®)")
    print(f"  é™ç»´å‹ç¼©: {n_80} ä¸ªä¸»æˆåˆ† (80%æ–¹å·®)")
    print(f"  ç»Ÿè®¡åˆ†æ: {n_kaiser} ä¸ªä¸»æˆåˆ† (Kaiserå‡†åˆ™)")
    print(f"  é¢„æµ‹ä»»åŠ¡: {best_n_components} ä¸ªä¸»æˆåˆ† (äº¤å‰éªŒè¯)")
    
    return {
        'variance_80': n_80,
        'variance_95': n_95,
        'kaiser': n_kaiser,
        'elbow': elbow_point + 1,
        'cv_optimal': best_n_components
    }

selection_results = pca_component_selection_strategies()
```

### PCAçš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹
```python
def pca_pitfalls_and_considerations():
    """PCAçš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹"""
    
    print("PCAçš„é™·é˜±å’Œæ³¨æ„äº‹é¡¹")
    print("=" * 50)
    
    # é™·é˜±1: å¿˜è®°æ ‡å‡†åŒ–
    print("é™·é˜±1: å¿˜è®°æ ‡å‡†åŒ–")
    
    # åˆ›å»ºä¸åŒå°ºåº¦çš„æ•°æ®
    np.random.seed(42)
    X = np.random.randn(100, 3)
    X[:, 0] *= 1000  # ç¬¬ä¸€ä¸ªç‰¹å¾å°ºåº¦å¾ˆå¤§
    X[:, 1] *= 1     # ç¬¬äºŒä¸ªç‰¹å¾æ­£å¸¸å°ºåº¦
    X[:, 2] *= 0.01  # ç¬¬ä¸‰ä¸ªç‰¹å¾å°ºåº¦å¾ˆå°
    
    # ä¸æ ‡å‡†åŒ–çš„PCA
    pca_no_scale = PCA()
    pca_no_scale.fit(X)
    
    # æ ‡å‡†åŒ–çš„PCA
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    pca_scaled = PCA()
    pca_scaled.fit(X_scaled)
    
    print(f"  ä¸æ ‡å‡†åŒ–çš„å‰3ä¸ªä¸»æˆåˆ†æ–¹å·®æ¯”ä¾‹: {pca_no_scale.explained_variance_ratio_}")
    print(f"  æ ‡å‡†åŒ–åçš„å‰3ä¸ªä¸»æˆåˆ†æ–¹å·®æ¯”ä¾‹: {pca_scaled.explained_variance_ratio_}")
    print(f"  ç»“è®º: å¤§å°ºåº¦ç‰¹å¾ä¼šä¸»å¯¼ä¸»æˆåˆ†")
    
    # é™·é˜±2: æ•°æ®æ³„éœ²
    print(f"\né™·é˜±2: æ•°æ®æ³„éœ²")
    
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.datasets import make_classification
    
    # ç”Ÿæˆåˆ†ç±»æ•°æ®
    X_class, y_class = make_classification(n_samples=1000, n_features=20, n_informative=10, random_state=42)
    
    # é”™è¯¯æ–¹å¼: åœ¨åˆ†å‰²å‰åº”ç”¨PCA
    pca_wrong = PCA(n_components=10)
    X_pca_wrong = pca_wrong.fit_transform(X_class)
    X_train_wrong, X_test_wrong, y_train, y_test = train_test_split(
        X_pca_wrong, y_class, test_size=0.2, random_state=42
    )
    
    clf_wrong = LogisticRegression(random_state=42)
    clf_wrong.fit(X_train_wrong, y_train)
    accuracy_wrong = clf_wrong.score(X_test_wrong, y_test)
    
    # æ­£ç¡®æ–¹å¼: åªåœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆPCA
    X_train_right, X_test_right, y_train, y_test = train_test_split(
        X_class, y_class, test_size=0.2, random_state=42
    )
    
    pca_right = PCA(n_components=10)
    X_train_pca_right = pca_right.fit_transform(X_train_right)
    X_test_pca_right = pca_right.transform(X_test_right)
    
    clf_right = LogisticRegression(random_state=42)
    clf_right.fit(X_train_pca_right, y_train)
    accuracy_right = clf_right.score(X_test_pca_right, y_test)
    
    print(f"  é”™è¯¯æ–¹å¼å‡†ç¡®ç‡: {accuracy_wrong:.3f}")
    print(f"  æ­£ç¡®æ–¹å¼å‡†ç¡®ç‡: {accuracy_right:.3f}")
    print(f"  ç»“è®º: æ•°æ®æ³„éœ²ä¼šå¯¼è‡´è¿‡åº¦ä¹è§‚çš„ç»“æœ")
    
    # é™·é˜±3: å¿½è§†å¼‚å¸¸å€¼
    print(f"\né™·é˜±3: å¿½è§†å¼‚å¸¸å€¼")
    
    # åˆ›å»ºå¸¦å¼‚å¸¸å€¼çš„æ•°æ®
    X_normal = np.random.randn(100, 2)
    X_outlier = np.copy(X_normal)
    X_outlier[0] = [10, 10]  # æ·»åŠ å¼‚å¸¸å€¼
    
    # æ­£å¸¸æ•°æ®çš„PCA
    pca_normal = PCA()
    pca_normal.fit(X_normal)
    
    # å¸¦å¼‚å¸¸å€¼æ•°æ®çš„PCA
    pca_outlier = PCA()
    pca_outlier.fit(X_outlier)
    
    print(f"  æ­£å¸¸æ•°æ®çš„ä¸»æˆåˆ†æ–¹å‘: {pca_normal.components_[0]}")
    print(f"  å¼‚å¸¸å€¼æ•°æ®çš„ä¸»æˆåˆ†æ–¹å‘: {pca_outlier.components_[0]}")
    print(f"  ç»“è®º: å¼‚å¸¸å€¼ä¼šæ˜¾è‘—å½±å“ä¸»æˆåˆ†æ–¹å‘")
    
    # é™·é˜±4: è¿‡åº¦è§£é‡Šä¸»æˆåˆ†
    print(f"\né™·é˜±4: è¿‡åº¦è§£é‡Šä¸»æˆåˆ†")
    
    # ç”Ÿæˆéšæœºæ•°æ®
    X_random = np.random.randn(100, 10)
    pca_random = PCA()
    pca_random.fit(X_random)
    
    print(f"  éšæœºæ•°æ®çš„è§£é‡Šæ–¹å·®æ¯”ä¾‹: {pca_random.explained_variance_ratio_[:3]}")
    print(f"  ç»“è®º: å³ä½¿æ˜¯éšæœºæ•°æ®ï¼Œä¸»æˆåˆ†ä¹Ÿä¼šæ˜¾ç¤ºæŸäº›'ç»“æ„'")
    
    # æœ€ä½³å®è·µå»ºè®®
    print(f"\næœ€ä½³å®è·µå»ºè®®:")
    print(f"  1. æ€»æ˜¯æ ‡å‡†åŒ–æ•°æ®ï¼ˆé™¤éæœ‰ç‰¹æ®ŠåŸå› ï¼‰")
    print(f"  2. æ£€æŸ¥å’Œå¤„ç†å¼‚å¸¸å€¼")
    print(f"  3. é¿å…æ•°æ®æ³„éœ²ï¼šåªåœ¨è®­ç»ƒé›†ä¸Šæ‹ŸåˆPCA")
    print(f"  4. è°¨æ…è§£é‡Šä¸»æˆåˆ†çš„å®é™…æ„ä¹‰")
    print(f"  5. ä½¿ç”¨å¤šç§æ–¹æ³•é€‰æ‹©ä¸»æˆåˆ†æ•°é‡")
    print(f"  6. æ£€æŸ¥å‡è®¾ï¼šçº¿æ€§å…³ç³»ã€æ­£æ€åˆ†å¸ƒ")
    print(f"  7. è€ƒè™‘ä½¿ç”¨ç¨³å¥çš„PCAå˜ä½“å¤„ç†å¼‚å¸¸å€¼")

pca_pitfalls_and_considerations()
```

---

## ğŸ“š æ€»ç»“ä¸å»ºè®®

### PCAçš„ä¼˜ç¼ºç‚¹æ€»ç»“
```python
def pca_summary():
    """PCAçš„ä¼˜ç¼ºç‚¹æ€»ç»“"""
    
    print("PCAä¼˜ç¼ºç‚¹æ€»ç»“")
    print("=" * 50)
    
    advantages = [
        "é™ç»´æ•ˆæœå¥½ï¼šä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯",
        "æ¶ˆé™¤ç›¸å…³æ€§ï¼šä¸»æˆåˆ†ä¹‹é—´æ­£äº¤",
        "æ•°æ®å‹ç¼©ï¼šå‡å°‘å­˜å‚¨ç©ºé—´",
        "å™ªå£°è¿‡æ»¤ï¼šå»é™¤å°çš„ä¸»æˆåˆ†",
        "å¯è§†åŒ–å‹å¥½ï¼šé™åˆ°2D/3Dä¾¿äºè§‚å¯Ÿ",
        "è®¡ç®—æ•ˆç‡ï¼šåŸºäºçº¿æ€§ä»£æ•°ï¼Œé€Ÿåº¦å¿«",
        "ç†è®ºåŸºç¡€ï¼šæ•°å­¦åŸç†æ¸…æ™°"
    ]
    
    disadvantages = [
        "çº¿æ€§å‡è®¾ï¼šåªèƒ½æ•æ‰çº¿æ€§å…³ç³»",
        "ä¸»æˆåˆ†è§£é‡Šï¼šéš¾ä»¥ç†è§£å®é™…æ„ä¹‰",
        "å‚æ•°é€‰æ‹©ï¼šä¸»æˆåˆ†æ•°é‡éœ€è¦è°ƒä¼˜",
        "æ ‡å‡†åŒ–æ•æ„Ÿï¼šä¸åŒå°ºåº¦å½±å“ç»“æœ",
        "å¼‚å¸¸å€¼æ•æ„Ÿï¼šç¦»ç¾¤ç‚¹å½±å“ä¸»æˆåˆ†",
        "ä¿¡æ¯ä¸¢å¤±ï¼šä¸å¯é€†çš„ä¿¡æ¯æŸå¤±",
        "å…¨å±€æ–¹æ³•ï¼šéœ€è¦çœ‹åˆ°æ‰€æœ‰æ•°æ®"
    ]
    
    print("ä¼˜ç‚¹:")
    for i, advantage in enumerate(advantages, 1):
        print(f"  {i}. {advantage}")
    
    print("\nç¼ºç‚¹:")
    for i, disadvantage in enumerate(disadvantages, 1):
        print(f"  {i}. {disadvantage}")
    
    # é€‚ç”¨åœºæ™¯
    print(f"\né€‚ç”¨åœºæ™¯:")
    use_cases = [
        "æ¢ç´¢æ€§æ•°æ®åˆ†æï¼šç†è§£æ•°æ®ç»“æ„",
        "æ•°æ®å¯è§†åŒ–ï¼šé«˜ç»´æ•°æ®é™ç»´å±•ç¤º",
        "ç‰¹å¾æå–ï¼šå‡å°‘ç‰¹å¾æ•°é‡",
        "æ•°æ®å‹ç¼©ï¼šå›¾åƒã€éŸ³é¢‘ç­‰æ•°æ®å‹ç¼©",
        "å™ªå£°è¿‡æ»¤ï¼šå»é™¤æ•°æ®ä¸­çš„å™ªå£°",
        "é¢„å¤„ç†æ­¥éª¤ï¼šä¸ºå…¶ä»–ç®—æ³•é™ç»´",
        "åæ–¹å·®åˆ†æï¼šç†è§£å˜é‡é—´å…³ç³»"
    ]
    
    for i, use_case in enumerate(use_cases, 1):
        print(f"  {i}. {use_case}")
    
    # æ›¿ä»£æ–¹æ³•
    print(f"\næ›¿ä»£æ–¹æ³•:")
    alternatives = [
        "t-SNEï¼šéçº¿æ€§é™ç»´ï¼Œé€‚åˆå¯è§†åŒ–",
        "UMAPï¼šå¿«é€Ÿçš„éçº¿æ€§é™ç»´",
        "ICAï¼šç‹¬ç«‹æˆåˆ†åˆ†æ",
        "NMFï¼šéè´ŸçŸ©é˜µåˆ†è§£",
        "Autoencoderï¼šç¥ç»ç½‘ç»œé™ç»´",
        "LDAï¼šçº¿æ€§åˆ¤åˆ«åˆ†æï¼ˆç›‘ç£ï¼‰",
        "æ ¸PCAï¼šå¤„ç†éçº¿æ€§æ•°æ®"
    ]
    
    for i, alternative in enumerate(alternatives, 1):
        print(f"  {i}. {alternative}")

pca_summary()
```

---

## ğŸ¯ å­¦ä¹ å»ºè®®

### æŒæ¡PCAçš„å…³é”®æ­¥éª¤
1. **ç†è§£æ•°å­¦åŸç†**ï¼šåæ–¹å·®çŸ©é˜µã€ç‰¹å¾å€¼åˆ†è§£
2. **å®è·µç¼–ç¨‹å®ç°**ï¼šæ‰‹å·¥å®ç°åŠ æ·±ç†è§£
3. **ç†Ÿæ‚‰å·¥å…·ä½¿ç”¨**ï¼šscikit-learnçš„PCAç±»
4. **æŒæ¡å‚æ•°è°ƒä¼˜**ï¼šä¸»æˆåˆ†æ•°é‡é€‰æ‹©ç­–ç•¥
5. **ç†è§£åº”ç”¨åœºæ™¯**ï¼šçŸ¥é“ä½•æ—¶ä½¿ç”¨PCA
6. **æ³¨æ„å¸¸è§é™·é˜±**ï¼šæ ‡å‡†åŒ–ã€æ•°æ®æ³„éœ²ç­‰

### æ·±å…¥å­¦ä¹ è·¯å¾„
1. **æ•°å­¦åŸºç¡€**ï¼šçº¿æ€§ä»£æ•°ã€ç»Ÿè®¡å­¦åŸºç¡€
2. **ç›¸å…³ç®—æ³•**ï¼šSVDã€ICAã€NMF
3. **åº”ç”¨é¢†åŸŸ**ï¼šå›¾åƒå¤„ç†ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€ç”Ÿç‰©ä¿¡æ¯å­¦
4. **é«˜çº§æŠ€å·§**ï¼šç¨€ç–PCAã€é²æ£’PCAã€åœ¨çº¿PCA

---

**ğŸ” è®°ä½ï¼šPCAæ˜¯æ•°æ®ç§‘å­¦å®¶çš„åŸºæœ¬å·¥å…·ï¼ŒæŒæ¡å®ƒçš„åŸç†å’Œåº”ç”¨æ˜¯è¿›å…¥æœºå™¨å­¦ä¹ é¢†åŸŸçš„é‡è¦ä¸€æ­¥ï¼** 