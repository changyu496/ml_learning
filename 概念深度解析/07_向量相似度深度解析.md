# ğŸ“ å‘é‡ç›¸ä¼¼åº¦æ·±åº¦è§£æ

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> **å‘é‡ç›¸ä¼¼åº¦æ˜¯è¡¡é‡ä¸¤ä¸ªå‘é‡æ¥è¿‘ç¨‹åº¦çš„é‡è¦æŒ‡æ ‡ï¼Œæ˜¯æ¨èç³»ç»Ÿã€ä¿¡æ¯æ£€ç´¢å’Œè‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€**

### ä»€ä¹ˆæ˜¯å‘é‡ç›¸ä¼¼åº¦ï¼Ÿ
**å®šä¹‰**ï¼šå‘é‡ç›¸ä¼¼åº¦æ˜¯ç”¨æ¥è¡¡é‡ä¸¤ä¸ªå‘é‡åœ¨æ–¹å‘ã€å¤§å°æˆ–æ¨¡å¼ä¸Šç›¸ä¼¼ç¨‹åº¦çš„æ•°å€¼æŒ‡æ ‡ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šé€šè¿‡æ•°å­¦è®¡ç®—é‡åŒ–ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼æ€§ï¼Œæ•°å€¼è¶Šé«˜è¡¨ç¤ºè¶Šç›¸ä¼¼ã€‚

**åº”ç”¨åœºæ™¯**ï¼šæ¨èç³»ç»Ÿã€æœç´¢å¼•æ“ã€èšç±»åˆ†æã€å›¾åƒè¯†åˆ«ç­‰ã€‚

---

## ğŸ§  å‡ ä½•ç›´è§‰ç†è§£

### å‘é‡çš„å‡ ä½•è¡¨ç¤º
```python
import numpy as np
import matplotlib.pyplot as plt

def vector_geometry_basics():
    """ç†è§£å‘é‡çš„å‡ ä½•è¡¨ç¤º"""
    
    # å®šä¹‰å‡ ä¸ªäºŒç»´å‘é‡
    v1 = np.array([3, 4])
    v2 = np.array([4, 3])
    v3 = np.array([6, 8])
    v4 = np.array([-2, 1])
    
    vectors = [v1, v2, v3, v4]
    labels = ['v1', 'v2', 'v3', 'v4']
    
    print("å‘é‡çš„å‡ ä½•è¡¨ç¤º")
    print("=" * 30)
    
    for i, (v, label) in enumerate(zip(vectors, labels)):
        # å‘é‡é•¿åº¦
        length = np.linalg.norm(v)
        
        # å‘é‡æ–¹å‘ï¼ˆå•ä½å‘é‡ï¼‰
        direction = v / length
        
        # ä¸xè½´çš„è§’åº¦
        angle = np.arctan2(v[1], v[0])
        angle_degrees = np.degrees(angle)
        
        print(f"{label} = {v}")
        print(f"  é•¿åº¦: {length:.3f}")
        print(f"  æ–¹å‘: {direction}")
        print(f"  è§’åº¦: {angle_degrees:.1f}Â°")
        print()
    
    # å‘é‡é—´çš„å…³ç³»
    print("å‘é‡é—´çš„å…³ç³»åˆ†æ:")
    print("-" * 20)
    
    # v1 å’Œ v3 çš„å…³ç³»
    print(f"v1 = {v1}, v3 = {v3}")
    print(f"v3 æ˜¯å¦ä¸º v1 çš„å€æ•°: {np.allclose(v3, 2 * v1)}")
    print(f"å®ƒä»¬æ–¹å‘ç›¸åŒï¼Œä½†é•¿åº¦ä¸åŒ")
    
    # v1 å’Œ v2 çš„å…³ç³»
    print(f"\nv1 = {v1}, v2 = {v2}")
    print(f"å®ƒä»¬é•¿åº¦ç›¸è¿‘ä½†æ–¹å‘ä¸åŒ")
    
    # v1 å’Œ v4 çš„å…³ç³»
    print(f"\nv1 = {v1}, v4 = {v4}")
    print(f"å®ƒä»¬æ—¢ä¸å¹³è¡Œä¹Ÿä¸å‚ç›´")
    
    return vectors

vectors = vector_geometry_basics()
```

---

## ğŸ“ ä¸»è¦ç›¸ä¼¼åº¦åº¦é‡æ–¹æ³•

### 1. æ¬§æ°è·ç¦» (Euclidean Distance)
```python
def euclidean_distance_analysis():
    """æ¬§æ°è·ç¦»è¯¦è§£"""
    
    print("æ¬§æ°è·ç¦» (Euclidean Distance)")
    print("=" * 50)
    
    # å®šä¹‰å‘é‡
    v1 = np.array([1, 2, 3])
    v2 = np.array([4, 5, 6])
    v3 = np.array([1, 2, 4])
    
    # æ‰‹å·¥è®¡ç®—æ¬§æ°è·ç¦»
    def euclidean_distance(a, b):
        return np.sqrt(np.sum((a - b) ** 2))
    
    # è®¡ç®—è·ç¦»
    dist_12 = euclidean_distance(v1, v2)
    dist_13 = euclidean_distance(v1, v3)
    dist_23 = euclidean_distance(v2, v3)
    
    print(f"v1 = {v1}")
    print(f"v2 = {v2}")
    print(f"v3 = {v3}")
    print()
    print(f"æ¬§æ°è·ç¦»:")
    print(f"  d(v1, v2) = {dist_12:.3f}")
    print(f"  d(v1, v3) = {dist_13:.3f}")
    print(f"  d(v2, v3) = {dist_23:.3f}")
    
    # éªŒè¯ä¸numpyçš„ç»“æœ
    print(f"\nNumPyéªŒè¯:")
    print(f"  d(v1, v2) = {np.linalg.norm(v1 - v2):.3f}")
    print(f"  d(v1, v3) = {np.linalg.norm(v1 - v3):.3f}")
    print(f"  d(v2, v3) = {np.linalg.norm(v2 - v3):.3f}")
    
    # æ¬§æ°è·ç¦»çš„æ€§è´¨
    print(f"\næ¬§æ°è·ç¦»çš„æ€§è´¨:")
    print(f"1. éè´Ÿæ€§: d(a,b) â‰¥ 0")
    print(f"2. å¯¹ç§°æ€§: d(a,b) = d(b,a)")
    print(f"3. ä¸‰è§’ä¸ç­‰å¼: d(a,c) â‰¤ d(a,b) + d(b,c)")
    print(f"4. è·ç¦»ä¸º0å½“ä¸”ä»…å½“å‘é‡ç›¸ç­‰")
    
    # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
    # ç›¸ä¼¼åº¦ = 1 / (1 + è·ç¦»)
    sim_12 = 1 / (1 + dist_12)
    sim_13 = 1 / (1 + dist_13)
    sim_23 = 1 / (1 + dist_23)
    
    print(f"\nè½¬æ¢ä¸ºç›¸ä¼¼åº¦ (1/(1+è·ç¦»)):")
    print(f"  sim(v1, v2) = {sim_12:.3f}")
    print(f"  sim(v1, v3) = {sim_13:.3f}")
    print(f"  sim(v2, v3) = {sim_23:.3f}")
    
    # åº”ç”¨åœºæ™¯
    print(f"\nåº”ç”¨åœºæ™¯:")
    print(f"â€¢ èšç±»ç®—æ³• (K-means)")
    print(f"â€¢ æœ€è¿‘é‚»åˆ†ç±»")
    print(f"â€¢ å¼‚å¸¸æ£€æµ‹")
    print(f"â€¢ å›¾åƒå¤„ç†")
    
    return dist_12, dist_13, dist_23

euclidean_distances = euclidean_distance_analysis()
```

### 2. ä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)
```python
def cosine_similarity_analysis():
    """ä½™å¼¦ç›¸ä¼¼åº¦è¯¦è§£"""
    
    print("\nä½™å¼¦ç›¸ä¼¼åº¦ (Cosine Similarity)")
    print("=" * 50)
    
    # å®šä¹‰å‘é‡
    v1 = np.array([1, 2, 3])
    v2 = np.array([2, 4, 6])  # v1 çš„ 2å€
    v3 = np.array([1, 0, 0])  # ä¸v1å‚ç›´
    v4 = np.array([-1, -2, -3])  # v1 çš„åå‘
    
    # æ‰‹å·¥è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    def cosine_similarity(a, b):
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        return dot_product / (norm_a * norm_b)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    cos_12 = cosine_similarity(v1, v2)
    cos_13 = cosine_similarity(v1, v3)
    cos_14 = cosine_similarity(v1, v4)
    
    print(f"v1 = {v1}")
    print(f"v2 = {v2} (v1çš„2å€)")
    print(f"v3 = {v3} (ä¸v1å‚ç›´)")
    print(f"v4 = {v4} (v1çš„åå‘)")
    print()
    print(f"ä½™å¼¦ç›¸ä¼¼åº¦:")
    print(f"  cos(v1, v2) = {cos_12:.3f}")
    print(f"  cos(v1, v3) = {cos_13:.3f}")
    print(f"  cos(v1, v4) = {cos_14:.3f}")
    
    # å‡ ä½•æ„ä¹‰
    print(f"\nå‡ ä½•æ„ä¹‰:")
    print(f"â€¢ cos = 1: å®Œå…¨ç›¸åŒæ–¹å‘")
    print(f"â€¢ cos = 0: å‚ç›´")
    print(f"â€¢ cos = -1: å®Œå…¨ç›¸åæ–¹å‘")
    
    # è§’åº¦è®¡ç®—
    angle_12 = np.arccos(np.clip(cos_12, -1, 1))
    angle_13 = np.arccos(np.clip(cos_13, -1, 1))
    angle_14 = np.arccos(np.clip(cos_14, -1, 1))
    
    print(f"\nå¯¹åº”è§’åº¦:")
    print(f"  âˆ (v1, v2) = {np.degrees(angle_12):.1f}Â°")
    print(f"  âˆ (v1, v3) = {np.degrees(angle_13):.1f}Â°")
    print(f"  âˆ (v1, v4) = {np.degrees(angle_14):.1f}Â°")
    
    # ä¸æ¬§æ°è·ç¦»çš„æ¯”è¾ƒ
    print(f"\nä¸æ¬§æ°è·ç¦»çš„æ¯”è¾ƒ:")
    print(f"ä½™å¼¦ç›¸ä¼¼åº¦å…³æ³¨æ–¹å‘ï¼Œæ¬§æ°è·ç¦»å…³æ³¨ä½ç½®")
    
    euclidean_12 = np.linalg.norm(v1 - v2)
    euclidean_13 = np.linalg.norm(v1 - v3)
    
    print(f"  v1 vs v2: ä½™å¼¦={cos_12:.3f}, æ¬§æ°={euclidean_12:.3f}")
    print(f"  v1 vs v3: ä½™å¼¦={cos_13:.3f}, æ¬§æ°={euclidean_13:.3f}")
    print(f"  v1å’Œv2æ–¹å‘ç›¸åŒä½†è·ç¦»è¿œï¼Œv1å’Œv3è·ç¦»è¿‘ä½†æ–¹å‘ä¸åŒ")
    
    # åº”ç”¨åœºæ™¯
    print(f"\nåº”ç”¨åœºæ™¯:")
    print(f"â€¢ æ–‡æœ¬ç›¸ä¼¼åº¦")
    print(f"â€¢ æ¨èç³»ç»Ÿ")
    print(f"â€¢ ä¿¡æ¯æ£€ç´¢")
    print(f"â€¢ å›¾åƒè¯†åˆ«")
    
    return cos_12, cos_13, cos_14

cosine_similarities = cosine_similarity_analysis()
```

### 3. ç‚¹ç§¯ç›¸ä¼¼åº¦ (Dot Product)
```python
def dot_product_similarity_analysis():
    """ç‚¹ç§¯ç›¸ä¼¼åº¦è¯¦è§£"""
    
    print("\nç‚¹ç§¯ç›¸ä¼¼åº¦ (Dot Product)")
    print("=" * 50)
    
    # å®šä¹‰å‘é‡
    v1 = np.array([1, 2, 3])
    v2 = np.array([2, 4, 6])
    v3 = np.array([1, 0, 0])
    v4 = np.array([0, 1, 0])
    
    # è®¡ç®—ç‚¹ç§¯
    dot_12 = np.dot(v1, v2)
    dot_13 = np.dot(v1, v3)
    dot_14 = np.dot(v1, v4)
    dot_34 = np.dot(v3, v4)
    
    print(f"v1 = {v1}")
    print(f"v2 = {v2}")
    print(f"v3 = {v3}")
    print(f"v4 = {v4}")
    print()
    print(f"ç‚¹ç§¯:")
    print(f"  v1 Â· v2 = {dot_12}")
    print(f"  v1 Â· v3 = {dot_13}")
    print(f"  v1 Â· v4 = {dot_14}")
    print(f"  v3 Â· v4 = {dot_34}")
    
    # ç‚¹ç§¯çš„è®¡ç®—æ–¹æ³•
    print(f"\nç‚¹ç§¯çš„è®¡ç®—æ–¹æ³•:")
    print(f"æ–¹æ³•1: å¯¹åº”å…ƒç´ ç›¸ä¹˜å†æ±‚å’Œ")
    print(f"  v1 Â· v2 = 1Ã—2 + 2Ã—4 + 3Ã—6 = {1*2 + 2*4 + 3*6}")
    
    print(f"\næ–¹æ³•2: å‡ ä½•å…¬å¼")
    print(f"  v1 Â· v2 = ||v1|| Ã— ||v2|| Ã— cos(Î¸)")
    norm_v1 = np.linalg.norm(v1)
    norm_v2 = np.linalg.norm(v2)
    cos_angle = dot_12 / (norm_v1 * norm_v2)
    print(f"  = {norm_v1:.3f} Ã— {norm_v2:.3f} Ã— {cos_angle:.3f} = {norm_v1 * norm_v2 * cos_angle:.3f}")
    
    # ç‚¹ç§¯çš„æ€§è´¨
    print(f"\nç‚¹ç§¯çš„æ€§è´¨:")
    print(f"1. äº¤æ¢å¾‹: a Â· b = b Â· a")
    print(f"2. åˆ†é…å¾‹: a Â· (b + c) = a Â· b + a Â· c")
    print(f"3. æ ‡é‡ä¹˜æ³•: (ka) Â· b = k(a Â· b)")
    print(f"4. æ­£äº¤æ€§: å¦‚æœ a Â· b = 0ï¼Œåˆ™ a âŠ¥ b")
    
    # éªŒè¯æ­£äº¤æ€§
    print(f"\næ­£äº¤æ€§éªŒè¯:")
    print(f"  v3 Â· v4 = {dot_34} (v3 å’Œ v4 æ­£äº¤)")
    
    # ç‚¹ç§¯ä¸ç›¸ä¼¼åº¦çš„å…³ç³»
    print(f"\nç‚¹ç§¯ä¸ç›¸ä¼¼åº¦çš„å…³ç³»:")
    print(f"â€¢ ç‚¹ç§¯ > 0: å¤¹è§’ < 90Â°, æ–¹å‘ç›¸ä¼¼")
    print(f"â€¢ ç‚¹ç§¯ = 0: å¤¹è§’ = 90Â°, æ­£äº¤")
    print(f"â€¢ ç‚¹ç§¯ < 0: å¤¹è§’ > 90Â°, æ–¹å‘ç›¸å")
    
    # åº”ç”¨åœºæ™¯
    print(f"\nåº”ç”¨åœºæ™¯:")
    print(f"â€¢ ç¥ç»ç½‘ç»œ (æƒé‡è®¡ç®—)")
    print(f"â€¢ çº¿æ€§ä»£æ•°è¿ç®—")
    print(f"â€¢ ç‰©ç†å­¦ (åŠŸçš„è®¡ç®—)")
    print(f"â€¢ æœºå™¨å­¦ä¹  (ç‰¹å¾é‡è¦æ€§)")
    
    return dot_12, dot_13, dot_14

dot_products = dot_product_similarity_analysis()
```

### 4. æ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)
```python
def manhattan_distance_analysis():
    """æ›¼å“ˆé¡¿è·ç¦»è¯¦è§£"""
    
    print("\næ›¼å“ˆé¡¿è·ç¦» (Manhattan Distance)")
    print("=" * 50)
    
    # å®šä¹‰å‘é‡
    v1 = np.array([1, 2, 3])
    v2 = np.array([4, 5, 6])
    v3 = np.array([1, 2, 4])
    
    # è®¡ç®—æ›¼å“ˆé¡¿è·ç¦»
    def manhattan_distance(a, b):
        return np.sum(np.abs(a - b))
    
    # è®¡ç®—è·ç¦»
    man_12 = manhattan_distance(v1, v2)
    man_13 = manhattan_distance(v1, v3)
    man_23 = manhattan_distance(v2, v3)
    
    print(f"v1 = {v1}")
    print(f"v2 = {v2}")
    print(f"v3 = {v3}")
    print()
    print(f"æ›¼å“ˆé¡¿è·ç¦»:")
    print(f"  d_M(v1, v2) = {man_12}")
    print(f"  d_M(v1, v3) = {man_13}")
    print(f"  d_M(v2, v3) = {man_23}")
    
    # è®¡ç®—è¿‡ç¨‹å±•ç¤º
    print(f"\nè®¡ç®—è¿‡ç¨‹ (v1, v2):")
    diff = np.abs(v1 - v2)
    print(f"  |v1 - v2| = {diff}")
    print(f"  sum = {diff[0]} + {diff[1]} + {diff[2]} = {man_12}")
    
    # ä¸æ¬§æ°è·ç¦»æ¯”è¾ƒ
    euclidean_12 = np.linalg.norm(v1 - v2)
    euclidean_13 = np.linalg.norm(v1 - v3)
    
    print(f"\nä¸æ¬§æ°è·ç¦»çš„æ¯”è¾ƒ:")
    print(f"  v1 vs v2: æ›¼å“ˆé¡¿={man_12}, æ¬§æ°={euclidean_12:.3f}")
    print(f"  v1 vs v3: æ›¼å“ˆé¡¿={man_13}, æ¬§æ°={euclidean_13:.3f}")
    
    # å‡ ä½•æ„ä¹‰
    print(f"\nå‡ ä½•æ„ä¹‰:")
    print(f"â€¢ æ›¼å“ˆé¡¿è·ç¦»: åŸå¸‚è¡—åŒºè·ç¦»ï¼Œåªèƒ½æ²¿åæ ‡è½´ç§»åŠ¨")
    print(f"â€¢ æ¬§æ°è·ç¦»: ç›´çº¿è·ç¦»ï¼Œå¯ä»¥ä»»æ„æ–¹å‘ç§»åŠ¨")
    
    # åº”ç”¨åœºæ™¯
    print(f"\nåº”ç”¨åœºæ™¯:")
    print(f"â€¢ å‡ºç§Ÿè½¦è·¯å¾„è§„åˆ’")
    print(f"â€¢ å›¾åƒå¤„ç† (åƒç´ å·®å¼‚)")
    print(f"â€¢ ç¨€ç–æ•°æ®å¤„ç†")
    print(f"â€¢ å¼‚å¸¸æ£€æµ‹")
    
    return man_12, man_13, man_23

manhattan_distances = manhattan_distance_analysis()
```

### 5. çš®å°”é€Šç›¸å…³ç³»æ•° (Pearson Correlation)
```python
def pearson_correlation_analysis():
    """çš®å°”é€Šç›¸å…³ç³»æ•°è¯¦è§£"""
    
    print("\nçš®å°”é€Šç›¸å…³ç³»æ•° (Pearson Correlation)")
    print("=" * 50)
    
    # å®šä¹‰å‘é‡
    v1 = np.array([1, 2, 3, 4, 5])
    v2 = np.array([2, 4, 6, 8, 10])  # å®Œå…¨æ­£ç›¸å…³
    v3 = np.array([5, 4, 3, 2, 1])   # å®Œå…¨è´Ÿç›¸å…³
    v4 = np.array([1, 5, 2, 4, 3])   # æ— ç›¸å…³
    
    # æ‰‹å·¥è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
    def pearson_correlation(a, b):
        # ä¸­å¿ƒåŒ–
        a_centered = a - np.mean(a)
        b_centered = b - np.mean(b)
        
        # åæ–¹å·®
        covariance = np.mean(a_centered * b_centered)
        
        # æ ‡å‡†å·®
        std_a = np.std(a)
        std_b = np.std(b)
        
        # ç›¸å…³ç³»æ•°
        correlation = covariance / (std_a * std_b)
        
        return correlation
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    corr_12 = pearson_correlation(v1, v2)
    corr_13 = pearson_correlation(v1, v3)
    corr_14 = pearson_correlation(v1, v4)
    
    print(f"v1 = {v1}")
    print(f"v2 = {v2} (å®Œå…¨æ­£ç›¸å…³)")
    print(f"v3 = {v3} (å®Œå…¨è´Ÿç›¸å…³)")
    print(f"v4 = {v4} (æ— ç›¸å…³)")
    print()
    print(f"çš®å°”é€Šç›¸å…³ç³»æ•°:")
    print(f"  r(v1, v2) = {corr_12:.3f}")
    print(f"  r(v1, v3) = {corr_13:.3f}")
    print(f"  r(v1, v4) = {corr_14:.3f}")
    
    # ä½¿ç”¨NumPyéªŒè¯
    print(f"\nNumPyéªŒè¯:")
    print(f"  r(v1, v2) = {np.corrcoef(v1, v2)[0,1]:.3f}")
    print(f"  r(v1, v3) = {np.corrcoef(v1, v3)[0,1]:.3f}")
    print(f"  r(v1, v4) = {np.corrcoef(v1, v4)[0,1]:.3f}")
    
    # ç›¸å…³ç³»æ•°çš„æ„ä¹‰
    print(f"\nç›¸å…³ç³»æ•°çš„æ„ä¹‰:")
    print(f"â€¢ r = 1: å®Œå…¨æ­£ç›¸å…³")
    print(f"â€¢ r = 0: æ— çº¿æ€§ç›¸å…³")
    print(f"â€¢ r = -1: å®Œå…¨è´Ÿç›¸å…³")
    print(f"â€¢ |r| > 0.7: å¼ºç›¸å…³")
    print(f"â€¢ 0.3 < |r| < 0.7: ä¸­ç­‰ç›¸å…³")
    print(f"â€¢ |r| < 0.3: å¼±ç›¸å…³")
    
    # è¯¦ç»†è®¡ç®—è¿‡ç¨‹
    print(f"\nè¯¦ç»†è®¡ç®—è¿‡ç¨‹ (v1, v2):")
    v1_mean = np.mean(v1)
    v2_mean = np.mean(v2)
    print(f"  v1å‡å€¼: {v1_mean}")
    print(f"  v2å‡å€¼: {v2_mean}")
    
    v1_centered = v1 - v1_mean
    v2_centered = v2 - v2_mean
    print(f"  v1ä¸­å¿ƒåŒ–: {v1_centered}")
    print(f"  v2ä¸­å¿ƒåŒ–: {v2_centered}")
    
    covariance = np.mean(v1_centered * v2_centered)
    std_v1 = np.std(v1)
    std_v2 = np.std(v2)
    print(f"  åæ–¹å·®: {covariance:.3f}")
    print(f"  v1æ ‡å‡†å·®: {std_v1:.3f}")
    print(f"  v2æ ‡å‡†å·®: {std_v2:.3f}")
    print(f"  ç›¸å…³ç³»æ•°: {covariance:.3f} / ({std_v1:.3f} Ã— {std_v2:.3f}) = {corr_12:.3f}")
    
    # åº”ç”¨åœºæ™¯
    print(f"\nåº”ç”¨åœºæ™¯:")
    print(f"â€¢ é‡‘èåˆ†æ (è‚¡ç¥¨ç›¸å…³æ€§)")
    print(f"â€¢ æ¨èç³»ç»Ÿ (ç”¨æˆ·ç›¸ä¼¼æ€§)")
    print(f"â€¢ ç‰¹å¾é€‰æ‹© (ç‰¹å¾ç›¸å…³æ€§)")
    print(f"â€¢ è´¨é‡æ§åˆ¶ (å˜é‡å…³ç³»)")
    
    return corr_12, corr_13, corr_14

pearson_correlations = pearson_correlation_analysis()
```

---

## ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹1ï¼šæ¨èç³»ç»Ÿä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—
```python
def recommendation_system_similarity():
    """æ¨èç³»ç»Ÿä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—"""
    
    print("æ¨èç³»ç»Ÿä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—")
    print("=" * 50)
    
    # ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ
    # è¡Œ: ç”¨æˆ·, åˆ—: ç‰©å“
    ratings = np.array([
        [5, 3, 0, 1, 4],  # ç”¨æˆ·1
        [4, 0, 0, 1, 5],  # ç”¨æˆ·2
        [1, 1, 0, 5, 4],  # ç”¨æˆ·3
        [0, 0, 5, 4, 0],  # ç”¨æˆ·4
        [0, 3, 4, 4, 3]   # ç”¨æˆ·5
    ])
    
    users = ['Alice', 'Bob', 'Carol', 'Dave', 'Eve']
    items = ['ç”µå½±A', 'ç”µå½±B', 'ç”µå½±C', 'ç”µå½±D', 'ç”µå½±E']
    
    print("ç”¨æˆ·-ç‰©å“è¯„åˆ†çŸ©é˜µ:")
    print("       ", " ".join(f"{item:>6}" for item in items))
    for i, user in enumerate(users):
        print(f"{user:>7}", " ".join(f"{ratings[i,j]:>6}" for j in range(len(items))))
    
    # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
    print(f"\nç”¨æˆ·ç›¸ä¼¼åº¦è®¡ç®—:")
    
    def compute_user_similarity(ratings_matrix):
        """è®¡ç®—ç”¨æˆ·é—´çš„ç›¸ä¼¼åº¦"""
        n_users = ratings_matrix.shape[0]
        similarity_matrix = np.zeros((n_users, n_users))
        
        for i in range(n_users):
            for j in range(n_users):
                if i != j:
                    # æ‰¾åˆ°ä¸¤ä¸ªç”¨æˆ·éƒ½è¯„åˆ†è¿‡çš„ç‰©å“
                    user_i = ratings_matrix[i]
                    user_j = ratings_matrix[j]
                    
                    # è¿‡æ»¤æ‰0è¯„åˆ†ï¼ˆæœªè¯„åˆ†ï¼‰
                    mask = (user_i > 0) & (user_j > 0)
                    
                    if np.sum(mask) > 0:
                        # ä½¿ç”¨çš®å°”é€Šç›¸å…³ç³»æ•°
                        if np.sum(mask) > 1:
                            corr = np.corrcoef(user_i[mask], user_j[mask])[0,1]
                            if not np.isnan(corr):
                                similarity_matrix[i, j] = corr
                        else:
                            # å¦‚æœåªæœ‰ä¸€ä¸ªå…±åŒè¯„åˆ†ï¼Œä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                            similarity_matrix[i, j] = np.dot(user_i[mask], user_j[mask]) / (
                                np.linalg.norm(user_i[mask]) * np.linalg.norm(user_j[mask])
                            )
                else:
                    similarity_matrix[i, j] = 1.0
        
        return similarity_matrix
    
    user_similarity = compute_user_similarity(ratings)
    
    print("ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("       ", " ".join(f"{user:>8}" for user in users))
    for i, user in enumerate(users):
        print(f"{user:>7}", " ".join(f"{user_similarity[i,j]:>8.3f}" for j in range(len(users))))
    
    # ä¸ºç”¨æˆ·æ¨èç‰©å“
    def recommend_items(user_idx, ratings_matrix, similarity_matrix, n_recommendations=2):
        """ä¸ºç”¨æˆ·æ¨èç‰©å“"""
        target_user = ratings_matrix[user_idx]
        
        # æ‰¾åˆ°æœªè¯„åˆ†çš„ç‰©å“
        unrated_items = np.where(target_user == 0)[0]
        
        if len(unrated_items) == 0:
            return []
        
        # è®¡ç®—æ¯ä¸ªæœªè¯„åˆ†ç‰©å“çš„é¢„æµ‹è¯„åˆ†
        predictions = []
        
        for item_idx in unrated_items:
            # æ‰¾åˆ°è¯„åˆ†è¿‡è¯¥ç‰©å“çš„ç”¨æˆ·
            rated_users = np.where(ratings_matrix[:, item_idx] > 0)[0]
            
            if len(rated_users) == 0:
                continue
            
            # è®¡ç®—åŠ æƒå¹³å‡è¯„åˆ†
            numerator = 0
            denominator = 0
            
            for other_user in rated_users:
                if other_user != user_idx:
                    similarity = similarity_matrix[user_idx, other_user]
                    rating = ratings_matrix[other_user, item_idx]
                    
                    numerator += similarity * rating
                    denominator += abs(similarity)
            
            if denominator > 0:
                predicted_rating = numerator / denominator
                predictions.append((item_idx, predicted_rating))
        
        # æ’åºå¹¶è¿”å›top-Næ¨è
        predictions.sort(key=lambda x: x[1], reverse=True)
        return predictions[:n_recommendations]
    
    # ä¸ºAliceæ¨èç‰©å“
    alice_idx = 0
    recommendations = recommend_items(alice_idx, ratings, user_similarity)
    
    print(f"\nä¸º{users[alice_idx]}æ¨èç‰©å“:")
    for item_idx, predicted_rating in recommendations:
        print(f"  {items[item_idx]}: é¢„æµ‹è¯„åˆ† {predicted_rating:.2f}")
    
    # åˆ†ææ¨èç»“æœ
    print(f"\næ¨èåˆ†æ:")
    print(f"â€¢ åŸºäºç”¨æˆ·ç›¸ä¼¼åº¦çš„ååŒè¿‡æ»¤")
    print(f"â€¢ æ‰¾åˆ°ç›¸ä¼¼ç”¨æˆ·ï¼Œæ¨èä»–ä»¬å–œæ¬¢çš„ç‰©å“")
    print(f"â€¢ ä½¿ç”¨åŠ æƒå¹³å‡é¢„æµ‹è¯„åˆ†")
    
    return user_similarity, recommendations

user_similarity, recommendations = recommendation_system_similarity()
```

### æ¡ˆä¾‹2ï¼šæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
```python
def text_similarity_example():
    """æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹"""
    
    print("\næ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹")
    print("=" * 50)
    
    # ç¤ºä¾‹æ–‡æ¡£
    documents = [
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸ",
        "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
        "ä»Šå¤©å¤©æ°”å¾ˆå¥½é€‚åˆå‡ºå»æ•£æ­¥",
        "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€"
    ]
    
    print("æ–‡æ¡£é›†åˆ:")
    for i, doc in enumerate(documents):
        print(f"  æ–‡æ¡£{i+1}: {doc}")
    
    # ç®€å•çš„è¯è¢‹æ¨¡å‹
    def create_vocabulary(docs):
        """åˆ›å»ºè¯æ±‡è¡¨"""
        vocabulary = set()
        for doc in docs:
            words = doc.split()
            vocabulary.update(words)
        return sorted(list(vocabulary))
    
    def document_to_vector(doc, vocabulary):
        """å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡"""
        words = doc.split()
        vector = np.zeros(len(vocabulary))
        
        for word in words:
            if word in vocabulary:
                idx = vocabulary.index(word)
                vector[idx] += 1
        
        return vector
    
    # åˆ›å»ºè¯æ±‡è¡¨
    vocab = create_vocabulary(documents)
    print(f"\nè¯æ±‡è¡¨: {vocab}")
    
    # å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡
    doc_vectors = []
    for doc in documents:
        vector = document_to_vector(doc, vocab)
        doc_vectors.append(vector)
    
    doc_vectors = np.array(doc_vectors)
    
    print(f"\næ–‡æ¡£å‘é‡çŸ©é˜µå½¢çŠ¶: {doc_vectors.shape}")
    print("æ–‡æ¡£å‘é‡:")
    for i, vector in enumerate(doc_vectors):
        print(f"  æ–‡æ¡£{i+1}: {vector}")
    
    # è®¡ç®—æ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦
    def compute_document_similarity(vectors):
        """è®¡ç®—æ–‡æ¡£é—´çš„ç›¸ä¼¼åº¦"""
        n_docs = len(vectors)
        similarity_matrix = np.zeros((n_docs, n_docs))
        
        for i in range(n_docs):
            for j in range(n_docs):
                if i != j:
                    # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                    dot_product = np.dot(vectors[i], vectors[j])
                    norm_i = np.linalg.norm(vectors[i])
                    norm_j = np.linalg.norm(vectors[j])
                    
                    if norm_i > 0 and norm_j > 0:
                        similarity_matrix[i, j] = dot_product / (norm_i * norm_j)
                else:
                    similarity_matrix[i, j] = 1.0
        
        return similarity_matrix
    
    doc_similarity = compute_document_similarity(doc_vectors)
    
    print(f"\næ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("       ", " ".join(f"æ–‡æ¡£{i+1:>7}" for i in range(len(documents))))
    for i in range(len(documents)):
        print(f"æ–‡æ¡£{i+1:>5}", " ".join(f"{doc_similarity[i,j]:>7.3f}" for j in range(len(documents))))
    
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹
    max_similarity = 0
    most_similar_pair = None
    
    for i in range(len(documents)):
        for j in range(i+1, len(documents)):
            if doc_similarity[i, j] > max_similarity:
                max_similarity = doc_similarity[i, j]
                most_similar_pair = (i, j)
    
    if most_similar_pair:
        i, j = most_similar_pair
        print(f"\næœ€ç›¸ä¼¼çš„æ–‡æ¡£å¯¹:")
        print(f"  æ–‡æ¡£{i+1}: {documents[i]}")
        print(f"  æ–‡æ¡£{j+1}: {documents[j]}")
        print(f"  ç›¸ä¼¼åº¦: {max_similarity:.3f}")
    
    # ç»™å®šæŸ¥è¯¢ï¼Œæ‰¾åˆ°æœ€ç›¸ä¼¼çš„æ–‡æ¡£
    query = "æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œ"
    query_vector = document_to_vector(query, vocab)
    
    print(f"\næŸ¥è¯¢: {query}")
    print(f"æŸ¥è¯¢å‘é‡: {query_vector}")
    
    # è®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
    query_similarities = []
    for i, doc_vector in enumerate(doc_vectors):
        dot_product = np.dot(query_vector, doc_vector)
        norm_query = np.linalg.norm(query_vector)
        norm_doc = np.linalg.norm(doc_vector)
        
        if norm_query > 0 and norm_doc > 0:
            similarity = dot_product / (norm_query * norm_doc)
        else:
            similarity = 0
        
        query_similarities.append((i, similarity))
    
    # æ’åºå¹¶æ˜¾ç¤ºç»“æœ
    query_similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\næŸ¥è¯¢ç»“æœ (æŒ‰ç›¸ä¼¼åº¦æ’åº):")
    for i, (doc_idx, similarity) in enumerate(query_similarities):
        print(f"  {i+1}. æ–‡æ¡£{doc_idx+1}: {documents[doc_idx]} (ç›¸ä¼¼åº¦: {similarity:.3f})")
    
    return doc_similarity, query_similarities

doc_similarity, query_similarities = text_similarity_example()
```

### æ¡ˆä¾‹3ï¼šå›¾åƒç›¸ä¼¼åº¦è®¡ç®—
```python
def image_similarity_example():
    """å›¾åƒç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹"""
    
    print("\nå›¾åƒç›¸ä¼¼åº¦è®¡ç®—ç¤ºä¾‹")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿå›¾åƒæ•°æ®ï¼ˆç®€åŒ–ä¸ºå°å›¾åƒï¼‰
    np.random.seed(42)
    
    # åˆ›å»ºå‡ ä¸ªæ¨¡æ‹Ÿå›¾åƒ (8x8åƒç´ )
    def create_pattern_image(pattern_type, noise_level=0.1):
        """åˆ›å»ºå¸¦æœ‰ç‰¹å®šæ¨¡å¼çš„å›¾åƒ"""
        img = np.zeros((8, 8))
        
        if pattern_type == 'vertical':
            img[:, 2:6] = 1
        elif pattern_type == 'horizontal':
            img[2:6, :] = 1
        elif pattern_type == 'diagonal':
            for i in range(8):
                if i < 8:
                    img[i, i] = 1
        elif pattern_type == 'circle':
            center = (4, 4)
            for i in range(8):
                for j in range(8):
                    if (i - center[0])**2 + (j - center[1])**2 <= 9:
                        img[i, j] = 1
        
        # æ·»åŠ å™ªå£°
        img += noise_level * np.random.randn(8, 8)
        img = np.clip(img, 0, 1)
        
        return img
    
    # åˆ›å»ºå›¾åƒé›†åˆ
    images = {
        'vertical1': create_pattern_image('vertical', 0.05),
        'vertical2': create_pattern_image('vertical', 0.15),
        'horizontal1': create_pattern_image('horizontal', 0.05),
        'diagonal1': create_pattern_image('diagonal', 0.05),
        'circle1': create_pattern_image('circle', 0.05)
    }
    
    print("å›¾åƒé›†åˆ:")
    for name, img in images.items():
        print(f"  {name}: {img.shape} åƒç´ ")
    
    # å°†å›¾åƒè½¬æ¢ä¸ºå‘é‡
    image_vectors = {}
    for name, img in images.items():
        image_vectors[name] = img.flatten()
    
    image_names = list(image_vectors.keys())
    vectors = np.array(list(image_vectors.values()))
    
    print(f"\nå›¾åƒå‘é‡å½¢çŠ¶: {vectors.shape}")
    
    # è®¡ç®—å›¾åƒé—´çš„ç›¸ä¼¼åº¦
    def compute_image_similarity(vectors, names):
        """è®¡ç®—å›¾åƒé—´çš„ç›¸ä¼¼åº¦"""
        n_images = len(vectors)
        similarity_matrix = np.zeros((n_images, n_images))
        
        for i in range(n_images):
            for j in range(n_images):
                if i != j:
                    # ä½¿ç”¨å¤šç§ç›¸ä¼¼åº¦åº¦é‡
                    
                    # 1. ä½™å¼¦ç›¸ä¼¼åº¦
                    cosine_sim = np.dot(vectors[i], vectors[j]) / (
                        np.linalg.norm(vectors[i]) * np.linalg.norm(vectors[j])
                    )
                    
                    # 2. çš®å°”é€Šç›¸å…³ç³»æ•°
                    pearson_sim = np.corrcoef(vectors[i], vectors[j])[0, 1]
                    if np.isnan(pearson_sim):
                        pearson_sim = 0
                    
                    # 3. ç»“æ„ç›¸ä¼¼æ€§ï¼ˆç®€åŒ–ç‰ˆï¼‰
                    mse = np.mean((vectors[i] - vectors[j]) ** 2)
                    structural_sim = 1 / (1 + mse)
                    
                    # ç»¼åˆç›¸ä¼¼åº¦
                    similarity_matrix[i, j] = 0.4 * cosine_sim + 0.3 * pearson_sim + 0.3 * structural_sim
                else:
                    similarity_matrix[i, j] = 1.0
        
        return similarity_matrix
    
    img_similarity = compute_image_similarity(vectors, image_names)
    
    print(f"\nå›¾åƒç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("            ", " ".join(f"{name:>12}" for name in image_names))
    for i, name in enumerate(image_names):
        print(f"{name:>12}", " ".join(f"{img_similarity[i,j]:>12.3f}" for j in range(len(image_names))))
    
    # æ‰¾åˆ°æœ€ç›¸ä¼¼çš„å›¾åƒå¯¹
    max_similarity = 0
    most_similar_pair = None
    
    for i in range(len(image_names)):
        for j in range(i+1, len(image_names)):
            if img_similarity[i, j] > max_similarity:
                max_similarity = img_similarity[i, j]
                most_similar_pair = (i, j)
    
    if most_similar_pair:
        i, j = most_similar_pair
        print(f"\næœ€ç›¸ä¼¼çš„å›¾åƒå¯¹:")
        print(f"  {image_names[i]} vs {image_names[j]}")
        print(f"  ç›¸ä¼¼åº¦: {max_similarity:.3f}")
    
    # å›¾åƒæ£€ç´¢ç¤ºä¾‹
    query_image = create_pattern_image('vertical', 0.2)
    query_vector = query_image.flatten()
    
    print(f"\næŸ¥è¯¢å›¾åƒ: å‚ç›´æ¨¡å¼ (å¸¦å™ªå£°)")
    
    # è®¡ç®—æŸ¥è¯¢å›¾åƒä¸æ‰€æœ‰å›¾åƒçš„ç›¸ä¼¼åº¦
    query_similarities = []
    for i, img_vector in enumerate(vectors):
        cosine_sim = np.dot(query_vector, img_vector) / (
            np.linalg.norm(query_vector) * np.linalg.norm(img_vector)
        )
        query_similarities.append((i, cosine_sim))
    
    # æ’åºå¹¶æ˜¾ç¤ºç»“æœ
    query_similarities.sort(key=lambda x: x[1], reverse=True)
    
    print(f"\næ£€ç´¢ç»“æœ (æŒ‰ç›¸ä¼¼åº¦æ’åº):")
    for i, (img_idx, similarity) in enumerate(query_similarities):
        print(f"  {i+1}. {image_names[img_idx]}: {similarity:.3f}")
    
    # åˆ†æä¸åŒæ¨¡å¼çš„ç›¸ä¼¼æ€§
    print(f"\næ¨¡å¼åˆ†æ:")
    print(f"â€¢ ç›¸åŒæ¨¡å¼çš„å›¾åƒç›¸ä¼¼åº¦æœ€é«˜")
    print(f"â€¢ å™ªå£°å½±å“ç›¸ä¼¼åº¦è®¡ç®—")
    print(f"â€¢ ç»“æ„ç›¸ä¼¼æ€§æ¯”åƒç´ å€¼ç›¸ä¼¼æ€§æ›´é‡è¦")
    
    return img_similarity, query_similarities

img_similarity, query_similarities = image_similarity_example()
```

---

## ğŸ“Š ç›¸ä¼¼åº¦åº¦é‡çš„é€‰æ‹©æŒ‡å—

### ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä½³é€‰æ‹©
```python
def similarity_selection_guide():
    """ç›¸ä¼¼åº¦åº¦é‡é€‰æ‹©æŒ‡å—"""
    
    print("ç›¸ä¼¼åº¦åº¦é‡é€‰æ‹©æŒ‡å—")
    print("=" * 50)
    
    # ä¸åŒåº¦é‡çš„ç‰¹ç‚¹
    metrics = {
        "æ¬§æ°è·ç¦»": {
            "ç‰¹ç‚¹": ["è€ƒè™‘ç»å¯¹ä½ç½®", "å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ", "é€‚åˆè¿ç»­æ•°æ®"],
            "ä¼˜ç‚¹": ["ç›´è§‚æ˜“æ‡‚", "è®¡ç®—ç®€å•", "å‡ ä½•æ„ä¹‰æ˜ç¡®"],
            "ç¼ºç‚¹": ["ç»´åº¦ç¾éš¾", "å°ºåº¦æ•æ„Ÿ", "ä¸é€‚åˆç¨€ç–æ•°æ®"],
            "é€‚ç”¨åœºæ™¯": ["èšç±»åˆ†æ", "æœ€è¿‘é‚»åˆ†ç±»", "å¼‚å¸¸æ£€æµ‹", "å›¾åƒå¤„ç†"]
        },
        
        "ä½™å¼¦ç›¸ä¼¼åº¦": {
            "ç‰¹ç‚¹": ["å…³æ³¨æ–¹å‘", "å¿½ç•¥å¤§å°", "é€‚åˆé«˜ç»´æ•°æ®"],
            "ä¼˜ç‚¹": ["å°ºåº¦ä¸æ•æ„Ÿ", "é€‚åˆç¨€ç–æ•°æ®", "è®¡ç®—æ•ˆç‡é«˜"],
            "ç¼ºç‚¹": ["å¿½ç•¥æ•°å€¼å¤§å°", "å¯¹é›¶å‘é‡æ•æ„Ÿ"],
            "é€‚ç”¨åœºæ™¯": ["æ–‡æœ¬åˆ†æ", "æ¨èç³»ç»Ÿ", "ä¿¡æ¯æ£€ç´¢", "ç‰¹å¾åŒ¹é…"]
        },
        
        "çš®å°”é€Šç›¸å…³": {
            "ç‰¹ç‚¹": ["çº¿æ€§å…³ç³»", "æ ‡å‡†åŒ–å¤„ç†", "ç»Ÿè®¡æ„ä¹‰"],
            "ä¼˜ç‚¹": ["åæ˜ çº¿æ€§å…³ç³»", "æ¶ˆé™¤å‡å€¼å½±å“", "ç»Ÿè®¡æ˜¾è‘—æ€§"],
            "ç¼ºç‚¹": ["åªæ•æ‰çº¿æ€§å…³ç³»", "å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ"],
            "é€‚ç”¨åœºæ™¯": ["é‡‘èåˆ†æ", "æ¨èç³»ç»Ÿ", "ç‰¹å¾é€‰æ‹©", "è´¨é‡æ§åˆ¶"]
        },
        
        "æ›¼å“ˆé¡¿è·ç¦»": {
            "ç‰¹ç‚¹": ["L1èŒƒæ•°", "å¯¹å¼‚å¸¸å€¼è¾ƒé²æ£’", "ç¨€ç–å‹å¥½"],
            "ä¼˜ç‚¹": ["å¼‚å¸¸å€¼é²æ£’æ€§", "è®¡ç®—ç®€å•", "é€‚åˆç¨€ç–æ•°æ®"],
            "ç¼ºç‚¹": ["ä¸å¦‚æ¬§æ°è·ç¦»ç›´è§‚", "å¯èƒ½è¿‡åº¦æƒ©ç½š"],
            "é€‚ç”¨åœºæ™¯": ["ç¨€ç–æ•°æ®", "å¼‚å¸¸æ£€æµ‹", "è·¯å¾„è§„åˆ’", "å›¾åƒå¤„ç†"]
        }
    }
    
    for metric, info in metrics.items():
        print(f"\n{metric}:")
        print(f"  ç‰¹ç‚¹: {', '.join(info['ç‰¹ç‚¹'])}")
        print(f"  ä¼˜ç‚¹: {', '.join(info['ä¼˜ç‚¹'])}")
        print(f"  ç¼ºç‚¹: {', '.join(info['ç¼ºç‚¹'])}")
        print(f"  é€‚ç”¨åœºæ™¯: {', '.join(info['é€‚ç”¨åœºæ™¯'])}")
    
    # é€‰æ‹©å†³ç­–æ ‘
    print(f"\né€‰æ‹©å†³ç­–æ ‘:")
    print(f"â”œâ”€â”€ æ•°æ®ç±»å‹")
    print(f"â”‚   â”œâ”€â”€ æ–‡æœ¬æ•°æ® â†’ ä½™å¼¦ç›¸ä¼¼åº¦")
    print(f"â”‚   â”œâ”€â”€ å›¾åƒæ•°æ® â†’ æ¬§æ°è·ç¦» æˆ– ç»“æ„ç›¸ä¼¼åº¦")
    print(f"â”‚   â”œâ”€â”€ è¯„åˆ†æ•°æ® â†’ çš®å°”é€Šç›¸å…³")
    print(f"â”‚   â””â”€â”€ äºŒè¿›åˆ¶æ•°æ® â†’ Jaccardç³»æ•°")
    print(f"â”‚")
    print(f"â”œâ”€â”€ æ•°æ®ç‰¹å¾")
    print(f"â”‚   â”œâ”€â”€ é«˜ç»´ç¨€ç– â†’ ä½™å¼¦ç›¸ä¼¼åº¦")
    print(f"â”‚   â”œâ”€â”€ ä½ç»´å¯†é›† â†’ æ¬§æ°è·ç¦»")
    print(f"â”‚   â”œâ”€â”€ æœ‰å¼‚å¸¸å€¼ â†’ æ›¼å“ˆé¡¿è·ç¦»")
    print(f"â”‚   â””â”€â”€ éœ€è¦æ ‡å‡†åŒ– â†’ çš®å°”é€Šç›¸å…³")
    print(f"â”‚")
    print(f"â””â”€â”€ åº”ç”¨åœºæ™¯")
    print(f"    â”œâ”€â”€ èšç±»åˆ†æ â†’ æ¬§æ°è·ç¦»")
    print(f"    â”œâ”€â”€ æ¨èç³»ç»Ÿ â†’ ä½™å¼¦ç›¸ä¼¼åº¦ æˆ– çš®å°”é€Šç›¸å…³")
    print(f"    â”œâ”€â”€ ä¿¡æ¯æ£€ç´¢ â†’ ä½™å¼¦ç›¸ä¼¼åº¦")
    print(f"    â””â”€â”€ å¼‚å¸¸æ£€æµ‹ â†’ æ›¼å“ˆé¡¿è·ç¦» æˆ– é©¬æ°è·ç¦»")
    
    # æ€§èƒ½æ¯”è¾ƒ
    print(f"\næ€§èƒ½æ¯”è¾ƒ:")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 100
    
    X = np.random.randn(n_samples, n_features)
    
    # æµ‹è¯•ä¸åŒåº¦é‡çš„è®¡ç®—æ—¶é—´
    import time
    
    # é€‰æ‹©ä¸¤ä¸ªå‘é‡è¿›è¡Œæ¯”è¾ƒ
    v1 = X[0]
    v2 = X[1]
    
    # æµ‹è¯•å‡½æ•°
    def time_function(func, *args, n_runs=1000):
        start = time.time()
        for _ in range(n_runs):
            func(*args)
        end = time.time()
        return (end - start) / n_runs
    
    # å®šä¹‰åº¦é‡å‡½æ•°
    def euclidean_sim(a, b):
        return 1 / (1 + np.linalg.norm(a - b))
    
    def cosine_sim(a, b):
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def manhattan_sim(a, b):
        return 1 / (1 + np.sum(np.abs(a - b)))
    
    def pearson_sim(a, b):
        return np.corrcoef(a, b)[0, 1]
    
    # æµ‹è¯•æ—¶é—´
    functions = [
        ("æ¬§æ°è·ç¦»", euclidean_sim),
        ("ä½™å¼¦ç›¸ä¼¼åº¦", cosine_sim),
        ("æ›¼å“ˆé¡¿è·ç¦»", manhattan_sim),
        ("çš®å°”é€Šç›¸å…³", pearson_sim)
    ]
    
    print(f"  è®¡ç®—æ—¶é—´æ¯”è¾ƒ (å‘é‡ç»´åº¦: {n_features}):")
    for name, func in functions:
        avg_time = time_function(func, v1, v2)
        print(f"    {name}: {avg_time*1000:.4f} ms")

similarity_selection_guide()
```

---

## ğŸš€ ä¼˜åŒ–æŠ€å·§

### é«˜æ•ˆè®¡ç®—å¤§è§„æ¨¡ç›¸ä¼¼åº¦
```python
def efficient_similarity_computation():
    """é«˜æ•ˆè®¡ç®—å¤§è§„æ¨¡ç›¸ä¼¼åº¦"""
    
    print("é«˜æ•ˆè®¡ç®—å¤§è§„æ¨¡ç›¸ä¼¼åº¦")
    print("=" * 50)
    
    # ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®
    np.random.seed(42)
    n_samples = 5000
    n_features = 100
    
    X = np.random.randn(n_samples, n_features)
    
    print(f"æ•°æ®è§„æ¨¡: {X.shape}")
    print(f"å¦‚æœè®¡ç®—æ‰€æœ‰å¯¹çš„ç›¸ä¼¼åº¦ï¼Œéœ€è¦ {n_samples * (n_samples - 1) // 2:,} æ¬¡è®¡ç®—")
    
    import time
    
    # æ–¹æ³•1: æœ´ç´ æ–¹æ³• - åŒé‡å¾ªç¯
    print(f"\næ–¹æ³•1: æœ´ç´ æ–¹æ³•")
    start = time.time()
    
    # åªè®¡ç®—å‰100ä¸ªæ ·æœ¬çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    subset_size = 100
    similarity_naive = np.zeros((subset_size, subset_size))
    
    for i in range(subset_size):
        for j in range(subset_size):
            if i != j:
                similarity_naive[i, j] = np.dot(X[i], X[j]) / (
                    np.linalg.norm(X[i]) * np.linalg.norm(X[j])
                )
            else:
                similarity_naive[i, j] = 1.0
    
    time_naive = time.time() - start
    print(f"  æ—¶é—´: {time_naive:.4f}ç§’ (ä»…{subset_size}ä¸ªæ ·æœ¬)")
    
    # æ–¹æ³•2: å‘é‡åŒ–è®¡ç®—
    print(f"\næ–¹æ³•2: å‘é‡åŒ–è®¡ç®—")
    start = time.time()
    
    # æ ‡å‡†åŒ–æ•°æ®
    X_normalized = X / np.linalg.norm(X, axis=1, keepdims=True)
    
    # è®¡ç®—æ‰€æœ‰å¯¹çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarity_vectorized = X_normalized @ X_normalized.T
    
    time_vectorized = time.time() - start
    print(f"  æ—¶é—´: {time_vectorized:.4f}ç§’ (å…¨éƒ¨{n_samples}ä¸ªæ ·æœ¬)")
    print(f"  é€Ÿåº¦æå‡: {time_naive / time_vectorized:.1f}å€")
    
    # æ–¹æ³•3: ç¨€ç–ç›¸ä¼¼åº¦è®¡ç®— (åªè®¡ç®—ç›¸ä¼¼åº¦é«˜çš„å¯¹)
    print(f"\næ–¹æ³•3: ç¨€ç–ç›¸ä¼¼åº¦è®¡ç®—")
    start = time.time()
    
    # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
    threshold = 0.8
    
    # ä½¿ç”¨è¿‘ä¼¼æ–¹æ³•ï¼šéšæœºæŠ•å½±
    from sklearn.random_projection import SparseRandomProjection
    
    # é™ç»´åˆ°è¾ƒä½ç»´åº¦
    reducer = SparseRandomProjection(n_components=50, random_state=42)
    X_reduced = reducer.fit_transform(X)
    
    # åœ¨ä½ç»´ç©ºé—´è®¡ç®—ç›¸ä¼¼åº¦
    X_reduced_norm = X_reduced / np.linalg.norm(X_reduced, axis=1, keepdims=True)
    similarity_reduced = X_reduced_norm @ X_reduced_norm.T
    
    # åªä¿ç•™é«˜ç›¸ä¼¼åº¦çš„å¯¹
    high_similarity_pairs = np.where(similarity_reduced > threshold)
    
    time_sparse = time.time() - start
    print(f"  æ—¶é—´: {time_sparse:.4f}ç§’")
    print(f"  é«˜ç›¸ä¼¼åº¦å¯¹æ•°: {len(high_similarity_pairs[0]):,}")
    print(f"  å‹ç¼©æ¯”: {len(high_similarity_pairs[0]) / (n_samples * n_samples):.4f}")
    
    # æ–¹æ³•4: åˆ†å—è®¡ç®—
    print(f"\næ–¹æ³•4: åˆ†å—è®¡ç®—")
    start = time.time()
    
    block_size = 1000
    n_blocks = (n_samples + block_size - 1) // block_size
    
    # åªè®¡ç®—å¯¹è§’çº¿å—
    block_similarities = []
    for i in range(n_blocks):
        start_idx = i * block_size
        end_idx = min(start_idx + block_size, n_samples)
        
        block_data = X[start_idx:end_idx]
        block_norm = block_data / np.linalg.norm(block_data, axis=1, keepdims=True)
        block_sim = block_norm @ block_norm.T
        
        block_similarities.append(block_sim)
    
    time_block = time.time() - start
    print(f"  æ—¶é—´: {time_block:.4f}ç§’")
    print(f"  å¤„ç†äº† {n_blocks} ä¸ªå—")
    
    # æ–¹æ³•5: è¿‘ä¼¼æœ€è¿‘é‚»
    print(f"\næ–¹æ³•5: è¿‘ä¼¼æœ€è¿‘é‚»")
    
    try:
        from sklearn.neighbors import NearestNeighbors
        
        start = time.time()
        
        # ä½¿ç”¨è¿‘ä¼¼æœ€è¿‘é‚»ç®—æ³•
        nbrs = NearestNeighbors(n_neighbors=10, algorithm='auto', metric='cosine')
        nbrs.fit(X)
        
        # ä¸ºæ¯ä¸ªæ ·æœ¬æ‰¾åˆ°æœ€ç›¸ä¼¼çš„kä¸ªé‚»å±…
        distances, indices = nbrs.kneighbors(X)
        
        time_ann = time.time() - start
        print(f"  æ—¶é—´: {time_ann:.4f}ç§’")
        print(f"  ä¸ºæ¯ä¸ªæ ·æœ¬æ‰¾åˆ° {10} ä¸ªæœ€ç›¸ä¼¼çš„é‚»å±…")
        print(f"  æ€»ç›¸ä¼¼åº¦å¯¹æ•°: {n_samples * 10:,}")
        
    except ImportError:
        print("  éœ€è¦å®‰è£… scikit-learn")
    
    # å†…å­˜ä½¿ç”¨åˆ†æ
    print(f"\nå†…å­˜ä½¿ç”¨åˆ†æ:")
    full_matrix_size = n_samples * n_samples * 8  # 8å­—èŠ‚per float64
    print(f"  å®Œæ•´ç›¸ä¼¼åº¦çŸ©é˜µ: {full_matrix_size / (1024**3):.2f} GB")
    
    sparse_matrix_size = len(high_similarity_pairs[0]) * 8 * 3  # è¡Œã€åˆ—ã€å€¼
    print(f"  ç¨€ç–ç›¸ä¼¼åº¦çŸ©é˜µ: {sparse_matrix_size / (1024**2):.2f} MB")
    
    # æ€»ç»“
    print(f"\nä¼˜åŒ–æ€»ç»“:")
    print(f"â€¢ å‘é‡åŒ–è®¡ç®—: æœ€å¤§çš„æ€§èƒ½æå‡")
    print(f"â€¢ ç¨€ç–è®¡ç®—: èŠ‚çœå†…å­˜å’Œè®¡ç®—")
    print(f"â€¢ åˆ†å—å¤„ç†: é€‚åˆè¶…å¤§æ•°æ®é›†")
    print(f"â€¢ è¿‘ä¼¼æ–¹æ³•: å¹³è¡¡ç²¾åº¦å’Œæ•ˆç‡")
    print(f"â€¢ é™ç»´é¢„å¤„ç†: å‡å°‘è®¡ç®—å¤æ‚åº¦")

efficient_similarity_computation()
```

---

## ğŸ“š æ€»ç»“ä¸å»ºè®®

### å‘é‡ç›¸ä¼¼åº¦çš„å®Œæ•´æ€»ç»“
```python
def vector_similarity_summary():
    """å‘é‡ç›¸ä¼¼åº¦çš„å®Œæ•´æ€»ç»“"""
    
    print("å‘é‡ç›¸ä¼¼åº¦å®Œæ•´æ€»ç»“")
    print("=" * 50)
    
    # åº¦é‡æ–¹æ³•æ€»ç»“
    print("ä¸»è¦åº¦é‡æ–¹æ³•:")
    print("1. æ¬§æ°è·ç¦» - ä½ç½®ç›¸ä¼¼åº¦")
    print("2. ä½™å¼¦ç›¸ä¼¼åº¦ - æ–¹å‘ç›¸ä¼¼åº¦")
    print("3. çš®å°”é€Šç›¸å…³ - çº¿æ€§å…³ç³»")
    print("4. æ›¼å“ˆé¡¿è·ç¦» - é²æ£’æ€§å¥½")
    print("5. ç‚¹ç§¯ - ç®€å•é«˜æ•ˆ")
    
    # é€‰æ‹©å»ºè®®
    print(f"\né€‰æ‹©å»ºè®®:")
    scenarios = [
        ("æ–‡æœ¬åˆ†æ", "ä½™å¼¦ç›¸ä¼¼åº¦", "å…³æ³¨è¯è¯­å…±ç°æ¨¡å¼"),
        ("å›¾åƒåŒ¹é…", "æ¬§æ°è·ç¦»", "åƒç´ çº§å·®å¼‚é‡è¦"),
        ("æ¨èç³»ç»Ÿ", "çš®å°”é€Šç›¸å…³", "ç”¨æˆ·è¯„åˆ†è¡Œä¸º"),
        ("å¼‚å¸¸æ£€æµ‹", "æ›¼å“ˆé¡¿è·ç¦»", "å¯¹å¼‚å¸¸å€¼é²æ£’"),
        ("ç¥ç»ç½‘ç»œ", "ç‚¹ç§¯", "è®¡ç®—æ•ˆç‡é«˜")
    ]
    
    for scenario, method, reason in scenarios:
        print(f"  {scenario}: {method} ({reason})")
    
    # å®ç°è¦ç‚¹
    print(f"\nå®ç°è¦ç‚¹:")
    print("â€¢ æ•°æ®é¢„å¤„ç†: æ ‡å‡†åŒ–ã€å½’ä¸€åŒ–")
    print("â€¢ ç»´åº¦å¤„ç†: é™ç»´ã€ç‰¹å¾é€‰æ‹©")
    print("â€¢ è®¡ç®—ä¼˜åŒ–: å‘é‡åŒ–ã€å¹¶è¡ŒåŒ–")
    print("â€¢ å†…å­˜ç®¡ç†: åˆ†å—ã€ç¨€ç–å­˜å‚¨")
    print("â€¢ ç»“æœéªŒè¯: äº¤å‰éªŒè¯ã€å¯è§†åŒ–")
    
    # å¸¸è§é™·é˜±
    print(f"\nå¸¸è§é™·é˜±:")
    print("â€¢ å¿˜è®°æ ‡å‡†åŒ–æ•°æ®")
    print("â€¢ é«˜ç»´ç©ºé—´çš„è·ç¦»æ„ä¹‰")
    print("â€¢ ç¨€ç–æ•°æ®çš„é›¶å€¼å¤„ç†")
    print("â€¢ è®¡ç®—æº¢å‡ºå’Œæ•°å€¼ç¨³å®šæ€§")
    print("â€¢ ç›¸ä¼¼åº¦ä¸è·ç¦»çš„è½¬æ¢")
    
    # æ‰©å±•æ–¹å‘
    print(f"\næ‰©å±•æ–¹å‘:")
    print("â€¢ å­¦ä¹ ç›¸ä¼¼åº¦åº¦é‡")
    print("â€¢ å¤šæ¨¡æ€ç›¸ä¼¼åº¦")
    print("â€¢ åŠ¨æ€ç›¸ä¼¼åº¦")
    print("â€¢ è¿‘ä¼¼ç®—æ³•")
    print("â€¢ åˆ†å¸ƒå¼è®¡ç®—")

vector_similarity_summary()
```

---

## ğŸ¯ å­¦ä¹ å»ºè®®

### æŒæ¡å‘é‡ç›¸ä¼¼åº¦çš„å…³é”®æ­¥éª¤
1. **ç†è§£å‡ ä½•æ„ä¹‰**ï¼šä»äºŒç»´ç©ºé—´å¼€å§‹ç†è§£å‘é‡å…³ç³»
2. **ç†Ÿæ‚‰æ•°å­¦å…¬å¼**ï¼šæŒæ¡å„ç§åº¦é‡çš„è®¡ç®—æ–¹æ³•
3. **å®è·µç¼–ç¨‹å®ç°**ï¼šæ‰‹å·¥å®ç°åŠ æ·±ç†è§£
4. **åº”ç”¨åœºæ™¯åˆ†æ**ï¼šçŸ¥é“åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨å“ªç§åº¦é‡
5. **æ€§èƒ½ä¼˜åŒ–æŠ€å·§**ï¼šå­¦ä¼šå¤„ç†å¤§è§„æ¨¡æ•°æ®

### æ·±å…¥å­¦ä¹ å»ºè®®
- **çº¿æ€§ä»£æ•°åŸºç¡€**ï¼šå‘é‡ç©ºé—´ã€å†…ç§¯ã€èŒƒæ•°
- **ç»Ÿè®¡å­¦çŸ¥è¯†**ï¼šç›¸å…³æ€§ã€åˆ†å¸ƒã€å‡è®¾æ£€éªŒ
- **æœºå™¨å­¦ä¹ åº”ç”¨**ï¼šèšç±»ã€åˆ†ç±»ã€æ¨èç³»ç»Ÿ
- **ç®—æ³•ä¼˜åŒ–**ï¼šå¹¶è¡Œè®¡ç®—ã€è¿‘ä¼¼ç®—æ³•ã€åˆ†å¸ƒå¼å¤„ç†

---

**ğŸ“ è®°ä½ï¼šé€‰æ‹©åˆé€‚çš„ç›¸ä¼¼åº¦åº¦é‡æ˜¯æˆåŠŸåº”ç”¨çš„å…³é”®ï¼Œç†è§£å…¶èƒŒåçš„æ•°å­¦åŸç†å’Œå‡ ä½•æ„ä¹‰æ›´ä¸ºé‡è¦ï¼** 