# ğŸ¯ æ¢¯åº¦ä¸‹é™ç®—æ³•è¯¦è§£

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

> **æ¢¯åº¦ä¸‹é™æ˜¯ä¼˜åŒ–ç®—æ³•çš„åŸºçŸ³ï¼Œç†è§£å®ƒå°±ç†è§£äº†æœºå™¨å­¦ä¹ ä¸­å‚æ•°å­¦ä¹ çš„æœ¬è´¨**

### ä»€ä¹ˆæ˜¯æ¢¯åº¦ä¸‹é™ï¼Ÿ
**å®šä¹‰**ï¼šæ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§å¯»æ‰¾å‡½æ•°æœ€å°å€¼çš„è¿­ä»£ä¼˜åŒ–ç®—æ³•ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šæ²¿ç€å‡½æ•°æ¢¯åº¦çš„è´Ÿæ–¹å‘ç§»åŠ¨ï¼Œé€æ­¥é€¼è¿‘æœ€å°å€¼ç‚¹ã€‚

**ç›´è§‰ç†è§£**ï¼šå°±åƒåœ¨å±±å¡ä¸Šæ‰¾æœ€ä½ç‚¹ï¼Œæ¯æ¬¡éƒ½æœç€æœ€é™¡å³­çš„ä¸‹é™æ–¹å‘èµ°ã€‚

---

## ğŸ§  æ•°å­¦åŸç†

### æ¢¯åº¦çš„å®šä¹‰
```python
import numpy as np
import matplotlib.pyplot as plt

def gradient_intuition():
    """ç†è§£æ¢¯åº¦çš„å‡ ä½•æ„ä¹‰"""
    
    # å®šä¹‰ä¸€ä¸ªç®€å•çš„äºŒæ¬¡å‡½æ•°
    def f(x, y):
        return x**2 + 2*y**2 + x*y + 2*x + 3*y + 1
    
    # è®¡ç®—æ¢¯åº¦
    def gradient_f(x, y):
        df_dx = 2*x + y + 2
        df_dy = 4*y + x + 3
        return np.array([df_dx, df_dy])
    
    # æµ‹è¯•ç‚¹
    x, y = 1, 1
    
    print(f"å‡½æ•°å€¼ f({x}, {y}) = {f(x, y):.3f}")
    
    gradient = gradient_f(x, y)
    print(f"æ¢¯åº¦ âˆ‡f({x}, {y}) = {gradient}")
    
    # æ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘
    # è´Ÿæ¢¯åº¦æ–¹å‘æ˜¯å‡½æ•°å‡å°‘æœ€å¿«çš„æ–¹å‘
    print(f"è´Ÿæ¢¯åº¦æ–¹å‘: {-gradient}")
    
    # éªŒè¯ï¼šæ²¿æ¢¯åº¦æ–¹å‘å‡½æ•°å€¼å¢åŠ 
    step_size = 0.1
    x_new = x + step_size * gradient[0]
    y_new = y + step_size * gradient[1]
    
    print(f"\næ²¿æ¢¯åº¦æ–¹å‘ç§»åŠ¨å:")
    print(f"æ–°ä½ç½®: ({x_new:.3f}, {y_new:.3f})")
    print(f"æ–°å‡½æ•°å€¼: {f(x_new, y_new):.3f}")
    print(f"å‡½æ•°å€¼å˜åŒ–: {f(x_new, y_new) - f(x, y):.3f} (åº”è¯¥>0)")
    
    # æ²¿è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨
    x_new_neg = x - step_size * gradient[0]
    y_new_neg = y - step_size * gradient[1]
    
    print(f"\næ²¿è´Ÿæ¢¯åº¦æ–¹å‘ç§»åŠ¨å:")
    print(f"æ–°ä½ç½®: ({x_new_neg:.3f}, {y_new_neg:.3f})")
    print(f"æ–°å‡½æ•°å€¼: {f(x_new_neg, y_new_neg):.3f}")
    print(f"å‡½æ•°å€¼å˜åŒ–: {f(x_new_neg, y_new_neg) - f(x, y):.3f} (åº”è¯¥<0)")

gradient_intuition()
```

### æ¢¯åº¦ä¸‹é™çš„æ•°å­¦è¡¨è¿°
```python
def gradient_descent_math():
    """æ¢¯åº¦ä¸‹é™çš„æ•°å­¦è¡¨è¿°"""
    
    print("æ¢¯åº¦ä¸‹é™ç®—æ³•çš„æ•°å­¦è¡¨è¿°:")
    print("=" * 40)
    
    print("1. ç›®æ ‡å‡½æ•°: f(Î¸)")
    print("2. æ¢¯åº¦: âˆ‡f(Î¸) = [âˆ‚f/âˆ‚Î¸â‚, âˆ‚f/âˆ‚Î¸â‚‚, ..., âˆ‚f/âˆ‚Î¸â‚™]")
    print("3. æ›´æ–°è§„åˆ™: Î¸áµ¢â‚Šâ‚ = Î¸áµ¢ - Î±âˆ‡f(Î¸áµ¢)")
    print("   å…¶ä¸­ Î± æ˜¯å­¦ä¹ ç‡")
    
    print("\nå…³é”®è¦ç´ :")
    print("â€¢ å­¦ä¹ ç‡ Î±: æ§åˆ¶æ­¥é•¿å¤§å°")
    print("â€¢ æ¢¯åº¦ âˆ‡f: æŒ‡ç¤ºæ–¹å‘å’Œé™¡å³­ç¨‹åº¦")
    print("â€¢ è¿­ä»£è¿‡ç¨‹: é‡å¤æ›´æ–°ç›´åˆ°æ”¶æ•›")
    
    print("\næ”¶æ•›æ¡ä»¶:")
    print("â€¢ ||âˆ‡f(Î¸)|| < Îµ (æ¢¯åº¦è¶³å¤Ÿå°)")
    print("â€¢ |f(Î¸áµ¢â‚Šâ‚) - f(Î¸áµ¢)| < Îµ (å‡½æ•°å€¼å˜åŒ–è¶³å¤Ÿå°)")
    print("â€¢ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°")

gradient_descent_math()
```

---

## ğŸ”¢ åŸºæœ¬æ¢¯åº¦ä¸‹é™å®ç°

### ä¸€ç»´æƒ…å†µ
```python
def gradient_descent_1d():
    """ä¸€ç»´æ¢¯åº¦ä¸‹é™ç¤ºä¾‹"""
    
    # å®šä¹‰ç›®æ ‡å‡½æ•°: f(x) = xÂ² + 4x + 3
    def f(x):
        return x**2 + 4*x + 3
    
    # å¯¼æ•°: f'(x) = 2x + 4
    def df_dx(x):
        return 2*x + 4
    
    # çœŸå®æœ€å°å€¼ç‚¹: x = -2
    true_minimum = -2
    
    print("ä¸€ç»´æ¢¯åº¦ä¸‹é™ç¤ºä¾‹")
    print("ç›®æ ‡å‡½æ•°: f(x) = xÂ² + 4x + 3")
    print(f"çœŸå®æœ€å°å€¼ç‚¹: x = {true_minimum}")
    
    # æ¢¯åº¦ä¸‹é™
    x = 3.0  # åˆå§‹ç‚¹
    learning_rate = 0.1
    max_iterations = 20
    
    print(f"\nåˆå§‹ç‚¹: x = {x}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    
    trajectory = [x]
    
    for i in range(max_iterations):
        gradient = df_dx(x)
        x_new = x - learning_rate * gradient
        
        print(f"è¿­ä»£ {i+1}: x = {x:.4f}, f(x) = {f(x):.4f}, "
              f"æ¢¯åº¦ = {gradient:.4f}, æ–°x = {x_new:.4f}")
        
        trajectory.append(x_new)
        
        # æ£€æŸ¥æ”¶æ•›
        if abs(x_new - x) < 1e-6:
            print(f"åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£åæ”¶æ•›")
            break
        
        x = x_new
    
    print(f"\næœ€ç»ˆç»“æœ: x = {x:.6f}")
    print(f"æœ€ç»ˆå‡½æ•°å€¼: f(x) = {f(x):.6f}")
    print(f"ä¸çœŸå®æœ€å°å€¼çš„è¯¯å·®: {abs(x - true_minimum):.6f}")
    
    return trajectory

trajectory = gradient_descent_1d()
```

### å¤šç»´æƒ…å†µ
```python
def gradient_descent_multidimensional():
    """å¤šç»´æ¢¯åº¦ä¸‹é™ç¤ºä¾‹"""
    
    # å®šä¹‰äºŒæ¬¡å‡½æ•°: f(x,y) = xÂ² + 2yÂ² + xy + 2x + 3y + 1
    def f(params):
        x, y = params
        return x**2 + 2*y**2 + x*y + 2*x + 3*y + 1
    
    # æ¢¯åº¦
    def gradient_f(params):
        x, y = params
        df_dx = 2*x + y + 2
        df_dy = 4*y + x + 3
        return np.array([df_dx, df_dy])
    
    # è§£æè§£ï¼ˆé€šè¿‡ä»¤æ¢¯åº¦ä¸ºé›¶æ±‚å¾—ï¼‰
    # 2x + y + 2 = 0
    # x + 4y + 3 = 0
    # è§£å¾—: x = -1, y = -0.5
    true_minimum = np.array([-1, -0.5])
    
    print("å¤šç»´æ¢¯åº¦ä¸‹é™ç¤ºä¾‹")
    print("ç›®æ ‡å‡½æ•°: f(x,y) = xÂ² + 2yÂ² + xy + 2x + 3y + 1")
    print(f"çœŸå®æœ€å°å€¼ç‚¹: {true_minimum}")
    
    # æ¢¯åº¦ä¸‹é™
    params = np.array([3.0, 2.0])  # åˆå§‹ç‚¹
    learning_rate = 0.1
    max_iterations = 50
    
    print(f"\nåˆå§‹ç‚¹: {params}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    
    trajectory = [params.copy()]
    
    for i in range(max_iterations):
        gradient = gradient_f(params)
        params_new = params - learning_rate * gradient
        
        if i < 10 or i % 10 == 0:  # åªæ‰“å°å‰10æ¬¡å’Œæ¯10æ¬¡
            print(f"è¿­ä»£ {i+1}: params = {params}, f = {f(params):.4f}, "
                  f"||gradient|| = {np.linalg.norm(gradient):.4f}")
        
        trajectory.append(params_new.copy())
        
        # æ£€æŸ¥æ”¶æ•›
        if np.linalg.norm(params_new - params) < 1e-6:
            print(f"åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£åæ”¶æ•›")
            break
        
        params = params_new
    
    print(f"\næœ€ç»ˆç»“æœ: {params}")
    print(f"æœ€ç»ˆå‡½æ•°å€¼: f = {f(params):.6f}")
    print(f"ä¸çœŸå®æœ€å°å€¼çš„è·ç¦»: {np.linalg.norm(params - true_minimum):.6f}")
    
    return trajectory

trajectory_2d = gradient_descent_multidimensional()
```

---

## ğŸ¯ çº¿æ€§å›å½’ä¸­çš„æ¢¯åº¦ä¸‹é™

### æ‰¹é‡æ¢¯åº¦ä¸‹é™
```python
def linear_regression_gradient_descent():
    """çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™å®ç°"""
    
    # ç”Ÿæˆæ ·æœ¬æ•°æ®
    np.random.seed(42)
    m = 100  # æ ·æœ¬æ•°
    X = 2 * np.random.rand(m, 1)  # ç‰¹å¾
    y = 4 + 3 * X + np.random.randn(m, 1)  # ç›®æ ‡å€¼ï¼ŒçœŸå®å‚æ•°ä¸º w=3, b=4
    
    # æ·»åŠ åç½®é¡¹
    X_b = np.c_[np.ones((m, 1)), X]  # æ·»åŠ  x0 = 1
    
    print("çº¿æ€§å›å½’æ¢¯åº¦ä¸‹é™")
    print(f"æ ·æœ¬æ•°: {m}")
    print(f"çœŸå®å‚æ•°: w = 3, b = 4")
    
    # ä»£ä»·å‡½æ•°: J(Î¸) = (1/2m) * Î£(h(x) - y)Â²
    def cost_function(X, y, theta):
        m = len(y)
        predictions = X @ theta
        cost = (1 / (2 * m)) * np.sum((predictions - y) ** 2)
        return cost
    
    # æ¢¯åº¦: âˆ‡J(Î¸) = (1/m) * X^T * (X*Î¸ - y)
    def compute_gradient(X, y, theta):
        m = len(y)
        predictions = X @ theta
        gradient = (1 / m) * X.T @ (predictions - y)
        return gradient
    
    # åˆå§‹åŒ–å‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.01
    max_iterations = 1000
    
    print(f"\nåˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    
    cost_history = []
    
    for i in range(max_iterations):
        cost = cost_function(X_b, y, theta)
        gradient = compute_gradient(X_b, y, theta)
        theta = theta - learning_rate * gradient
        
        cost_history.append(cost)
        
        if i % 100 == 0:
            print(f"è¿­ä»£ {i}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    print(f"æœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    
    # ä¸æ­£è§„æ–¹ç¨‹çš„è§£æ¯”è¾ƒ
    theta_normal = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y
    print(f"æ­£è§„æ–¹ç¨‹è§£: Î¸ = {theta_normal.flatten()}")
    
    return theta, cost_history

theta_gd, cost_history = linear_regression_gradient_descent()
```

### éšæœºæ¢¯åº¦ä¸‹é™
```python
def stochastic_gradient_descent():
    """éšæœºæ¢¯åº¦ä¸‹é™å®ç°"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("éšæœºæ¢¯åº¦ä¸‹é™ (SGD)")
    
    # SGDå‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.01
    n_epochs = 50
    
    print(f"åˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    print(f"epochs: {n_epochs}")
    
    theta_history = []
    
    for epoch in range(n_epochs):
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(m)
        X_shuffled = X_b[indices]
        y_shuffled = y[indices]
        
        for i in range(m):
            xi = X_shuffled[i:i+1]
            yi = y_shuffled[i:i+1]
            
            # è®¡ç®—å•ä¸ªæ ·æœ¬çš„æ¢¯åº¦
            prediction = xi @ theta
            gradient = xi.T @ (prediction - yi)
            
            # æ›´æ–°å‚æ•°
            theta = theta - learning_rate * gradient
        
        theta_history.append(theta.copy())
        
        if epoch % 10 == 0:
            cost = np.mean((X_b @ theta - y) ** 2) / 2
            print(f"Epoch {epoch}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    
    return theta_history

theta_sgd_history = stochastic_gradient_descent()
```

### å°æ‰¹é‡æ¢¯åº¦ä¸‹é™
```python
def mini_batch_gradient_descent():
    """å°æ‰¹é‡æ¢¯åº¦ä¸‹é™å®ç°"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("å°æ‰¹é‡æ¢¯åº¦ä¸‹é™ (Mini-batch GD)")
    
    # å‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.01
    batch_size = 20
    n_epochs = 50
    
    print(f"åˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    print(f"æ‰¹é‡å¤§å°: {batch_size}")
    print(f"epochs: {n_epochs}")
    
    cost_history = []
    
    for epoch in range(n_epochs):
        # æ‰“ä¹±æ•°æ®
        indices = np.random.permutation(m)
        X_shuffled = X_b[indices]
        y_shuffled = y[indices]
        
        for i in range(0, m, batch_size):
            end_idx = min(i + batch_size, m)
            X_batch = X_shuffled[i:end_idx]
            y_batch = y_shuffled[i:end_idx]
            
            # è®¡ç®—æ‰¹é‡æ¢¯åº¦
            predictions = X_batch @ theta
            gradient = (1 / len(X_batch)) * X_batch.T @ (predictions - y_batch)
            
            # æ›´æ–°å‚æ•°
            theta = theta - learning_rate * gradient
        
        # è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ä»£ä»·
        cost = np.mean((X_b @ theta - y) ** 2) / 2
        cost_history.append(cost)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    
    return cost_history

mini_batch_cost_history = mini_batch_gradient_descent()
```

---

## ğŸš€ é«˜çº§æ¢¯åº¦ä¸‹é™ç®—æ³•

### åŠ¨é‡æ³• (Momentum)
```python
def momentum_gradient_descent():
    """åŠ¨é‡æ³•æ¢¯åº¦ä¸‹é™"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("åŠ¨é‡æ³•æ¢¯åº¦ä¸‹é™")
    
    # å‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.01
    momentum = 0.9
    max_iterations = 1000
    
    # åˆå§‹åŒ–åŠ¨é‡
    velocity = np.zeros_like(theta)
    
    print(f"åˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    print(f"åŠ¨é‡ç³»æ•°: Î² = {momentum}")
    
    cost_history = []
    
    for i in range(max_iterations):
        # è®¡ç®—æ¢¯åº¦
        predictions = X_b @ theta
        gradient = (1 / m) * X_b.T @ (predictions - y)
        
        # æ›´æ–°åŠ¨é‡
        velocity = momentum * velocity - learning_rate * gradient
        
        # æ›´æ–°å‚æ•°
        theta = theta + velocity
        
        # è®¡ç®—ä»£ä»·
        cost = np.mean((predictions - y) ** 2) / 2
        cost_history.append(cost)
        
        if i % 100 == 0:
            print(f"è¿­ä»£ {i}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    print(f"æœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    
    return cost_history

momentum_cost_history = momentum_gradient_descent()
```

### Adamä¼˜åŒ–å™¨
```python
def adam_optimizer():
    """Adamä¼˜åŒ–ç®—æ³•"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("Adamä¼˜åŒ–å™¨")
    
    # å‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.01
    beta1 = 0.9      # ä¸€é˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡
    beta2 = 0.999    # äºŒé˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡
    epsilon = 1e-8   # é˜²æ­¢é™¤é›¶
    max_iterations = 1000
    
    # åˆå§‹åŒ–çŸ©ä¼°è®¡
    m_t = np.zeros_like(theta)  # ä¸€é˜¶çŸ©ä¼°è®¡
    v_t = np.zeros_like(theta)  # äºŒé˜¶çŸ©ä¼°è®¡
    
    print(f"åˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    print(f"Î²â‚ = {beta1}, Î²â‚‚ = {beta2}")
    
    cost_history = []
    
    for t in range(1, max_iterations + 1):
        # è®¡ç®—æ¢¯åº¦
        predictions = X_b @ theta
        gradient = (1 / m) * X_b.T @ (predictions - y)
        
        # æ›´æ–°ä¸€é˜¶çŸ©ä¼°è®¡
        m_t = beta1 * m_t + (1 - beta1) * gradient
        
        # æ›´æ–°äºŒé˜¶çŸ©ä¼°è®¡
        v_t = beta2 * v_t + (1 - beta2) * (gradient ** 2)
        
        # åå·®ä¿®æ­£
        m_t_corrected = m_t / (1 - beta1 ** t)
        v_t_corrected = v_t / (1 - beta2 ** t)
        
        # æ›´æ–°å‚æ•°
        theta = theta - learning_rate * m_t_corrected / (np.sqrt(v_t_corrected) + epsilon)
        
        # è®¡ç®—ä»£ä»·
        cost = np.mean((predictions - y) ** 2) / 2
        cost_history.append(cost)
        
        if t % 100 == 0:
            print(f"è¿­ä»£ {t}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    print(f"æœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    
    return cost_history

adam_cost_history = adam_optimizer()
```

---

## ğŸ¨ å­¦ä¹ ç‡çš„å½±å“

### å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ
```python
def learning_rate_comparison():
    """æ¯”è¾ƒä¸åŒå­¦ä¹ ç‡çš„æ•ˆæœ"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    learning_rates = [0.001, 0.01, 0.1, 0.3]
    max_iterations = 1000
    
    print("å­¦ä¹ ç‡å¯¹æ¯”å®éªŒ")
    print("=" * 50)
    
    results = {}
    
    for lr in learning_rates:
        print(f"\nå­¦ä¹ ç‡: {lr}")
        
        # åˆå§‹åŒ–å‚æ•°
        theta = np.random.randn(2, 1)
        cost_history = []
        
        for i in range(max_iterations):
            # è®¡ç®—æ¢¯åº¦
            predictions = X_b @ theta
            gradient = (1 / m) * X_b.T @ (predictions - y)
            
            # æ›´æ–°å‚æ•°
            theta = theta - lr * gradient
            
            # è®¡ç®—ä»£ä»·
            cost = np.mean((predictions - y) ** 2) / 2
            cost_history.append(cost)
            
            # æ£€æŸ¥æ˜¯å¦å‘æ•£
            if np.isnan(cost) or cost > 1e10:
                print(f"  å‘æ•£ï¼åœ¨ç¬¬ {i+1} æ¬¡è¿­ä»£")
                break
        
        if not np.isnan(cost) and cost < 1e10:
            print(f"  æœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
            print(f"  æœ€ç»ˆä»£ä»·: {cost:.6f}")
            print(f"  æ”¶æ•›é€Ÿåº¦: {i+1} æ¬¡è¿­ä»£")
        
        results[lr] = cost_history
    
    return results

lr_results = learning_rate_comparison()
```

### è‡ªé€‚åº”å­¦ä¹ ç‡
```python
def adaptive_learning_rate():
    """è‡ªé€‚åº”å­¦ä¹ ç‡å®ç°"""
    
    # ä½¿ç”¨ç›¸åŒçš„æ•°æ®
    np.random.seed(42)
    m = 100
    X = 2 * np.random.rand(m, 1)
    y = 4 + 3 * X + np.random.randn(m, 1)
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("è‡ªé€‚åº”å­¦ä¹ ç‡")
    
    # å‚æ•°
    theta = np.random.randn(2, 1)
    learning_rate = 0.1
    max_iterations = 1000
    patience = 10  # è¿ç»­å¤šå°‘æ¬¡ä¸æ”¹å–„å°±é™ä½å­¦ä¹ ç‡
    
    print(f"åˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"åˆå§‹å­¦ä¹ ç‡: Î± = {learning_rate}")
    
    cost_history = []
    lr_history = []
    best_cost = float('inf')
    patience_counter = 0
    
    for i in range(max_iterations):
        # è®¡ç®—æ¢¯åº¦
        predictions = X_b @ theta
        gradient = (1 / m) * X_b.T @ (predictions - y)
        
        # æ›´æ–°å‚æ•°
        theta = theta - learning_rate * gradient
        
        # è®¡ç®—ä»£ä»·
        cost = np.mean((predictions - y) ** 2) / 2
        cost_history.append(cost)
        lr_history.append(learning_rate)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ”¹å–„
        if cost < best_cost:
            best_cost = cost
            patience_counter = 0
        else:
            patience_counter += 1
            
            # å¦‚æœè¿ç»­patienceæ¬¡æ²¡æœ‰æ”¹å–„ï¼Œé™ä½å­¦ä¹ ç‡
            if patience_counter >= patience:
                learning_rate *= 0.5
                patience_counter = 0
                print(f"è¿­ä»£ {i}: é™ä½å­¦ä¹ ç‡è‡³ {learning_rate:.6f}")
        
        if i % 100 == 0:
            print(f"è¿­ä»£ {i}: Cost = {cost:.6f}, LR = {learning_rate:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    print(f"æœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    print(f"æœ€ç»ˆå­¦ä¹ ç‡: {learning_rate:.6f}")
    
    return cost_history, lr_history

adaptive_cost_history, adaptive_lr_history = adaptive_learning_rate()
```

---

## ğŸ¯ å®é™…åº”ç”¨æ¡ˆä¾‹

### é€»è¾‘å›å½’ä¸­çš„æ¢¯åº¦ä¸‹é™
```python
def logistic_regression_gradient_descent():
    """é€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™å®ç°"""
    
    # ç”ŸæˆäºŒåˆ†ç±»æ•°æ®
    np.random.seed(42)
    m = 100
    X = np.random.randn(m, 2)
    y = (X[:, 0] + X[:, 1] > 0).astype(int).reshape(-1, 1)
    
    # æ·»åŠ åç½®é¡¹
    X_b = np.c_[np.ones((m, 1)), X]
    
    print("é€»è¾‘å›å½’æ¢¯åº¦ä¸‹é™")
    print(f"æ ·æœ¬æ•°: {m}")
    print(f"ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"æ­£æ ·æœ¬æ•°: {np.sum(y)}")
    
    # Sigmoidå‡½æ•°
    def sigmoid(z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # é˜²æ­¢æº¢å‡º
    
    # ä»£ä»·å‡½æ•°ï¼ˆäº¤å‰ç†µï¼‰
    def cost_function(X, y, theta):
        m = len(y)
        h = sigmoid(X @ theta)
        # é˜²æ­¢log(0)
        h = np.clip(h, 1e-15, 1 - 1e-15)
        cost = -(1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))
        return cost
    
    # æ¢¯åº¦
    def compute_gradient(X, y, theta):
        m = len(y)
        h = sigmoid(X @ theta)
        gradient = (1/m) * X.T @ (h - y)
        return gradient
    
    # åˆå§‹åŒ–å‚æ•°
    theta = np.random.randn(3, 1) * 0.01
    learning_rate = 0.1
    max_iterations = 1000
    
    print(f"\nåˆå§‹å‚æ•°: Î¸ = {theta.flatten()}")
    print(f"å­¦ä¹ ç‡: Î± = {learning_rate}")
    
    cost_history = []
    
    for i in range(max_iterations):
        cost = cost_function(X_b, y, theta)
        gradient = compute_gradient(X_b, y, theta)
        theta = theta - learning_rate * gradient
        
        cost_history.append(cost)
        
        if i % 100 == 0:
            print(f"è¿­ä»£ {i}: Cost = {cost:.6f}, Î¸ = {theta.flatten()}")
    
    print(f"\næœ€ç»ˆå‚æ•°: Î¸ = {theta.flatten()}")
    print(f"æœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    
    # è®¡ç®—å‡†ç¡®ç‡
    predictions = sigmoid(X_b @ theta) > 0.5
    accuracy = np.mean(predictions == y)
    print(f"è®­ç»ƒå‡†ç¡®ç‡: {accuracy:.2%}")
    
    return theta, cost_history

logistic_theta, logistic_cost_history = logistic_regression_gradient_descent()
```

### ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­
```python
def neural_network_backpropagation():
    """ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    # ç”ŸæˆXORæ•°æ®
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    print("ç¥ç»ç½‘ç»œåå‘ä¼ æ’­ (XORé—®é¢˜)")
    print("è¾“å…¥æ•°æ®:")
    for i in range(len(X)):
        print(f"  {X[i]} -> {y[i][0]}")
    
    # ç½‘ç»œç»“æ„ï¼š2-3-1 (è¾“å…¥å±‚2ä¸ªç¥ç»å…ƒï¼Œéšè—å±‚3ä¸ªï¼Œè¾“å‡ºå±‚1ä¸ª)
    input_size = 2
    hidden_size = 3
    output_size = 1
    
    # åˆå§‹åŒ–æƒé‡
    np.random.seed(42)
    W1 = np.random.randn(input_size, hidden_size) * 0.5
    b1 = np.zeros((1, hidden_size))
    W2 = np.random.randn(hidden_size, output_size) * 0.5
    b2 = np.zeros((1, output_size))
    
    print(f"\nç½‘ç»œç»“æ„: {input_size}-{hidden_size}-{output_size}")
    
    # æ¿€æ´»å‡½æ•°
    def sigmoid(z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def sigmoid_derivative(z):
        return z * (1 - z)
    
    # å‰å‘ä¼ æ’­
    def forward_pass(X):
        z1 = X @ W1 + b1
        a1 = sigmoid(z1)
        z2 = a1 @ W2 + b2
        a2 = sigmoid(z2)
        return a1, a2
    
    # è®­ç»ƒå‚æ•°
    learning_rate = 1.0
    epochs = 10000
    
    print(f"å­¦ä¹ ç‡: {learning_rate}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    
    cost_history = []
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        a1, a2 = forward_pass(X)
        
        # è®¡ç®—ä»£ä»·
        cost = np.mean((a2 - y) ** 2)
        cost_history.append(cost)
        
        # åå‘ä¼ æ’­
        # è¾“å‡ºå±‚æ¢¯åº¦
        dz2 = a2 - y
        dW2 = (1/len(X)) * a1.T @ dz2
        db2 = (1/len(X)) * np.sum(dz2, axis=0, keepdims=True)
        
        # éšè—å±‚æ¢¯åº¦
        da1 = dz2 @ W2.T
        dz1 = da1 * sigmoid_derivative(a1)
        dW1 = (1/len(X)) * X.T @ dz1
        db1 = (1/len(X)) * np.sum(dz1, axis=0, keepdims=True)
        
        # æ›´æ–°å‚æ•°
        W2 -= learning_rate * dW2
        b2 -= learning_rate * db2
        W1 -= learning_rate * dW1
        b1 -= learning_rate * db1
        
        if epoch % 1000 == 0:
            print(f"Epoch {epoch}: Cost = {cost:.6f}")
    
    print(f"\nè®­ç»ƒå®Œæˆï¼Œæœ€ç»ˆä»£ä»·: {cost_history[-1]:.6f}")
    
    # æµ‹è¯•ç»“æœ
    _, predictions = forward_pass(X)
    print("\né¢„æµ‹ç»“æœ:")
    for i in range(len(X)):
        pred = predictions[i][0]
        actual = y[i][0]
        print(f"  {X[i]} -> é¢„æµ‹: {pred:.4f}, å®é™…: {actual}, æ­£ç¡®: {abs(pred - actual) < 0.5}")
    
    return cost_history

nn_cost_history = neural_network_backpropagation()
```

---

## ğŸ“Š æ¢¯åº¦ä¸‹é™çš„å¯è§†åŒ–åˆ†æ

### ç­‰é«˜çº¿å›¾å¯è§†åŒ–
```python
def visualize_gradient_descent():
    """å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹"""
    
    # å®šä¹‰ç›®æ ‡å‡½æ•°
    def f(x, y):
        return (x - 2)**2 + (y - 1)**2
    
    # æ¢¯åº¦
    def gradient_f(x, y):
        return np.array([2*(x - 2), 2*(y - 1)])
    
    print("æ¢¯åº¦ä¸‹é™å¯è§†åŒ–")
    
    # åˆ›å»ºç½‘æ ¼
    x = np.linspace(-1, 5, 100)
    y = np.linspace(-2, 4, 100)
    X, Y = np.meshgrid(x, y)
    Z = f(X, Y)
    
    # æ¢¯åº¦ä¸‹é™
    start_point = np.array([0, 3])
    learning_rate = 0.1
    max_iterations = 50
    
    trajectory = [start_point]
    current_point = start_point.copy()
    
    for i in range(max_iterations):
        grad = gradient_f(current_point[0], current_point[1])
        new_point = current_point - learning_rate * grad
        trajectory.append(new_point)
        
        if np.linalg.norm(new_point - current_point) < 1e-6:
            break
        
        current_point = new_point
    
    trajectory = np.array(trajectory)
    
    print(f"èµ·å§‹ç‚¹: {start_point}")
    print(f"æœ€ç»ˆç‚¹: {trajectory[-1]}")
    print(f"çœŸå®æœ€å°å€¼ç‚¹: [2, 1]")
    print(f"è¿­ä»£æ¬¡æ•°: {len(trajectory) - 1}")
    
    # åˆ†ææ”¶æ•›è¿‡ç¨‹
    distances = [np.linalg.norm(point - np.array([2, 1])) for point in trajectory]
    print(f"\næ”¶æ•›è¿‡ç¨‹:")
    for i in range(0, len(distances), 10):
        print(f"  è¿­ä»£ {i}: è·ç¦»æœ€å°å€¼ {distances[i]:.6f}")
    
    return trajectory, Z

trajectory, Z = visualize_gradient_descent()
```

---

## ğŸ“š æ€»ç»“ä¸å®è·µå»ºè®®

### æ¢¯åº¦ä¸‹é™çš„ä¼˜ç¼ºç‚¹
```python
def gradient_descent_analysis():
    """æ¢¯åº¦ä¸‹é™æ–¹æ³•çš„åˆ†ææ€»ç»“"""
    
    print("æ¢¯åº¦ä¸‹é™ç®—æ³•æ€»ç»“")
    print("=" * 50)
    
    algorithms = {
        "æ‰¹é‡æ¢¯åº¦ä¸‹é™ (BGD)": {
            "ä¼˜ç‚¹": ["æ”¶æ•›ç¨³å®š", "èƒ½æ‰¾åˆ°å…¨å±€æœ€å°å€¼ï¼ˆå‡¸å‡½æ•°ï¼‰", "å®ç°ç®€å•"],
            "ç¼ºç‚¹": ["è®¡ç®—æ…¢ï¼ˆå¤§æ•°æ®é›†ï¼‰", "å†…å­˜éœ€æ±‚å¤§", "å¯èƒ½é™·å…¥å±€éƒ¨æœ€å°å€¼"],
            "é€‚ç”¨åœºæ™¯": ["å°åˆ°ä¸­ç­‰æ•°æ®é›†", "å‡¸ä¼˜åŒ–é—®é¢˜", "éœ€è¦ç²¾ç¡®è§£"]
        },
        
        "éšæœºæ¢¯åº¦ä¸‹é™ (SGD)": {
            "ä¼˜ç‚¹": ["é€Ÿåº¦å¿«", "å†…å­˜å‹å¥½", "èƒ½è·³å‡ºå±€éƒ¨æœ€å°å€¼"],
            "ç¼ºç‚¹": ["æ”¶æ•›ä¸ç¨³å®š", "å¯èƒ½æŒ¯è¡", "éœ€è¦è°ƒæ•´å­¦ä¹ ç‡"],
            "é€‚ç”¨åœºæ™¯": ["å¤§æ•°æ®é›†", "åœ¨çº¿å­¦ä¹ ", "éå‡¸ä¼˜åŒ–"]
        },
        
        "å°æ‰¹é‡æ¢¯åº¦ä¸‹é™": {
            "ä¼˜ç‚¹": ["å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§", "å¯ä»¥å¹¶è¡ŒåŒ–", "å™ªå£°é€‚ä¸­"],
            "ç¼ºç‚¹": ["éœ€è¦é€‰æ‹©æ‰¹é‡å¤§å°", "ä»å¯èƒ½æŒ¯è¡"],
            "é€‚ç”¨åœºæ™¯": ["å¤§å¤šæ•°æœºå™¨å­¦ä¹ ä»»åŠ¡", "æ·±åº¦å­¦ä¹ ", "å®é™…åº”ç”¨"]
        },
        
        "åŠ¨é‡æ³•": {
            "ä¼˜ç‚¹": ["åŠ é€Ÿæ”¶æ•›", "å‡å°‘æŒ¯è¡", "è·³å‡ºå±€éƒ¨æœ€å°å€¼"],
            "ç¼ºç‚¹": ["å¤šä¸€ä¸ªè¶…å‚æ•°", "å¯èƒ½è¿‡å†²"],
            "é€‚ç”¨åœºæ™¯": ["æ·±åº¦å­¦ä¹ ", "å¤æ‚ä¼˜åŒ–é—®é¢˜"]
        },
        
        "Adam": {
            "ä¼˜ç‚¹": ["è‡ªé€‚åº”å­¦ä¹ ç‡", "æ”¶æ•›å¿«", "å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ"],
            "ç¼ºç‚¹": ["å†…å­˜éœ€æ±‚å¤§", "å¯èƒ½ä¸æ”¶æ•›åˆ°æœ€ä¼˜è§£"],
            "é€‚ç”¨åœºæ™¯": ["æ·±åº¦å­¦ä¹ ", "é»˜è®¤é€‰æ‹©", "å¿«é€ŸåŸå‹"]
        }
    }
    
    for name, info in algorithms.items():
        print(f"\n{name}:")
        print(f"  ä¼˜ç‚¹: {', '.join(info['ä¼˜ç‚¹'])}")
        print(f"  ç¼ºç‚¹: {', '.join(info['ç¼ºç‚¹'])}")
        print(f"  é€‚ç”¨åœºæ™¯: {', '.join(info['é€‚ç”¨åœºæ™¯'])}")
    
    print("\n\nå®è·µå»ºè®®:")
    print("=" * 30)
    
    tips = [
        "1. ç‰¹å¾ç¼©æ”¾ï¼šç¡®ä¿ç‰¹å¾åœ¨ç›¸ä¼¼èŒƒå›´å†…",
        "2. å­¦ä¹ ç‡è°ƒæ•´ï¼šä»0.01å¼€å§‹ï¼Œæ ¹æ®æ”¶æ•›æƒ…å†µè°ƒæ•´",
        "3. æ”¶æ•›ç›‘æ§ï¼šç»˜åˆ¶æŸå¤±å‡½æ•°æ›²çº¿",
        "4. æ—©åœæ³•ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ",
        "5. æ‰¹é‡å¤§å°ï¼šé€šå¸¸32-512ä¹‹é—´",
        "6. æƒé‡åˆå§‹åŒ–ï¼šé¿å…å¯¹ç§°æ€§ç ´å",
        "7. æ¢¯åº¦æ£€æŸ¥ï¼šéªŒè¯æ¢¯åº¦è®¡ç®—æ­£ç¡®æ€§"
    ]
    
    for tip in tips:
        print(f"  {tip}")
    
    print("\n\nå¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ:")
    print("=" * 30)
    
    problems = {
        "å­¦ä¹ ç‡è¿‡å¤§": "å‡å°å­¦ä¹ ç‡æˆ–ä½¿ç”¨è‡ªé€‚åº”æ–¹æ³•",
        "å­¦ä¹ ç‡è¿‡å°": "å¢å¤§å­¦ä¹ ç‡æˆ–ä½¿ç”¨åŠ¨é‡æ³•",
        "é™·å…¥å±€éƒ¨æœ€å°å€¼": "ä½¿ç”¨SGDã€åŠ¨é‡æ³•æˆ–éšæœºé‡å¯",
        "æ¢¯åº¦çˆ†ç‚¸": "æ¢¯åº¦è£å‰ªã€é™ä½å­¦ä¹ ç‡",
        "æ¢¯åº¦æ¶ˆå¤±": "æ”¹å˜ç½‘ç»œç»“æ„ã€ä½¿ç”¨æ®‹å·®è¿æ¥",
        "æ”¶æ•›æ…¢": "ç‰¹å¾ç¼©æ”¾ã€é¢„æ¡ä»¶åŒ–ã€æ›´å¥½çš„åˆå§‹åŒ–"
    }
    
    for problem, solution in problems.items():
        print(f"  {problem}: {solution}")

gradient_descent_analysis()
```

---

## ğŸ¯ æ€»ç»“

æ¢¯åº¦ä¸‹é™ç®—æ³•æ˜¯æœºå™¨å­¦ä¹ çš„åŸºçŸ³ï¼Œç†è§£å…¶åŸç†å’Œå˜ä½“å¯¹äºæŒæ¡ç°ä»£æœºå™¨å­¦ä¹ è‡³å…³é‡è¦ã€‚

### æ ¸å¿ƒè¦ç‚¹
1. **æ•°å­¦åŸºç¡€**ï¼šæ¢¯åº¦æŒ‡å‘å‡½æ•°å¢é•¿æœ€å¿«çš„æ–¹å‘
2. **ç®—æ³•æœ¬è´¨**ï¼šæ²¿è´Ÿæ¢¯åº¦æ–¹å‘è¿­ä»£å¯»æ‰¾æœ€å°å€¼
3. **å‚æ•°è°ƒèŠ‚**ï¼šå­¦ä¹ ç‡æ˜¯æœ€å…³é”®çš„è¶…å‚æ•°
4. **å˜ä½“é€‰æ‹©**ï¼šæ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„å˜ä½“

### å®é™…åº”ç”¨
- **çº¿æ€§å›å½’**ï¼šæœ€ç®€å•çš„åº”ç”¨åœºæ™¯
- **é€»è¾‘å›å½’**ï¼šäºŒåˆ†ç±»é—®é¢˜çš„ç»å…¸åº”ç”¨
- **ç¥ç»ç½‘ç»œ**ï¼šæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç®—æ³•
- **æ¨èç³»ç»Ÿ**ï¼šçŸ©é˜µåˆ†è§£å’ŒååŒè¿‡æ»¤

### å­¦ä¹ å»ºè®®
1. **åŠ¨æ‰‹å®è·µ**ï¼šä»ç®€å•çš„1Dä¾‹å­å¼€å§‹
2. **å¯è§†åŒ–ç†è§£**ï¼šç»˜åˆ¶æŸå¤±å‡½æ•°å’Œæ”¶æ•›è¿‡ç¨‹
3. **å‚æ•°è°ƒä¼˜**ï¼šé€šè¿‡å®éªŒç†è§£è¶…å‚æ•°çš„å½±å“
4. **ç®—æ³•æ¯”è¾ƒ**ï¼šåœ¨ç›¸åŒé—®é¢˜ä¸Šæ¯”è¾ƒä¸åŒç®—æ³•

---

**ğŸ¯ è®°ä½ï¼šæ¢¯åº¦ä¸‹é™ä¸ä»…æ˜¯ä¸€ä¸ªç®—æ³•ï¼Œæ›´æ˜¯ä¸€ç§æ€ç»´æ–¹å¼â€”â€”é€šè¿‡è¿­ä»£æ”¹è¿›æ¥é€¼è¿‘æœ€ä¼˜è§£ï¼** 